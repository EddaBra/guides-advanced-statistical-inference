[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Guides for Advanced Statistical Inference",
    "section": "",
    "text": "This Quarto Book provides guidance for advanced methods of statistical inference and their implementation in R. The book is based on the course “Advanced Quantitative Data Analysis”, lectured by Prof Dr. Sabine Zinn, DIW Berlin) in summer term at Humboldt-Universität zu Berlin.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "multi-level.html",
    "href": "multi-level.html",
    "title": "2  Multilevel Data Modelling",
    "section": "",
    "text": "2.1 Load libraries\nRecommended literature:\nSnijders, T. A., & Bosker, R. J. (2011). Multilevel analysis: An introduction to basic and advanced multilevel modeling. Sage.\nlibrary(lme4)\nlibrary(lmerTest)\nlibrary(mitml)\nlibrary(sjlabelled)\nlibrary(tidyverse)\nlibrary(stargazer) ##comparing models\nlibrary(sjmisc)\nlibrary(lmerTest)\nlibrary(broom.mixed)\nlibrary(sjPlot) \nlibrary(glmmTMB)\nlibrary(stargazer)\nlibrary(modelsummary)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multilevel Data Modelling</span>"
    ]
  },
  {
    "objectID": "multi-level.html#multi-level-issue",
    "href": "multi-level.html#multi-level-issue",
    "title": "2  Multilevel Data Modelling",
    "section": "2.2 Multi level issue",
    "text": "2.2 Multi level issue\n\nMicro units are nested within macro units\nMicro units impact on macro units and vice versa\n\nMicro units within a macro unit tend to be more similar to each other as compared to other micro units in other macro units\nObservations are not any longer independent of each other\nIn statistical analysis this impacts on their variances\nNeglecting this issue leads likely at least to wrong variance estimates, in the worst case to biased estimates\n\n\n→ Multilevel analysis is a suitable approach to take into account the social contexts as well as the individual respondents or subjects.\n\n2.2.0.1 Sampling\n\nCluster sampling, multi-stage sampling are standard\nSecondary units are not selected independently of each other\n\nhaving selected the primary units increases the chance of sampling the secondary units → Ignoring may heavily impact on the variance estimates and may thus produce type I error (reject a true null hypothesis)\n\n\n\n\n\nSampling units\n\n\nTwo arguments for multilevel instead of single level regression of disaggregated data:\n\nDependence as a nuisance:\n\n\nStandard errors and tests base on single-level regression are suspect because the assumption of independent residuals is invalid.\n\n\nDependence as an interesting phenomenon:\n\n\nIt is interesting in itself to disentangle variability at the various levels; moreover, this can give insight in the directions where further explanation may fruitfully be sought.\n\n\n\n\nmulti level terms\n\n\n\n\n2.2.0.2 Modelling\nHierarchical linear model is a type of regression analysis for multilevel data where the dependent variable is at the lowest level.\n\nExplanatory variables can be defined at any level (including aggregates of level-one variables).\nAlso longitudinal data can be regarded as a nested structure; for such data the hierarchical linear model is convenient as well (with some particularities).\n\n\n\n\nModelling\n\n\n\n\n2.2.1 Exercise\n\nLoad library library(lme4)\n\nAlready done, see Section 2.1.\n\nLoad the data:\n\n\nDAT &lt;- read.csv2(\"data/1.Example.txt\")\n\n\nHave a look at the data\n\nHow many observations and variables?\n\nstr(DAT)\n\n'data.frame':   3454 obs. of  11 variables:\n $ schoolnr   : int  174 44 83 64 172 201 259 218 109 88 ...\n $ pupilNR_new: int  2923 681 1332 1041 2879 3405 4207 3682 1719 1435 ...\n $ langPOST   : int  37 43 41 52 35 49 22 42 30 52 ...\n $ ses        : num  -14.73 -2.73 -7.73 3.27 -2.73 ...\n $ IQ_verb    : num  -2.87 -0.37 0.13 1.13 -2.37 2.63 0.63 -0.37 -4.87 1.63 ...\n $ sex        : int  0 1 0 0 1 0 0 1 1 1 ...\n $ Minority   : int  0 0 0 0 0 0 0 1 1 0 ...\n $ denomina   : int  5 3 5 1 2 3 1 3 1 2 ...\n $ sch_ses    : num  -7.68 -8.98 5.75 3.23 -3.7 ...\n $ sch_iqv    : num  0.252 -0.742 0.503 0.252 -0.226 ...\n $ sch_min    : num  0 0.188 0 0.091 0 0.115 0.111 0.92 0.19 0 ...\n\n\nIn the data set for each pupil is measured:\n\nwhich school he/she belongs to (schoolnr)\nwhat her/ his unique pupil number is (pupuilNR_new)\nwhat score the pupil reach in his/hers language test (our dependent variable) (langPOST)\nwhat its socio-economic background is (ses) - a binary variable that indicates the sex, if the pupil belongs to a minority (Minority) - the verbal IQ of the pupil, centered by the mean (IQ_verbal)\n\nAnd the dataset contains measures for each school\n\nthe schools socio-economic backgrounds (school_ses)\nthe verbal IQ of all pupils in a school (school_iqv)\nan index how many pupils in a school belongs to a minority (school_min)\n\n\nlength(unique(DAT$pupilNR_new))\n\n[1] 3454\n\n\n3454 observations = pupils 11 variables over all. Each pupil has a pupil number, that is unique and is assigned to one of 211 schools.\nFor overview let’s get some inside in the descriptives of each variable:\n\nsummary(DAT)\n\n    schoolnr      pupilNR_new      langPOST          ses           \n Min.   :  1.0   Min.   :   3   Min.   : 8.00   Min.   :-17.73000  \n 1st Qu.: 69.0   1st Qu.:1130   1st Qu.:36.00   1st Qu.: -7.73000  \n Median :135.0   Median :2200   Median :42.00   Median : -1.73000  \n Mean   :132.1   Mean   :2171   Mean   :41.32   Mean   :  0.04678  \n 3rd Qu.:189.0   3rd Qu.:3209   3rd Qu.:48.00   3rd Qu.:  8.27000  \n Max.   :259.0   Max.   :4214   Max.   :58.00   Max.   : 22.27000  \n    IQ_verb              sex            Minority          denomina    \n Min.   :-7.87000   Min.   :0.0000   Min.   :0.00000   Min.   :1.000  \n 1st Qu.:-0.87000   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:1.000  \n Median : 0.13000   Median :0.0000   Median :0.00000   Median :2.000  \n Mean   : 0.03098   Mean   :0.4896   Mean   :0.04806   Mean   :2.257  \n 3rd Qu.: 1.13000   3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:3.000  \n Max.   : 6.63000   Max.   :1.0000   Max.   :1.00000   Max.   :5.000  \n    sch_ses             sch_iqv           sch_min       \n Min.   :-17.72700   Min.   :-4.8113   Min.   :0.00000  \n 1st Qu.: -4.38100   1st Qu.:-0.3668   1st Qu.:0.00000  \n Median :  0.13000   Median : 0.0932   Median :0.00000  \n Mean   :  0.03116   Mean   : 0.0127   Mean   :0.05202  \n 3rd Qu.:  4.43900   3rd Qu.: 0.4665   3rd Qu.:0.05900  \n Max.   : 15.54500   Max.   : 2.4769   Max.   :0.92000  \n\n\nThe schoolnr for the 211 schools ranges from 1 to 259.\n\ntable(is.na.data.frame(DAT))\n\n\nFALSE \n37994 \n\n\nNo missing values in the data frame.\nGet insight in the distribution of each variable:\n\nhist(DAT$ses)\n\n\n\n\n\n\n\nhist(DAT$sch_ses)\n\n\n\n\n\n\n\nhist(DAT$IQ_verb)\n\n\n\n\n\n\n\nhist(DAT$sch_iqv)\n\n\n\n\n\n\n\nhist(DAT$Minority)\n\n\n\n\n\n\n\nhist(DAT$sch_min)\n\n\n\n\n\n\n\n\n\nses is left scewed,more pupils have a lower ses than the mean,- this is not true for schools\niqverb is slightly right scewed, range is wider on the negative site than on the possitve, this is also true for distribution of schools\ndistribution for the mean IQ has more outliers on the negative side than on the positive side\nonly under 10 percent of the pupils belongs to a minority, there are a lot schools where none of the pupils belongs to a minority group, while there are some schools, where all pupils belongs to a minority.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multilevel Data Modelling</span>"
    ]
  },
  {
    "objectID": "multi-level.html#random-intercept-model-rim-or-random-effects-model",
    "href": "multi-level.html#random-intercept-model-rim-or-random-effects-model",
    "title": "2  Multilevel Data Modelling",
    "section": "2.3 Random Intercept Model (RIM) or Random Effects Model",
    "text": "2.3 Random Intercept Model (RIM) or Random Effects Model\n\n\\(i\\) indicates the individual level-one unit\n\\(j\\) indicates the group, level two unit\nvariables for individual \\(i\\) in group \\(j\\):\n\n\\(Y_{ij}\\)\n\\(x_{ij}\\) explanatory variable at level one\n\nfor group \\(j\\)\n\n\\(z_j\\) explanatory at level two\n\\(n_j\\) group size\nOLS regression model of \\(Y\\) on \\(x\\) ignoring groups\n\n\n\\[\nY_{ij} = \\beta_0 + \\beta_1x_{ij}+ R_{ij}\n\\]\n\\(R_{ij}\\) is not the error term, it is the residual term: An error term is generally unobservable and a residual is observable and calculable, making it much easier to quantify and visualize. In effect, while an error term represents the way observed data differs from the actual population, a residual represents the way observed data differs from sample population data.\ngroup dependent regression model is dependent on groups, the following notation means, that the units on level 2 (indivudals) have random intercepts\n\\[\nY_ij = \\beta_{0j} + \\beta_{1j}x_{ij}+ R_{ij}\n\\]\nIn the random intercept model, the intercepts \\(\\beta_{0j}\\) are random variables representing random differences between groups, where \\(\\beta_{0j} =\\) average intercept \\(\\gamma00\\) (for all level two units) plus group-dependent deviation \\(U_{0j}\\)\n\\[\n\\beta_{0j} = \\gamma_{00} + U_{0j}\n\\]\nIn this model, the regression coefficient \\(β_1\\) is common to all the groups (otherwise random slope model)\nIn a fixed effects model each group has own intercept \\(\\beta_{0j}\\) to be estimated, because \\(\\beta_{0j} =\\) is not a random variable.\nIn the random intercept model, the constant regression coefficient \\(β_1\\) is sometimes denoted \\(\\gamma_{10}\\) →\n\\[\nY_{ij} = \\gamma_{00} + \\gamma_{10}x_{ij}+U_{0j}+ R{ij}\n\\]\n\nIn the hierarchical linear model, the \\(U_{0j}\\) are random variables. The model assumption is that they are independent, normally distributed with expected value \\(0\\), and variance from\n\\[\n\\tau^2 = var(U_{0j})\n\\]\nThe statistical parameter in the model is not their individual values, but their variance \\(tau^2\\). \\(U_{0j}\\) represent the random intercept for each cluster. It’s really a residual term that measures the distance from each subject’s intercept around the overall intercept \\(β_0\\). Rather than calculate an estimate for every one of those distances, the model is able to just estimate a single variance. This is the between group variance.\nThe within-group variance is measured by the variance of residuals. Their sum is the total variance in \\(Y\\) that is not explained by \\(X\\).\n\n\n\nRandom regression lines\n\n\n\n2.3.1 Fixed or Random Effects for Groups?\nArguments for choosing between fixed (F) and random (R) coefficient models for the group dummies:\n\nIf groups are unique entities and inference should focus on these groups: F. This often is the case with a small number of groups. $rarr; good for countries\nIf groups are regarded as sample from a (perhaps hypothetical) population and inference should focus on this population: R This often is the case with a large number of groups.\nIf group sizes are small and there are many groups, and it is reasonable to assume exchangeability of group-level residuals, then R makes better use of the data.\nIf the researcher is interested only in within-group effects, and is suspicious about the model for between-group differences, then F is more robust.\n\n\nIn groups we need enough variance and distribution\n\n\nIf group effects \\(U_{0j}\\) (etc.) are not nearly normally distributed, R is risky (or use more complicated multilevel models).\n\nA fixed effect and a random effect are two different statistical models used in data analysis, including in the context of meta-analysis and regression analysis. Here is a brief explanation of each:\n\n2.3.1.1 Fixed Effect:\nIn a fixed effect model, the assumption is that the effects being studied are constant across all levels of the categorical variable(s) being considered. It assumes that the observed differences are due to the characteristics of the entities being studied, rather than being random or varying across different groups. In a fixed effect model, the parameters being estimated are specific to the entities or groups being analyzed. The fixed effect model is appropriate when the goal is to estimate the specific effects of each entity or group and to make inferences about those specific entities or groups. helen says\n\n\n2.3.1.2 Random Effect:\nIn a random effect model, the assumption is that the effects being studied are not constant across all levels of the categorical variable(s) being considered. It assumes that the observed differences are due to a combination of the characteristics of the entities being studied and random variation. In a random effect model, the parameters being estimated represent the average effects across all entities or groups being analyzed, as well as the variability between entities or groups. The random effect model is appropriate when the goal is to estimate the average effects across all entities or groups and to make inferences about the population of entities or groups. It is important to choose the appropriate model based on the research question and the characteristics of the data. The choice between fixed effect and random effect models depends on the underlying assumptions and the goals of the analysis.\nHere are some additional resources that may be helpful: pubmed.ncbi.nlm.nih.gov A basic introduction to fixed-effect and random-effects models … - PubMed PMID: 26061376 DOI: 10.1002/jrsm.12 Abstract There are two popular statistical models for meta-analysis, the fixed-effect model and the random-effects model…\n\n\n\n2.3.2 Intraclass Correlation Coefficent\nTo deduce if multi-level is necessary & appropriate use the intraclass correlation coefficient.\nIf their is no correlation among the observations within a cluster, cluster means won’t differ (no between-group variance)\nStart with the empty model (random effects ANOVA) is a model without explanatory variables:\n\\[\nY_{ij} = \\gamma_{00} + U_{0j} + R_{ij}\n\\]\nVariance decomposition:\n\\[\nvar (Y_{ij}) = var (U_{0j}) + var (R_{ij}) =\\tau^2 + \\sigma^2\n\\]\nCan the covariates on individual level be explained by the group j? Check for intraclass correlation coefficient (ICC). The ratio of the between-cluster variance to the total variance is called the Intraclass Correlation. If the correlation is zero, then nothing of the variance comes in addition.\n\nCovariance between two individuals (i ̸= k) in the same group j :\n\n\\[\n    cov(Y_{ij} ,Y_{kj}) = var(U_{0j}) = \\tau^2\n\\]\nAnd their correlation with the intraclass correlation coefficient (ICC).\n\\[\nρ(Y_{ij} ,Y_{kj}) = ρ(Y) =\\frac{\\tau^2}{\\tau^2 + \\sigma^2}\n\\]\nIt is the proportion what is left unexplained in only looking at the single level. The higher the covariance is, the more multi level analysis is appropriate and can give explanation. The higher the difference between the classes, the higher is the intraclass correlation coefficient. → what are the threshold values indicating an RIM oder random coefficent model?\n\nOften between 0.05 and 0.25 in social science research, where the groups represent some kind of social grouping.\n\n\n\n2.3.3 Exercise\n\nmicro units: 3454 pupils\nmacro units: 211 schools\nY is a language test langPost\n\n\nEstimate empty model:\n\n\nmod1 &lt;- lmerTest::lmer(langPOST #dependent variable\n             ~  (1|schoolnr), #independent variable & random effect is in this case the school number\n             data = DAT, #data\n             REML = TRUE) # a robust estimator \nsummary(mod1) #get summary\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: langPOST ~ (1 | schoolnr)\n   Data: DAT\n\nREML criterion at convergence: 24482.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.0887 -0.6236  0.0769  0.7279  2.5470 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n schoolnr (Intercept) 18.78    4.333   \n Residual             63.20    7.950   \nNumber of obs: 3454, groups:  schoolnr, 211\n\nFixed effects:\n            Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)  40.9224     0.3325 192.7084   123.1   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nfixef(mod1) # only fixed effects\n\n(Intercept) \n    40.9224 \n\nconfint(mod1) # for confindence intervals\n\nComputing profile confidence intervals ...\n\n\n                2.5 %    97.5 %\n.sig01       3.821700  4.889329\n.sigma       7.759807  8.147763\n(Intercept) 40.266476 41.574208\n\n\n.sig01 stands for \\(\\tau^2\\) → does not be centered around zero, like expected. .sigma stands for \\(\\sigma^2\\)\nREML stands for restricted maximum likelihood method. LMER uses this because numerically fitting a mixed-effects model can be difficult and the maximum likelihood approach often fails.\nThe empty model shows, how the language test results differ between the schools. It is the estimated true group mean. Overall we have an estimate of 40.9224. The residual variance on the school level is 18.78 (variance), the one on the individual level is 63.20.\n\nDerive the ICC from the output. Assess whether it is really necessary to use a multi-level model for analyzing this data set.\n\n\n\\(\\tau^2\\), is the variance of \\(U_{0j} = 18.78\\)\n\\(\\sigma^2\\) is the variance of the residual \\(R_{ij} = 63.2\\)\n\n\n18.78 / (18.78 + 63.2)\n\n[1] 0.2290803\n\n\nBecause ICC is between 0.05 and 0.25 an RIM is appropriate. It indicates, that there is a variance, that can be explained by schools → a between-group variance.\n\nWhat is the estimated mean of the total population of individual values \\(Y_{ij}\\) and the corresponding standard deviation?\n\nThe estimated mean of the total population is 40.9224 (see the summmary of mod1, fixed effects and the intercept) with a standard error 0.3325.\nThe standard deviation is for this total population is\n\n#Multiply the mean SE (derived from the fixed effects, SE for the intercept) with the square root of the sample size.\n0.3325 * (sqrt(3454))\n\n[1] 19.54127\n\n\n\nWhat is the estimated mean of the population of class means \\(\\beta_{0j}\\) and the corresponding standard deviation?\n\nIn this model no difference expected, because is the same, variance should be explained on the individual level\n\n\n2.3.4 Adding Covariates\nThe model becomes more interesting, when also fixed effects of explanatory variables are included:\n\\[\nY_{ij}  = γ_{00} + γ_10x_{ij} + U_{0j} + R_{ij}\n\\]\nNote the difference between fixed effects of explanatory variables and fixed effects of group dummies.\nThere are two kinds of parameters: 1. fixed effects: regression coefficients γ (just like in OLS regression) 2. random effects: \\(U_{0j}\\) determined by variance component \\(\\tau_0^2\\)\n\n\n2.3.5 Exercise\n\nAdd the covariate for IQ into the model on micro-level, linear model without multi-level\n\n\nmod2 &lt;- lm(langPOST ~ IQ_verb, data=DAT) #independent micro variable\nsummary(mod2)\n\n\nCall:\nlm(formula = langPOST ~ IQ_verb, data = DAT)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-29.2571  -4.5436   0.6551   5.0996  25.9592 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 41.23512    0.12120  340.21   &lt;2e-16 ***\nIQ_verb      2.64326    0.05928   44.59   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.122 on 3452 degrees of freedom\nMultiple R-squared:  0.3654,    Adjusted R-squared:  0.3652 \nF-statistic:  1988 on 1 and 3452 DF,  p-value: &lt; 2.2e-16\n\n\n\nEstimate also a multi-level model (RIM), and compare both. Discuss!\n\n\nmod3 &lt;- lmer(langPOST ~ IQ_verb + (1|schoolnr), data=DAT)\nsummary(mod3)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: langPOST ~ IQ_verb + (1 | schoolnr)\n   Data: DAT\n\nREML criterion at convergence: 22972.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.1563 -0.6283  0.0789  0.7130  3.1760 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n schoolnr (Intercept) 10.22    3.197   \n Residual             41.14    6.414   \nNumber of obs: 3454, groups:  schoolnr, 211\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept) 4.099e+01  2.497e-01 1.980e+02  164.13   &lt;2e-16 ***\nIQ_verb     2.497e+00  5.725e-02 3.432e+03   43.61   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n        (Intr)\nIQ_verb 0.004 \n\n\nWe can see, the model gives more explanation, if we used the linear mixed-effects model 3. The variance in model 3 for between group and within group is much more smaller than in model 1. The scaled residuals in model 3 are in an less expanded range, than the residuals in model 2, that indicates a better model fit.\n\nsigma(mod2)\n\n[1] 7.122464\n\nsigma(mod3)\n\n[1] 6.413664\n\n?sigma\n\nHelp on topic 'sigma' was found in the following packages:\n\n  Package               Library\n  glmmTMB               /home/runner/work/_temp/renv/cache/v5/R-4.3/x86_64-pc-linux-gnu/glmmTMB/1.1.10/09c1a82cf7f80e2289aaf8267345dfd1\n  lme4                  /home/runner/work/_temp/renv/cache/v5/R-4.3/x86_64-pc-linux-gnu/lme4/1.1-35.5/16a08fc75007da0d08e0c0388c7c33e6\n  stats                 /opt/R/4.3.1/lib/R/library\n\n\nUsing the first match ...\n\n\nIn addition the overall variance is decreasing, too.\n\n2.3.5.1 More Covariates\nThe model becomes more interesting, when also fixed effects of explanatory variables are included:\n\\[\nY_{ij} = γ_{00} \\\\\n+ γ_{10}x_{1ij} + . . . + γ_{p0}x_{pij} \\\\\n+ γ_{01z1j} + . . . + γ_{0qzqj}\\\\\n+ U_{0j} + R_{ij}\\\\\n\\]\n\n\\(γ_00\\) is zero\n\\(+ γ_10x1ij + . . . + γp0xpij\\) within-group (micro units) differences, for individual variable 1 you have a covariate for the unit belonging to a group. For variable \\(p\\) you have also a covariate for the unit belonging to a group\n\\(+ γ_01z1j + . . . + γ_0qzqj\\) all the between groups (macro units) coefficients,for group variable 1 you have a covariate for the unit belonging to a group. For variable p you have also a covariate for the unit belonging to a group\n\n\n\n\n2.3.6 Between- / Within-Groups\nWhat difference you are interested in?\nDifference between within-group and between-group regressions:\n\nThe within-group regression coefficient (\\(+ γ_10x1ij + . . . + γp_0xpij\\)) is the regression coefficient within each group, assumed to be the same across the groups. → \\(\\sigma^2\\)\nThe between-group regression coefficient (\\(+ γ_01z1j + . . . + γ_0qzqj\\)) is defined as the regression coefficient for the regression of the group means of \\(Y\\) on the group means of \\(X\\).→ \\(\\tau^2\\)\nThis distinction is essential to avoid ecological fallacies → if you ignore, what happen on the lowest level, you will make ecological fallacies\n\n\n\n2.3.7 Ecological fallacy\n(also ecological inference fallacy or population fallacy) is a formal fallacy in the interpretation of statistical data that occurs when inferences about the nature of individuals are deduced from inferences about the group to which those individuals belong.\n\nExample: Inference and generalization mistake: All the French eat snails, what is obviously not true\n\n4 types:\n\nconfusion between ecological (population) correlations and individual correlations\nconfusion between group average and total average\nSimpson’s paradox: as Simpson-Paradoxon (auch simpsonsches Paradoxon oder Simpson’sches Paradoxon, benannt nach Edward Hugh Simpson) ist ein Paradoxon aus der Statistik. Dabei scheint es, dass die Bewertung verschiedener Gruppen unterschiedlich ausfällt, je nachdem ob man die Ergebnisse der Gruppen kombiniert oder nicht. Dieses Phänomen tritt auf, wenn Störvariablen in der statistischen Analyse nicht betrachtet werden. Durch die Nichtbeachtung der Gruppen kommt es zu einer Scheinkorrelation.\nconfusion between higher average and higher likelihood\n\n\n\n\nSimpson Paradox\n\n\n\n\n\nRegression lines\n\n\nIn the model with separate effects for the original variable \\(x_{ij}\\) and the group mean\n\\[\nYij = \\tilde{y}_{00} + \\tilde{y}_{10}\\hat{x}_{ij} + \\tilde{y}_{01}\\hat{x.}_j + U_{0j} + R_{ij}\n\\]\n\nthe within-group regression coefficient is \\(γ_{10}\\)\nbetween-group regression coefficient is \\(γ_{10} + γ_{01}\\)\n\nTest difference between within-group and between-group coefficients: consider \\(\\tilde{γ}_{10}= y_{10}, \\tilde{γ}_{01} = y_{10} + y_{01}\\)\nBoth models are equivalent, and have the same fit:\n\\[\n\\tilde{γ}_{10} = γ_{10}, \\tilde{γ}_{01} = γ_{10} + γ_{01}\n\\]\n\n\n2.3.8 Model fit with R squares\n\\(R^2\\) = 1- (unexplained variance / total variance)\n\nmultilevelR2(mod3, print = c(\"RB1\", \"RB2\", \"SB\", \"MVP\"))\n\n      RB1       RB2        SB       MVP \n0.3491123 0.4556720 0.3735188 0.3365584 \n\n\n\nRB1 and RB2 returns the explained variance at level 1 and level 2, respectively, according to Raudenbush and Bryk (2002, pp. 74 and 79)\nSpecifying SB returns the total variance explained according to Snijders and Bosker (2012, p. 112).\nSpecifying MVP returns the total variance explained based on “multilevel variance partitioning” as proposed by LaHuis, Hartman, Hakoyama, and Clark (2014).\n\nBut note: Don’t infer from \\(R^2\\) if multi level should be used or not! Use the ICC for that\n\n\n2.3.9 Exercise\n\nAdd a group level effect for IQ using the R commands\n\n\ngroupM &lt;- aggregate(DAT$IQ_verb, by=list(DAT$schoolnr), FUN=mean)\ncolnames(groupM) &lt;- c(\"schoolnr\", \"IQ_mean\")\nDAT &lt;- merge(DAT, groupM, by=\"schoolnr\", all.x = TRUE)\n\nWith this command we add a group mean for the verbal IQ per school. With this group mean we separate the effects from each other, So we have the intercept for the school & the verbal IQ mean in school for explaining the between group variance and for the within group variance the verbal IQ\n\nCompare the RIM model with and without this group level variable, with a special focus on the level-2 variance. Explain!\n\n\nmod4 &lt;- lmer(langPOST ~ IQ_verb + IQ_mean + (1|schoolnr), data=DAT)\nsummary(mod3)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: langPOST ~ IQ_verb + (1 | schoolnr)\n   Data: DAT\n\nREML criterion at convergence: 22972.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.1563 -0.6283  0.0789  0.7130  3.1760 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n schoolnr (Intercept) 10.22    3.197   \n Residual             41.14    6.414   \nNumber of obs: 3454, groups:  schoolnr, 211\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept) 4.099e+01  2.497e-01 1.980e+02  164.13   &lt;2e-16 ***\nIQ_verb     2.497e+00  5.725e-02 3.432e+03   43.61   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n        (Intr)\nIQ_verb 0.004 \n\nsummary(mod4)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: langPOST ~ IQ_verb + IQ_mean + (1 | schoolnr)\n   Data: DAT\n\nREML criterion at convergence: 22951.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.1741 -0.6263  0.0696  0.7037  3.1791 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n schoolnr (Intercept)  9.119   3.020   \n Residual             41.096   6.411   \nNumber of obs: 3454, groups:  schoolnr, 211\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept) 4.102e+01  2.389e-01 2.012e+02 171.677  &lt; 2e-16 ***\nIQ_verb     2.438e+00  5.857e-02 3.240e+03  41.634  &lt; 2e-16 ***\nIQ_mean     1.275e+00  2.641e-01 2.616e+02   4.827 2.36e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n        (Intr) IQ_vrb\nIQ_verb  0.000       \nIQ_mean  0.018 -0.222\n\n\nModel 1 (empty model with classes fixed), Model 2 (+ individual intelligence score), Model 3 (+group mean of intelligence score). → more explanation, group mean is significant!\n\nsigma(mod3)\n\n[1] 6.413664\n\nsigma(mod4)\n\n[1] 6.410607\n\n\nThe variance in the random effects between model 3 and 4 is decreasing and more is explained. In mod4 the variance of the residuals and the residual standard deviation is decreasing. Also standard errors are slightly decreasing.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multilevel Data Modelling</span>"
    ]
  },
  {
    "objectID": "multi-level.html#prediction-on-random-effects",
    "href": "multi-level.html#prediction-on-random-effects",
    "title": "2  Multilevel Data Modelling",
    "section": "2.4 Prediction on Random Effects",
    "text": "2.4 Prediction on Random Effects\nonly possible after model computation, because only then we have the estimates, we can predict from!\nThe random effects \\(U_{0j}\\) are not statistical parameters and therefore they are not estimated as part of the estimation routine. However, sometimes it might be necessary to predict them.\nNote: It is often not a good idea, because their are predicted, not statistical facts!\nThere are two ways to get them:\n\nThe ML estimator of \\(U_{0j}\\) is the group means of the residuals \\(Yij − (\\tilde{γ}_{00} + x′{ij} \\hat{γ})\\) → we have estimated the parameters out of the available data. The ML estimator of \\(U_{0j}\\)of the group means of the residuals is possible, because \\(\\tilde{γ}_{00}\\) is equal to zero. (See the Section 2.2.1)\nBy the empirical Bayes method; the resulting predictions are the posterior means. The posterior mean for group j is based on two kinds of information:\n\n\nsample information: the data in group j population information:\nthe value U0j was drawn from a normal distribution with mean 0 and variance τ 2\n\nSuppose we wish to predict the ‘true group mean’ \\(γ_00 + U_{0j}\\) For simplicity consider empty model (without coefficient): \\(Y{ij} = β_0j + R{ij} = γ_00 + U0j + R{ij}\\) → Estimating \\(β_0j\\) and \\(U0j\\) are equivalent problems given that \\(\\hatγ_00\\) is available\nThe empirical Bayes estimate (combined estimate) is a weighted average of the group mean and the overall mean:\n\\[\n\\hat{\\beta}^{EB}_{0j} = λj \\overline{Y}_{.j} + (1 − λj )\\hat{γ_00}\n\\]\nwhere the weight \\(λ_j\\) is the ‘reliability’ of the mean of group \\(j\\) (with \\(n_j\\) is related group size)\n\\[\nλj =\\frac{τ^2_0}{τ 2 0 + σ^2/n_j}\n=\n\\frac{n_jρ_I}{1 + (n_j − 1)ρ_I}\n\\]\nThe reliability coefficient indicates how well the true mean \\(γ_00 + U_{0j}\\) is measured by the observed mean \\(\\overline{Y}_{.j}\\).\n\n\n\nEmpirical Bayes estimators\n\n\nEmpirical Bayes ‘estimates’ (EB) are biased, but the bias gets smaller with increasing group sizes.\nWe see that the EB are “shrunk towards the mean”. (EB are also called shrinkage estimators.)\n\n\n\nError Bayes\n\n\n→ checking for standard errors is very important! Therefore, compute the comparative and diagnostic standard errors.\nComparative (use: comparison of random effects of different level 2 units): * deviation from unobserved random variable \\(U_{0j}\\)\n\\[\nS.E_{comp}(\\hat{U}^{EB}_{0j}) = S.E. (\\hat{U}^{EB}_{0j} - U_{0j})\n\\]\nhow well can the unobserved level-2 contribution of \\(U_{0j}\\) can be ‘estimated’ from the data\nDiagnostic (use: model checking (checking normality of the level-1 residuals))\n\nExpress the deviation from 0, which is the overall mean of the variables \\(U_{0j}\\)\n\n\\[\nS.E_{comp}(\\hat{U}^{EB}_{0j}) = S.E. (\\hat{U}^{EB}_{0j})\n\\]\n\nan be seen as standardized residuals having a std. normal distribution if model is correct\n\n\n2.4.1 Exercise\nEstimates of random effects with ranef. This function gibes the conditional modes of the random effects of a fitted model object. → it is the difference between the population-level average predicted response for a given set of fixed-effects values and the response predicted for a particular individual.\n\nYou can think of these as individual-level effects, i.e. how much does any individual differ from the population?\nThey are also, roughly, equivalent to the modes of the Bayesian posterior densities for the deviations of the individual group effects from the population means\n\n→ important to keep in mind: not parameters, so no standard errors are defined.\n\nPredict the school-level random effects (EB, for all N = 211 schools), using the R command ranef(model) (‘model’ is name of estimated model in R)\n\n\nbe_u &lt;- ranef(mod4)$schoolnr[[1]] # schoolnr[[1]] is the bayes estimate for the level-1 unit for each school, computed in model 4\nplot(1:length(be_u), sort(be_u), pch=20, xlab=\"schoolNr\", ylab=expression('U'['0j'])) #plot all the bayes estimates for the schools sorted from the largest to the highest\nmlm &lt;- lm(sort(be_u) ~ c(1:length(be_u))) ## plot a line: estimate linear model first\nabline(a=coef(summary(mlm))[1,1], b=coef(summary(mlm))[2,1]) ## then use intercept and slope of that model to plot the line\n\n\n\n\n\n\n\n\nOn the vertical axis is the \\(U_{0j}\\) displayed, group-dependent deviation from the average for each school on the x-axis. If a linear regression fit the estimates perfectly, we had perfect normal distribution, because the group dependent variance is around zero.Looking at schools around the median (nearly 50% of the ranked schools), they are centered around \\(U_{0j}\\) → good sign of a random effect.\n\nPlot the confidence intervals to show the differences within the school:\n\n\nplot_model(mod4, type = \"re\", col=\"blue\", \n          title=\"Random Effects on School Level\")\n\n\n\n\n\n\n\n\nThis plot shows the Bayes estimates for each of the 211 schools in the data set and their confidence intervals. We can see, there are varying means for each school and varying confidence intervals. This indicates variance on the school level and supports multi-level analysis.\nImportant: Not a robustness check, more a diagnosis, if the method is appropriate! In the attachment, not in the paper!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multilevel Data Modelling</span>"
    ]
  },
  {
    "objectID": "multi-level.html#random-slope-model",
    "href": "multi-level.html#random-slope-model",
    "title": "2  Multilevel Data Modelling",
    "section": "2.5 Random Slope Model",
    "text": "2.5 Random Slope Model\nIt is possible that not only the group average of Y, but also the effect of X on Y is randomly dependent on the group. That is, in\n\\[\nY_{ij} = β_{0j} + β_{1j}x_{ij} + R_{ij}\n\\]\nit is\n\\[\nβ_{0j} = γ_{00} + U_{0j}\nβ_{1j} = γ_{10} + U_{1j}\n\\]\nExample: The effect of gender has a different effects on the grade in different classes.\nThis gives (X has a random slope):\n\\[\nY_{ij} = γ_{00} + γ_{10}x_{ij} + U_{0j} + U_{1j}x{ij} + R{ij}\n\\]\n\n\\(y_00\\) intercept for overall population \\(γ_{10}x_{ij}\\) fixed effects part = fixed effect on gender!\n\\(U_{0j}\\) random effect of group dependent deviation in the class from the intercept\n\\(U_{1j}x{ij}\\) effect from random effect of class on the fixed effect gender!\n\nBecause we would get too much estimators, we need\nassumptions:\n\ngroup-dependent coefficients (\\(U_{0j}\\) ,\\(U_{1j}\\)) are independent across \\(j\\) with a bivariate normal distribution with expected values (0, 0) and covariance matrix defined byVariance of level-1 units (schools)\n\n\\[\nvar(U_{0j}) = τ_{00} = τ^2_0\n\\] Variance of the effect of level-1 units (random effect) on the individual x effect (IQ)\n\\[\n(U_{1j}) = τ_{11} = τ_{21}\n\\]\nCo-Variance of school number and IQ\n\\[\ncov(U_{0j},U_{1j}) = τ_{01}\n\\]\nThis yields: A linear model for the mean structure, and a parameterized covariance matrix within groups with independence between groups.\n\n2.5.0.1 Exercise\n\nAdd a random slope for IQ to your last model. For this: Have a look in the help function of lme4 package who this works.\n\n\nmod5 &lt;- lmer(langPOST ~  IQ_mean + IQ_verb +  (IQ_verb|schoolnr),  #instead of an intercept (1), we insert IQ_verb as a random slope\n             data=DAT) \nsummary(mod5)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: langPOST ~ IQ_mean + IQ_verb + (IQ_verb | schoolnr)\n   Data: DAT\n\nREML criterion at convergence: 22932.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.2115 -0.6253  0.0740  0.7047  2.7517 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n schoolnr (Intercept)  9.3268  3.0540        \n          IQ_verb      0.1896  0.4354   -0.62\n Residual             40.3709  6.3538        \nNumber of obs: 3454, groups:  schoolnr, 211\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)  41.03652    0.24111 202.40695 170.201  &lt; 2e-16 ***\nIQ_mean       1.03026    0.26543 260.55139   3.882 0.000132 ***\nIQ_verb       2.46418    0.06687 179.41415  36.852  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n        (Intr) IQ_men\nIQ_mean -0.025       \nIQ_verb -0.253 -0.197\n\n\nModel 4 and model 5 have really similar estimates with very similar standard errors and p-values, BUT:\n\nRandom effect variance of groups is in model 4 slightly smaller (9.119, Std. Error 3.02) than in model 5 (9.3268, Std. Error 3.054)\nStandard error in model 4 for IQ_verb(0.058) is smaller than in model 5 (0.06) and so the standard error for the intercept is\nModel 5 does not decrease the variance is not an improvement in model fit. Though, it is more complex.\n\n\nWrite the regression equation for the estimated model.\n\n\\[\nY_{ij} = γ_{00} + γ_{10}x_{ij} + U_{0j} + U_{1j}x{ij} + R{ij}\\\\\nlangPost_{ij}= γ_{00} + γ_{10}*IQverb_{ij} + y_{01}*IQmean_{j} + U_{0j} + U_{1j}*IQmean_{ij} + R_{ij}\\\\\nY_{ij} = 41.03 + 2.4864 *IQverb_{ij} +1.030*IQmean_{j} +U_{0j} + U_{1j}*IQverb_{ij} + R_{ij}\n\\]\n3.What is the average of the slope for IQ, and what is the related standard deviation?\nIn model 5 the average of the slope for IQ is 2.46, it is the estimate for IQ_verb. The standard deviation is:\n\n#Multiply the SE (derived from the fixed effects, SE for the intercept) with the square root of the sample size.\n0.06687 * (sqrt(3454))\n\n[1] 3.929999\n\n\n\nPlot the regression lines for all N=211 schools. What do you find concerning the variance of Y (language test score) related to X (IQ)? (One option for plotting: use the ‘predict’ function for each school separately and plot the predicted Y s along X.)\n\n\nschNr &lt;- unique(DAT$schoolnr)\nplot(0, xlim= range(DAT$IQ_verb), ylim=range(DAT$langPOST), col=\"white\", xlab=\"IQ\", ylab=\"langScore\")\nfor(i in 1: length(schNr)) {\n  pr &lt;- predict(mod5, newdata=DAT[DAT$schoolnr %in% schNr[i],], type=\"response\")\n  iq &lt;- DAT[DAT$schoolnr %in% schNr[i],\"IQ_verb\"]\n  res &lt;- cbind(iq,pr)\n  lines(res[,1], res[,2])\n}\nabline(v=0)\n\n\n\n\n\n\n\n\nIn the plot you can see a regression line for the connection between the verbal IQ and the score in the language test. While for lower verbal IQs there is more variance in the language test score, it is less variance for pupils with higher IQ. This could be an indicator, that pupils with an higher IQ have a higher probability to get higher language test scores, while the scores for pupils with lower IQs are more dependent on whhich school you are.\nThis is a sign for heteroscedacity.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multilevel Data Modelling</span>"
    ]
  },
  {
    "objectID": "multi-level.html#cross-level-interaction",
    "href": "multi-level.html#cross-level-interaction",
    "title": "2  Multilevel Data Modelling",
    "section": "2.6 Cross-Level Interaction",
    "text": "2.6 Cross-Level Interaction\nWhat can we find beyond the random slope?\nWe add level-2 covariates:\nIntercept in the specific group. Is described by the grand mean \\(γ_{00}\\), the random term \\(U_{0j}\\) and by something on the other level \\(γ_{01}z_{j}\\)\n\\[\nβ_{0j} = γ_{00} + γ_{01}z_{j} + U_{0j}\n\\] Coefficient on the slope, the coefficient on the other level and the random term on the other level.\nIs described by the grand mean\\(γ_{10}\\), the random term \\(U_{1j}\\) and by something on the other level \\(γ_{11}z_{j}\\).\n\\[\nβ_{1j} = γ_{10} + γ_{11}z_{j} + U_{1j}\n\\]\nSubstitution yields\n\\(Y{ij} = (y_{00} + y_{01}1z_j + U_{0j})\\) (intercept part) \\(+ (y_10 + y_11z_j + U_{1j})x_{ij}\\) (slope part) + \\(R_{ij} (residual part)\\)\n\\(= y_00\\) (grand mean) \\(+ y_{01}1z_j\\) (fixed effect of the teachers gender on the score)+ \\(y_{10}x_{ij}\\) (fixed effect of the gender slope part) + \\(y_{11}z_{j}x_{ij}\\) (cross-level-interaction effect) fixed effect of the teachers gender + the gender on the student of the score) + \\(U_{0j}\\) (random part) + \\(U_{1j}x_{ij}\\)(random slope part of the gender) + \\(R_{ij}\\) residiual part)\nNote: * Random effects are still in the model, it don’t have to be included, but can! * Two level equation is brought to one long equatation.\n\n2.6.1 Exercise\n\nAdd a cross-level interaction effect for IQ and IQ to your last model. (Hint: Mind that this is still an interaction effect, commonly quantified by the product of two variables.)\n\n\nmod6 &lt;- lmer(langPOST ~  IQ_mean #group level effect\n             + IQ_verb  #individual effect and main predictor\n             + IQ_mean*IQ_verb #interaction effect\n             + (IQ_verb|schoolnr), #group effect on the individual effect\n             data=DAT)\nsummary(mod6)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: langPOST ~ IQ_mean + IQ_verb + IQ_mean * IQ_verb + (IQ_verb |  \n    schoolnr)\n   Data: DAT\n\nREML criterion at convergence: 22929.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.2142 -0.6201  0.0777  0.6992  2.8250 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n schoolnr (Intercept)  9.0486  3.0081        \n          IQ_verb      0.1666  0.4081   -0.68\n Residual             40.4457  6.3597        \nNumber of obs: 3454, groups:  schoolnr, 211\n\nFixed effects:\n                 Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)      41.15728    0.24303 212.95671 169.349  &lt; 2e-16 ***\nIQ_mean           1.11457    0.26469 258.40361   4.211 3.52e-05 ***\nIQ_verb           2.45059    0.06598 176.63512  37.143  &lt; 2e-16 ***\nIQ_mean:IQ_verb  -0.16753    0.06580 173.67334  -2.546   0.0118 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) IQ_men IQ_vrb\nIQ_mean      0.003              \nIQ_verb     -0.270 -0.207       \nIQ_mn:IQ_vr -0.198 -0.154  0.065\n\n\n\nWhat does this effect tell you in the considered case?\n\nIncluding the cross-level interaction effect decreases the variance in the random effects in comparison to model 4 and 5. Especially the variance in IQ_verb is very small now. Its a slightly better model than model 4.\nWhat does the model say us? The higher the IQ mean in a class, the IQ counts less for the score and vice versa.\n\n\n2.6.2 Exercise Model comparision\n\nGlobal intercept model / empty model\n\n\nm1 &lt;- lm(langPOST ~ 1, data=DAT)\nsummary(m1)\n\n\nCall:\nlm(formula = langPOST ~ 1, data = DAT)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.317  -5.317   0.683   6.683  16.683 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  41.3170     0.1521   271.6   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.94 on 3453 degrees of freedom\n\n\nThe intercept in the global intercept model is 41.32. This is the mean score for the sample.\nSee:\n\nDAT %&gt;% summarize(avg=mean(langPOST), sd=sd(langPOST), se= sd/sqrt(3454))\n\n       avg     sd        se\n1 41.31702 8.9398 0.1521131\n\n\n\nTwo Explanatory variables, IQ and ses on level 1\n\n\nm2 &lt;- lm(langPOST ~ IQ_verb + ses, data =DAT)\nsummary(m2)\n\n\nCall:\nlm(formula = langPOST ~ IQ_verb + ses, data = DAT)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-30.0374  -4.3797   0.5262   4.8774  25.2335 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 41.23621    0.11799  349.50   &lt;2e-16 ***\nIQ_verb      2.36855    0.06102   38.81   &lt;2e-16 ***\nses          0.15868    0.01146   13.85   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.933 on 3451 degrees of freedom\nMultiple R-squared:  0.3988,    Adjusted R-squared:  0.3985 \nF-statistic:  1145 on 2 and 3451 DF,  p-value: &lt; 2.2e-16\n\n\nBoth explanatory variables and the intercept are significant. Intercept is near to the overall intercept.\n\nindicates correlation between verbal IQ and language test score: With each verbal IQ point more, the score in the language test will increasee by 2.36.\nindicates correlation between socio economic background and language test score: Whith each point on the ses scale more, the score will increase by 0.15\n\n\ntwo explanatory variables IQ and SES on level-2 (with \\(\\overline{.}\\) denotes the group mean)\n\n\n#add group mean for ses to the data set\ngroupM &lt;- aggregate(DAT$ses, by=list(DAT$schoolnr), FUN=mean)\ncolnames(groupM) &lt;- c(\"schoolnr\", \"SES_mean\")\nDAT &lt;- merge(DAT, groupM, by=\"schoolnr\", all.x = TRUE)\nm3 &lt;- lm(langPOST~  IQ_mean + SES_mean, data=DAT)\nsummary(m3)\n\n\nCall:\nlm(formula = langPOST ~ IQ_mean + SES_mean, data = DAT)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.323  -5.325   0.737   6.478  22.030 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  41.2096     0.1428 288.535  &lt; 2e-16 ***\nIQ_mean       3.3465     0.1970  16.986  &lt; 2e-16 ***\nSES_mean      0.0799     0.0270   2.959  0.00311 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.387 on 3451 degrees of freedom\nMultiple R-squared:  0.1203,    Adjusted R-squared:  0.1198 \nF-statistic: 235.9 on 2 and 3451 DF,  p-value: &lt; 2.2e-16\n\n\nOn level 2 we have significant effects, too. The verbal IQ of a school and the ses mean of school have also significant effects on the language test score.\n→ this indicates, that the higher the verbal IQ or the ses mean on a school is, the higher are the language test scores and vice versa.\n\nAn interaction effect for IQ and SES on level-1\n\n\nm4 &lt;- lm(langPOST ~ IQ_verb + ses + IQ_verb*ses, data = DAT)\nstargazer(m2,m4, type=\"text\", out = \"ml.html\")\n\n\n=========================================================================\n                                     Dependent variable:                 \n                    -----------------------------------------------------\n                                          langPOST                       \n                                (1)                        (2)           \n-------------------------------------------------------------------------\nIQ_verb                      2.369***                   2.338***         \n                              (0.061)                    (0.061)         \n                                                                         \nses                          0.159***                   0.163***         \n                              (0.011)                    (0.011)         \n                                                                         \nIQ_verb:ses                                             -0.022***        \n                                                         (0.005)         \n                                                                         \nConstant                     41.236***                  41.398***        \n                              (0.118)                    (0.124)         \n                                                                         \n-------------------------------------------------------------------------\nObservations                   3,454                      3,454          \nR2                             0.399                      0.402          \nAdjusted R2                    0.398                      0.402          \nResidual Std. Error      6.933 (df = 3451)          6.916 (df = 3450)    \nF Statistic         1,144.808*** (df = 2; 3451) 773.270*** (df = 3; 3450)\n=========================================================================\nNote:                                         *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nOverall, with a look on \\(R^2\\) and the Adjusted \\(R^2\\) model 4 has a slightly better model fit to explain the variance on level one. All explanatory variables are still significant.\nIts very interesting: The positive effect of IQ_verb is slightly decreasing, while the positive effect of ses is slightly increasing, when the interaction term is included. This means, that an increase in ses will decrease the significance and the effect of IQ or vice versa.\n→ It could be a hint, that with with a high level socio economic background a low IQ has less effect or with a low level ses a high verbal IQ has less effect on the test score.\n\nAn interaction effect for IQ and SES on level-2\n\n\nm5 &lt;- lm(langPOST~  IQ_mean + SES_mean + IQ_mean*SES_mean, data=DAT)\nstargazer(m3,m5, type=\"text\", out = \"ml.html\")\n\n\n=======================================================================\n                                    Dependent variable:                \n                    ---------------------------------------------------\n                                         langPOST                      \n                               (1)                       (2)           \n-----------------------------------------------------------------------\nIQ_mean                     3.347***                  3.171***         \n                             (0.197)                   (0.200)         \n                                                                       \nSES_mean                    0.080***                  0.075***         \n                             (0.027)                   (0.027)         \n                                                                       \nIQ_mean:SES_mean                                      -0.114***        \n                                                       (0.023)         \n                                                                       \nConstant                    41.210***                 41.516***        \n                             (0.143)                   (0.155)         \n                                                                       \n-----------------------------------------------------------------------\nObservations                  3,454                     3,454          \nR2                            0.120                     0.126          \nAdjusted R2                   0.120                     0.126          \nResidual Std. Error     8.387 (df = 3451)         8.359 (df = 3450)    \nF Statistic         235.878*** (df = 2; 3451) 166.418*** (df = 3; 3450)\n=======================================================================\nNote:                                       *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nBoth models has a small and \\(R^2\\) in consequence on its own few explanatory power. Though, the model with the interaction term has higher \\(R^2\\), what is an indicator for better model fit.\nAdding the interaction term decreases the effect of the mean IQ and the mean SES in a school. Like on level 1 there is a negative and significant effect of the interaction term.\n→ This one is a bit tough to interpret: If the IQ in one school is on average higher than in another, a higher ses on average has a lower effect on the language test school than in the other school. If the ses in a school on average is very high, the mean IQ on this school will have a lower effect than on a school with lower ses on average.\n\na cross-level interaction effect for \\(IQ\\) and \\(\\overline{IQ}\\)\n\nSee Section 2.6.1 Cross-level interaction:\n\nm6 &lt;- lmer(langPOST ~  IQ_mean #group level effect\n             + IQ_verb  #individual effectof IQ\n             + IQ_mean*IQ_verb #interaction effect\n             + (IQ_verb|schoolnr), #group effect on the individual effect\n             data=DAT)\nsummary(m6)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: langPOST ~ IQ_mean + IQ_verb + IQ_mean * IQ_verb + (IQ_verb |  \n    schoolnr)\n   Data: DAT\n\nREML criterion at convergence: 22929.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.2142 -0.6201  0.0777  0.6992  2.8250 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n schoolnr (Intercept)  9.0486  3.0081        \n          IQ_verb      0.1666  0.4081   -0.68\n Residual             40.4457  6.3597        \nNumber of obs: 3454, groups:  schoolnr, 211\n\nFixed effects:\n                 Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)      41.15728    0.24303 212.95671 169.349  &lt; 2e-16 ***\nIQ_mean           1.11457    0.26469 258.40361   4.211 3.52e-05 ***\nIQ_verb           2.45059    0.06598 176.63512  37.143  &lt; 2e-16 ***\nIQ_mean:IQ_verb  -0.16753    0.06580 173.67334  -2.546   0.0118 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) IQ_men IQ_vrb\nIQ_mean      0.003              \nIQ_verb     -0.270 -0.207       \nIQ_mn:IQ_vr -0.198 -0.154  0.065\n\n\nWe can see that the mean IQ in schools and IQ of an individual have both positive effects on the language score test.\nThe following hypotheses can be deduced from the results:\n\nThe higher the verbal IQ, the higher the probability of a better language score results\nThe higher the IQ is in a school on average, the higher is the probability for pupils to get better language score results. This could be explained by the mechanism, that in school the IQ is high on average, the individual IQ for a majority of pupils is also very high.\nThe cross interaction effect is negative: If the IQ is in its mean higher in one school, there is lower effect for the individual IQ or vice versa.\nThat makes sense, because grades and test results are often relational. If there are pupils with very high IQ in one class, a high IQ is not that outstanding and to get good scores is more difficult.\n\n\nA cross-level interaction effect for \\(ses\\) and \\(\\overline{SES}\\)\n\n\nm7 &lt;- lmer(langPOST ~  SES_mean #group level effect\n             + ses  #individual effect of ses\n             + SES_mean*ses #interaction effect\n             + (ses|schoolnr), #group effect on the individual effect\n             data=DAT)\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.00205268 (tol = 0.002, component 1)\n\nsummary(m7)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: langPOST ~ SES_mean + ses + SES_mean * ses + (ses | schoolnr)\n   Data: DAT\n\nREML criterion at convergence: 24016.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.0743 -0.6023  0.0655  0.7254  3.0281 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev. Corr \n schoolnr (Intercept) 13.989984 3.740         \n          ses          0.003248 0.057    -0.95\n Residual             55.270465 7.434         \nNumber of obs: 3454, groups:  schoolnr, 211\n\nFixed effects:\n               Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)   41.254904   0.306093 229.665686 134.779   &lt;2e-16 ***\nSES_mean       0.036492   0.048370 245.391134   0.754   0.4513    \nses            0.310211   0.014904 175.009073  20.813   &lt;2e-16 ***\nSES_mean:ses  -0.005794   0.002390 211.679345  -2.424   0.0162 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) SES_mn ses   \nSES_mean     0.033              \nses         -0.168 -0.302       \nSES_mean:ss -0.305 -0.161 -0.162\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.00205268 (tol = 0.002, component 1)\n\n\nWe can see that the mean SES in schools and the ses of an individual have both positive effects on the language score test, but only the individual ses is significant and moderate high. The cross interaction effect is negative and very small.\n\nThe higher the ses of a pupil, the higher the probability to get good test scores.\nIf the school has on average a high the ses, the effect of the individual on test scores decreases. This makes sense, because like intelligence the socio-economic background is perceived relational in a school.\n\n\na cross-level interaction effect for \\(IQ\\) and \\(\\overline{SES}\\)\n\n\nm8 &lt;- lmer(langPOST ~  SES_mean #group level effect\n             + IQ_verb  #individual effect of IQ\n             + SES_mean*IQ_verb #interaction effect of the SES in a school on the individual IQ\n             + (IQ_verb|schoolnr), #group effect  on the individual effect\n             data=DAT)\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.00241126 (tol = 0.002, component 1)\n\nsummary(m8)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: langPOST ~ SES_mean + IQ_verb + SES_mean * IQ_verb + (IQ_verb |  \n    schoolnr)\n   Data: DAT\n\nREML criterion at convergence: 22941.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.2273 -0.6109  0.0784  0.7108  2.7610 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n schoolnr (Intercept)  9.3370  3.0557        \n          IQ_verb      0.1787  0.4228   -0.71\n Residual             40.4293  6.3584        \nNumber of obs: 3454, groups:  schoolnr, 211\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)       41.136846   0.242649 205.668782 169.533  &lt; 2e-16 ***\nSES_mean           0.150259   0.038448 215.849463   3.908 0.000125 ***\nIQ_verb            2.478319   0.065495 172.593979  37.840  &lt; 2e-16 ***\nSES_mean:IQ_verb  -0.021017   0.009911 153.790000  -2.120 0.035571 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) SES_mn IQ_vrb\nSES_mean     0.048              \nIQ_verb     -0.299 -0.119       \nSES_mn:IQ_v -0.115 -0.260  0.065\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.00241126 (tol = 0.002, component 1)\n\n\nNot controlled for individual ses, we have a positive effect of the ses on school level and a positive effect of the individual IQ, without controlled for school level IQ. Both effects and the cross level interaction effect is significant. The cross level interaction effect is negative.\n\nThe higher the ses of pupils on one school is on average, the higher the probability to get high test results (not controlled for individual values of ses)\nThe higher the the IQ, the higher the probability to get high test results (not controlled for group level values of IQ)\nIf on a school the average ses is high, the effect of individual IQ on the probablity to get high test results are decreasing and vice versa\n\n\nA cross-level interaction effect for \\(ses\\) and \\(\\overline{IQ}\\)\n\n\nm9 &lt;- lmer(langPOST ~  IQ_mean #group level effect\n             + ses  #individual effect of ses\n             + ses*IQ_mean #interaction effect of the IQ n a school on the individual ses\n             + (ses|schoolnr), #group effect  on the individual effect\n             data=DAT)\n\nboundary (singular) fit: see help('isSingular')\n\nsummary(m9)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: langPOST ~ IQ_mean + ses + ses * IQ_mean + (ses | schoolnr)\n   Data: DAT\n\nREML criterion at convergence: 23935.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.2166 -0.6165  0.0820  0.7182  3.0790 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev. Corr \n schoolnr (Intercept) 8.870e+00 2.97819       \n          ses         5.278e-04 0.02297  -1.00\n Residual             5.529e+01 7.43601       \nNumber of obs: 3454, groups:  schoolnr, 211\n\nFixed effects:\n              Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)   41.21654    0.25065  211.67686 164.440  &lt; 2e-16 ***\nIQ_mean        2.51955    0.27462  260.13025   9.175  &lt; 2e-16 ***\nses            0.28517    0.01357 2848.76635  21.022  &lt; 2e-16 ***\nIQ_mean:ses   -0.04940    0.01623 2610.71459  -3.045  0.00235 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) IQ_men ses   \nIQ_mean     -0.018              \nses         -0.076 -0.196       \nIQ_mean:ses -0.200  0.075 -0.107\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nThere are positive, significant effect of the mean IQ in a school and the individual ses on the language test score and a negative, significant cross level interaction effect.\n\nThe higher the IQ of pupils on one school is on average, the higher the probability to get high test results (not controlled for individual values of IQ)\nThe higher the the individual ses, the higher the probability to get high test results (not controlled for group level values of ses)\nIf on a school the average IQ is high, the effect of individual ses on the probability to get high test results are decreasing and vice versa\n\n\na random intercept on school level and a random slope for IQ\n\nSee Section 2.5.0.1 Random Slope Model:\n\nm10 &lt;- lmer(langPOST ~  IQ_mean + IQ_verb +  (1+IQ_verb|schoolnr),  #instead of an intercept (1), we insert IQ_verb as a random slope\n             data=DAT) \nsummary(m10)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: langPOST ~ IQ_mean + IQ_verb + (1 + IQ_verb | schoolnr)\n   Data: DAT\n\nREML criterion at convergence: 22932.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.2115 -0.6253  0.0740  0.7047  2.7517 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n schoolnr (Intercept)  9.3268  3.0540        \n          IQ_verb      0.1896  0.4354   -0.62\n Residual             40.3709  6.3538        \nNumber of obs: 3454, groups:  schoolnr, 211\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)  41.03652    0.24111 202.40695 170.201  &lt; 2e-16 ***\nIQ_mean       1.03026    0.26543 260.55139   3.882 0.000132 ***\nIQ_verb       2.46418    0.06687 179.41415  36.852  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n        (Intr) IQ_men\nIQ_mean -0.025       \nIQ_verb -0.253 -0.197\n\n\nIn this model not only the difference in the intercept is random, but also the slope. The slope is determined by the individual IQ, too.\nThis means, that not only the individual IQ and the IQ on school level has an effect on the test results, the effect of IQ has different effects on different classes.\nIf I had to choose for one of these models, I would go with the last one. For me, it is a bit neglectful to measure cross level interactions without taken individual effects for the explanatory variables and the group level effects into account. This is especially true for model 9. Model 6 give hints, that the individual effect of IQ is critical for better scores and is weakened by the the mean IQ. Consequently, to test, if the mean IQ is explanatory for better language test results without regarding the individual level appears a bit strange and contradictory. What assumptions should derive from that? That an overall higher IQ level in a school is a cause for better language tests for the individuals? It is a bit far away, to assume, that pupils with higher verbal IQs help the others out and therefore language test results are better for the pupils in the schools, than to assume, that the individual IQ level at a school and a school with a lot high level IQ pupils will have better test results than a school with less high level IQ pupils.\nThe last one has also the lowest variance in the residuals of random effects.\nAnother option is to compute random intercept models and random slope models, combining SES and IQ as explanatory variables. Like always, it depends on theory, which model makes sense and then test for statistical inference. For example behind model 4 lays the assumption, that if you have a higher ses, your IQ is less important for test results.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multilevel Data Modelling</span>"
    ]
  },
  {
    "objectID": "multi-level.html#statistical-testing",
    "href": "multi-level.html#statistical-testing",
    "title": "2  Multilevel Data Modelling",
    "section": "2.7 Statistical Testing",
    "text": "2.7 Statistical Testing\nFor the fixed parts, the \\(\\gamma\\) use the t-test with test statistic (compares the mean between groups):\nThe estimates comes from the sample data and are in consequence not the true scores → estimates are dependent on data and not taken for true\n\\[\nT(γh) =\n\\frac{\\hat{γ}}\n{S.E.(\\hat{γ}h)}\n\\]\n(The F-test (test for variance between samples or groups) or Wald test for testing several parameters simultaneously.) The standard error should be based on REML estimation. → robust estimator and really recommended.\nDegrees of freedom for the t-test / the denominator of the F-test\n\nFor a level-1 variable: \\(N − r − 1\\), where N = total number of level-1 units, r = number of tested level-1 variables\nFor a level-2 variable: \\(M − q − 1\\), where M = number of level-2 units, q = number of tested level-2 variables\nFor a cross-level interaction: M − q − 1, where q = number of other level-2 variables interacting with the level-1 variable\nNote: If d.f. ≥ 40, the t-distribution can be replaced by a standard normal\n\n→ the more variables I add, the less power my model is going to have\n\n2.7.1 Exercise\nExtended model like in the R script:\n\nmod7 &lt;- lmer(langPOST ~  IQ_verb + ses + \n                         IQ_mean + SES_mean +  \n                         IQ_verb*ses +\n                         IQ_mean*SES_mean +\n                         IQ_mean*IQ_verb + \n                         SES_mean*ses + \n                         IQ_mean*ses + \n                         SES_mean*IQ_verb +                          \n                      + (IQ_verb|schoolnr), data=DAT)\nsummary(mod7)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: langPOST ~ IQ_verb + ses + IQ_mean + SES_mean + IQ_verb * ses +  \n    IQ_mean * SES_mean + IQ_mean * IQ_verb + SES_mean * ses +  \n    IQ_mean * ses + SES_mean * IQ_verb + +(IQ_verb | schoolnr)\n   Data: DAT\n\nREML criterion at convergence: 22763.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.2346 -0.6159  0.0738  0.6964  2.8555 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n schoolnr (Intercept)  8.9260  2.9876        \n          IQ_verb      0.1754  0.4189   -0.76\n Residual             38.0150  6.1656        \nNumber of obs: 3454, groups:  schoolnr, 211\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)       4.152e+01  2.669e-01  2.362e+02 155.570  &lt; 2e-16 ***\nIQ_verb           2.217e+00  6.676e-02  1.957e+02  33.204  &lt; 2e-16 ***\nses               1.751e-01  1.253e-02  3.258e+03  13.978  &lt; 2e-16 ***\nIQ_mean           7.868e-01  3.141e-01  2.356e+02   2.505  0.01292 *  \nSES_mean         -8.495e-02  4.589e-02  2.353e+02  -1.851  0.06544 .  \nIQ_verb:ses      -1.459e-02  6.690e-03  3.263e+03  -2.181  0.02925 *  \nIQ_mean:SES_mean -1.227e-01  3.842e-02  4.195e+02  -3.194  0.00151 ** \nIQ_verb:IQ_mean  -5.289e-02  8.486e-02  2.332e+02  -0.623  0.53369    \nses:SES_mean      5.579e-04  2.315e-03  3.066e+03   0.241  0.80953    \nses:IQ_mean       1.154e-02  1.847e-02  3.343e+03   0.625  0.53224    \nIQ_verb:SES_mean  7.266e-04  1.426e-02  3.081e+02   0.051  0.95941    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) IQ_vrb ses    IQ_men SES_mn IQ_vr: IQ_m:SES_ IQ_:IQ s:SES_\nIQ_verb     -0.269                                                           \nses          0.036 -0.244                                                    \nIQ_mean     -0.096 -0.176  0.051                                             \nSES_mean     0.037  0.055 -0.267 -0.510                                      \nIQ_verb:ses -0.131  0.042 -0.085  0.014 -0.004                               \nIQ_mn:SES_m -0.327 -0.030  0.066  0.207 -0.022  0.136                        \nIQ_vrb:IQ_m -0.097  0.028  0.027 -0.257  0.148 -0.002 -0.148                 \nses:SES_men -0.236 -0.009 -0.147  0.018  0.009  0.073 -0.076     0.152       \nses:IQ_mean  0.151  0.018 -0.047 -0.023  0.032 -0.310 -0.313    -0.222 -0.419\nIQ_vrb:SES_  0.132  0.013  0.030  0.110 -0.245 -0.497 -0.105    -0.490 -0.238\n            ss:IQ_\nIQ_verb           \nses               \nIQ_mean           \nSES_mean          \nIQ_verb:ses       \nIQ_mn:SES_m       \nIQ_vrb:IQ_m       \nses:SES_men       \nses:IQ_mean       \nIQ_vrb:SES_  0.238\n\n\n\n& 2. Based on a t-test: Which explanatory variables should stay in the model?, Based on a t-test: Which interaction effects should stay in the model?\n\nSignificance indicated by p can be taken out of the models → using the p-values from the t-test.\nThe lmertest test the linear models and give the p-values for the explanatory variables. It computes for fixed effects the significance with t-tests. Looking at the results displayed in the summary the following variables and interaction effects should stay in the model:\neffects on individual levels\n\nIQ_verb\nses\n\neffects - IQ_mean - SES_mean\ncross-interaction effects: - IQ_verb:ses - IQ_mean:SES_mean\n\nReduce your model according to your findings.\n\n\nmod8 &lt;- lmer(langPOST ~  IQ_verb + ses + \n               IQ_mean + SES_mean +  \n               IQ_verb*ses +\n               IQ_mean*SES_mean +\n               (IQ_verb|schoolnr), data=DAT)\nsummary(mod8)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: langPOST ~ IQ_verb + ses + IQ_mean + SES_mean + IQ_verb * ses +  \n    IQ_mean * SES_mean + (IQ_verb | schoolnr)\n   Data: DAT\n\nREML criterion at convergence: 22737.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.2351 -0.6167  0.0817  0.6937  2.8556 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n schoolnr (Intercept)  8.9302  2.9883        \n          IQ_verb      0.1683  0.4102   -0.77\n Residual             37.9938  6.1639        \nNumber of obs: 3454, groups:  schoolnr, 211\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)       4.151e+01  2.582e-01  2.093e+02 160.802  &lt; 2e-16 ***\nIQ_verb           2.217e+00  6.638e-02  1.982e+02  33.403  &lt; 2e-16 ***\nses               1.770e-01  1.228e-02  3.242e+03  14.418  &lt; 2e-16 ***\nIQ_mean           7.452e-01  3.027e-01  2.203e+02   2.462 0.014580 *  \nSES_mean         -8.549e-02  4.429e-02  2.519e+02  -1.930 0.054699 .  \nIQ_verb:ses      -1.439e-02  5.171e-03  9.880e+02  -2.783 0.005491 ** \nIQ_mean:SES_mean -1.164e-01  3.378e-02  2.680e+02  -3.445 0.000662 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) IQ_vrb ses    IQ_men SES_mn IQ_vr:\nIQ_verb     -0.278                                   \nses          0.008 -0.251                            \nIQ_mean     -0.113 -0.174  0.062                     \nSES_mean     0.060  0.057 -0.273 -0.504              \nIQ_verb:ses -0.096  0.084 -0.122 -0.016 -0.127       \nIQ_mn:SES_m -0.383 -0.014  0.024  0.173 -0.014 -0.133\n\n\nIn the reduced model all variables are still significant. SES_mean has only a significance level of 0.055, so we could think about kicking it out.\n→ The individual IQ has a positive, significant effect on the score. If the IQ is one point higher, the language score result will probably increase by 2.17. Also the ses has a positive, significant effect on the score.If the ses increase by one point, it increase 0.17.\n→ If the average IQ on a school is higher, than the language score test are probably higher, too. And if the average SES is higher on a school, the score will increase, too.\n→ There is an interaction effect between IQ and ses on individual and school level: The positive, significant effect of IQ is damped by the ses or vice versa. A low level in ses dampens the positive effect of an high IQ on a language test score. OR: A higher level in ses compensate low levels in IQ.\n→ There is also an interaction effect on the school level. This could be, because of the strong correlation between the IQ_mean and SES_mean.\nModel 8 has compared to all other models the smallest variance on school level and in the residuals and consequently, will have the a higher ICC than the other models.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multilevel Data Modelling</span>"
    ]
  },
  {
    "objectID": "multi-level.html#testing-random-effects-with-the-lieklihood-ratio-test-lrt",
    "href": "multi-level.html#testing-random-effects-with-the-lieklihood-ratio-test-lrt",
    "title": "2  Multilevel Data Modelling",
    "section": "2.8 Testing random effects with the lieklihood-ratio test (LRT)",
    "text": "2.8 Testing random effects with the lieklihood-ratio test (LRT)\nDifference between linear regression and HLM is in the \\(U_{ij}\\), the \\(\\tau^2\\)\n\nAssesses goodness of fit of two nested statistical models (model 1 and model 2) based on the ratio of their likelihoods\nModels are considered as nested, when one model is a restricted or constrained version of another model\nData is fixed, we can compute different models. The likelihood Λ gives the probability that the parameters fits the distribution of the sample, comparing different models\nModel 2 has parameter space \\(θ\\) ∈ \\(Θ\\), with \\(θ\\) is unknown / to be estimated\n\nH0: \\(Θ\\) (the complexer model 2) is in subset \\(Θ_0\\) of \\(Θ\\)\n\nSimpler model 1 has parameter space \\(Θ_0\\), more complex model 2 parameter space \\(Θ\\)\nNull Hypothesis: Model 2 are a subset of the linear regression, therefore model 2 and \\(U_{ij}\\) is not necessary → the more complexer model is equal to the simpler model\nMore complex model 2 can be transformed into simpler model 1 by imposing constraints on model’s 2 parameters: model 1 is nested in model 2 → model 1 is nested into model 2, while model 2 is tested, because in model 1 are parameters set to 0 and consequently its simpler\n\nUsually: model 2 found by maximization over the entire parameter space and model 1 found after imposing some constraint\n\n→ key question: what is tested (model 2) and what is nested (model 1)?\nNote: Model 1 doesn’t have to be a linear regression, could also be a simpler HLM compared to a more complex model 2\nNote: If two models are nested, then we have the most options in terms of comparing the two models. For example, we can evaluate whether Model A is a statistically significantly better fit than is Model B using a Likelihood Ratio Test (LRT)\n\nLR test tests whether related likelihood ratio is significantly different from one\nLR test statistic is the deviance difference:\n\n\\[\nλ_{LR} = −2 ln\n\\frac{supθ∈Θ0Λ(θ)}\n{supθ∈ΘΛ(θ)}\n\\]\n\\[\nλ_{LR} = −2[l(θ_0) − l\\hat{θ})]\n\\]\nwhere \\(l(\\hatθ)\\) is the logarithm of the maximized likelihood function \\(Λ\\) (under model 2), and \\(l(θ_0)\\) is the maximal value if H0 is true (i.e. under model 1).\nHigher likelihood means better fit of the data.\n\n\n\nLikelihoood Ratio\n\n\nThe deviance difference is the difference between the highest points between the two curves. In this case it is 2. If the difference is zero, we don’t need the more complex model and can reject H0.\nH0 is true means that model 2 (tested model) is not sign. better in fitting the data than model 1 (nested model).\nMultiplication with −2 ensures \\(λ_{LR}\\) assym. converges to \\(χ^2\\) distribution under H0\nSmall p-value (related to \\(χ^2\\) distr.) indicates that H0 is wrong, i.e. evidence for model 2\n\n2.8.0.1 Deviances and \\(\\lambda_{LR}\\)\nWe have (with \\(\\hat{θ}\\) denotes the fitted parameters for the saturated model(include more or too much parameter):\n\nDeviance model 1: \\(D1 = −2(l(\\hatθ_s ) − l(\\hatθ_1))\\) with \\(\\hatθ_1\\) fitted parameters of model 1, \\(|θ_1 = m1\\)\nDeviance model 2: \\(D2 = −2(l(\\hatθ_s ) − l(\\hatθ_2))\\) with \\(\\hatθ_2\\) fitted parameters of model 2, $|θ_2| = m2, m1 &lt; m2 $\n\nThen: The test statistic for testing model 2 against model 1 is:\n\\[\n\\hat{\\lambda}_{LR} = −2(l(\\hatθ_2) − l(\\hatθ_1)) = D1 − D2\n\\]\nWhen the deviance is zero or near to zero, we don’t need a multilevel model.\nIf the null hypothesis of no difference is true in the population, then \\(λ\\) will follow a chi-squared distribution with degrees of freedom equal to the number of parameters constrained to 0 in Model 1 from Model 2, the difference in degrees of freedom used for each model, that is:\n\\[\n\\lambda  \\sim \\chi^2(df1-df2)\n\\]\nThus we often use a chi-square distribution in the LRT to look up the p-value. Finally, note that because LRTs are based on the log likelihoods (LL) from a model, we need true log likelihoods for the LRT to be valid. Therefore, we cannot use restricted maximum likelihood, we need to use maximum likelihood estimation.\nOne special circumstance: variance parameters are necessarily positive. Therefore, they may be tested one-sided.\nE.g., in the random intercept model: \\(H_0 : τ^ 2_0 = 0\\) Asymptotic distribution of the deviance difference is a mixture of a point mass at 0 (with prob. 0.5) and a \\(χ^2\\) [distribution (with prob. 0.5) Interpretation: if observed between-group variance ≤ as expected under H0 (which happens with prob 0.5) the estimate is \\(τ6 2 0 = 0\\) and the log-likelihood ratio is 0 The test works as follows:\n\nif deviance difference \\(= 0\\): no significance\nif deviance difference \\(&gt; 0\\): calculate p-value from \\(X^2_1\\) and divide by 2\n\nd stands for the degrees of freedom → the parameters you have added\n\n\n\nchisquared distribution",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multilevel Data Modelling</span>"
    ]
  },
  {
    "objectID": "multi-level.html#random-slope-variances",
    "href": "multi-level.html#random-slope-variances",
    "title": "2  Multilevel Data Modelling",
    "section": "2.9 Random Slope Variances",
    "text": "2.9 Random Slope Variances\nAs before: LR test Number of tested parameters (variances & covariances): \\(d + 1\\) → because another parameter is added to model 2\nE.g.: testing for a random slope in a model that further contains the random intercept but no other random slopes: \\(d = 1\\)\n\np-values can be obtained as the average of the p-values for the \\(χ^2_d\\) and \\(χ^2_{d+1}\\) distributions\n\n\n\n\nP-values for random slopes\n\n\n\n2.9.0.1 Exercise\nData (as before) Tasks: Which model does give the better model fit? Test it!\n\n#Linear Model, simple model\nmod9a &lt;- lm(langPOST ~  IQ_verb + ses + \n               IQ_mean + SES_mean +  \n               IQ_verb*ses +\n               IQ_mean*SES_mean, data=DAT)\nsummary(mod9a)\n\n\nCall:\nlm(formula = langPOST ~ IQ_verb + ses + IQ_mean + SES_mean + \n    IQ_verb * ses + IQ_mean * SES_mean, data = DAT)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-30.0910  -4.4280   0.6795   4.8196  24.0581 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      41.580399   0.129703 320.583  &lt; 2e-16 ***\nIQ_verb           2.194084   0.065389  33.554  &lt; 2e-16 ***\nses               0.176449   0.013577  12.996  &lt; 2e-16 ***\nIQ_mean           0.962597   0.176064   5.467 4.89e-08 ***\nSES_mean         -0.099739   0.025883  -3.853 0.000119 ***\nIQ_verb:ses      -0.013872   0.005336  -2.600 0.009375 ** \nIQ_mean:SES_mean -0.099846   0.019623  -5.088 3.81e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.852 on 3447 degrees of freedom\nMultiple R-squared:  0.4135,    Adjusted R-squared:  0.4125 \nF-statistic: 405.1 on 6 and 3447 DF,  p-value: &lt; 2.2e-16\n\n#Random Intercept, more complex model\nmod9b &lt;- lmer(langPOST ~  IQ_verb + ses + \n               IQ_mean + SES_mean +  \n               IQ_verb*ses +\n               IQ_mean*SES_mean +\n               (1|schoolnr), data=DAT)\nsummary(mod9b)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: langPOST ~ IQ_verb + ses + IQ_mean + SES_mean + IQ_verb * ses +  \n    IQ_mean * SES_mean + (1 | schoolnr)\n   Data: DAT\n\nREML criterion at convergence: 22761.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.2028 -0.6359  0.0735  0.6928  3.1440 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n schoolnr (Intercept)  8.655   2.942   \n Residual             38.645   6.217   \nNumber of obs: 3454, groups:  schoolnr, 211\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)       4.143e+01  2.550e-01  2.034e+02 162.453  &lt; 2e-16 ***\nIQ_verb           2.193e+00  5.933e-02  3.239e+03  36.965  &lt; 2e-16 ***\nses               1.767e-01  1.232e-02  3.239e+03  14.340  &lt; 2e-16 ***\nIQ_mean           9.135e-01  3.153e-01  2.284e+02   2.898  0.00413 ** \nSES_mean         -9.145e-02  4.538e-02  2.345e+02  -2.015  0.04502 *  \nIQ_verb:ses      -1.463e-02  4.929e-03  3.302e+03  -2.969  0.00301 ** \nIQ_mean:SES_mean -8.972e-02  3.325e-02  2.620e+02  -2.699  0.00742 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) IQ_vrb ses    IQ_men SES_mn IQ_vr:\nIQ_verb     -0.009                                   \nses          0.010 -0.282                            \nIQ_mean     -0.117 -0.187  0.051                     \nSES_mean     0.028  0.075 -0.269 -0.494              \nIQ_verb:ses -0.092  0.096 -0.114 -0.002  0.013       \nIQ_mn:SES_m -0.381 -0.014  0.016  0.277  0.020 -0.144\n\n#, Random Slope, most complex model\nmod9c &lt;- lmer(langPOST ~  IQ_verb + ses + \n               IQ_mean + SES_mean +  \n               IQ_verb*ses +\n               IQ_mean*SES_mean +\n               (IQ_verb|schoolnr), data=DAT)\nsummary(mod9c)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: langPOST ~ IQ_verb + ses + IQ_mean + SES_mean + IQ_verb * ses +  \n    IQ_mean * SES_mean + (IQ_verb | schoolnr)\n   Data: DAT\n\nREML criterion at convergence: 22737.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.2351 -0.6167  0.0817  0.6937  2.8556 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n schoolnr (Intercept)  8.9302  2.9883        \n          IQ_verb      0.1683  0.4102   -0.77\n Residual             37.9938  6.1639        \nNumber of obs: 3454, groups:  schoolnr, 211\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)       4.151e+01  2.582e-01  2.093e+02 160.802  &lt; 2e-16 ***\nIQ_verb           2.217e+00  6.638e-02  1.982e+02  33.403  &lt; 2e-16 ***\nses               1.770e-01  1.228e-02  3.242e+03  14.418  &lt; 2e-16 ***\nIQ_mean           7.452e-01  3.027e-01  2.203e+02   2.462 0.014580 *  \nSES_mean         -8.549e-02  4.429e-02  2.519e+02  -1.930 0.054699 .  \nIQ_verb:ses      -1.439e-02  5.171e-03  9.880e+02  -2.783 0.005491 ** \nIQ_mean:SES_mean -1.164e-01  3.378e-02  2.680e+02  -3.445 0.000662 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) IQ_vrb ses    IQ_men SES_mn IQ_vr:\nIQ_verb     -0.278                                   \nses          0.008 -0.251                            \nIQ_mean     -0.113 -0.174  0.062                     \nSES_mean     0.060  0.057 -0.273 -0.504              \nIQ_verb:ses -0.096  0.084 -0.122 -0.016 -0.127       \nIQ_mn:SES_m -0.383 -0.014  0.024  0.173 -0.014 -0.133\n\n\nThe following steps for LRT are from this website\nCheck that all models are fitted to the same data and the same observations are included:\n\nnobs(mod9a)\n\n[1] 3454\n\nnobs(mod9b)\n\n[1] 3454\n\nnobs(mod9c)\n\n[1] 3454\n\n\nGet the log Likelihoods for the models:\n\nlogLik(mod9a) # simple model\n\n'log Lik.' -11545.01 (df=8)\n\nlogLik(mod9b) # more complex model\n\n'log Lik.' -11380.82 (df=9)\n\nlogLik(mod9c) # most complex model\n\n'log Lik.' -11368.69 (df=11)\n\n\nCompute lambda for mod9a and mod9b\n\n#lamdba for more complex model (nested) and most complex model (tested)\n-2* (-11545.01 - -11368.69)\n\n[1] 352.64\n\n\n\n# p-value from a chi-square\npchisq(352.64, df= 1, lower.tail= FALSE)\n\n[1] 1.127842e-78\n\n\n→ reject the H0, go with the more complex model\nCompute lambda for mod9b and mod9c\n\n#lamdba for more complex model (nested) and most complex model (tested)\n-2* (-11380.82 - -11368.69)\n\n[1] 24.26\n\n\n\n# p-value from a chi-square, in this case 2 df, because model 9b has 9 and model 9c has 11\npchisq(24.26, df= 2, lower.tail= FALSE)\n\n[1] 5.395205e-06\n\n\n→ also in this case reject the H0, go with the more complex model\nIn practice, we don’t have to do it by hand, we can use the ranova function.\nThe first test compares the reduction from the random intercept to not include a random intercept like in mod9a, the random intercept is removed (see for details the description for the ranova function)\n\nranova(mod9b)\n\nANOVA-like table for random-effects: Single term deletions\n\nModel:\nlangPOST ~ IQ_verb + ses + IQ_mean + SES_mean + (1 | schoolnr) + IQ_verb:ses + IQ_mean:SES_mean\n               npar logLik   AIC    LRT Df Pr(&gt;Chisq)    \n&lt;none&gt;            9 -11381 22780                         \n(1 | schoolnr)    8 -11563 23142 363.96  1  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nranova(mod9c)\n\nANOVA-like table for random-effects: Single term deletions\n\nModel:\nlangPOST ~ IQ_verb + ses + IQ_mean + SES_mean + (IQ_verb | schoolnr) + IQ_verb:ses + IQ_mean:SES_mean\n                                npar logLik   AIC   LRT Df Pr(&gt;Chisq)    \n&lt;none&gt;                            11 -11369 22759                        \nIQ_verb in (IQ_verb | schoolnr)    9 -11381 22780 24.27  2  5.368e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n#other option:\nanova(mod9b, mod9c, refit=FALSE)\n\nData: DAT\nModels:\nmod9b: langPOST ~ IQ_verb + ses + IQ_mean + SES_mean + IQ_verb * ses + IQ_mean * SES_mean + (1 | schoolnr)\nmod9c: langPOST ~ IQ_verb + ses + IQ_mean + SES_mean + IQ_verb * ses + IQ_mean * SES_mean + (IQ_verb | schoolnr)\n      npar   AIC   BIC logLik deviance Chisq Df Pr(&gt;Chisq)    \nmod9b    9 22780 22835 -11381    22762                        \nmod9c   11 22759 22827 -11369    22737 24.27  2  5.368e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n→ also in this case reject the H0, go with the most complex model",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multilevel Data Modelling</span>"
    ]
  },
  {
    "objectID": "multi-level.html#explained-variance",
    "href": "multi-level.html#explained-variance",
    "title": "2  Multilevel Data Modelling",
    "section": "2.10 Explained Variance",
    "text": "2.10 Explained Variance\nIn ordinary linear models we compute \\(R^2\\) for model \\(ω\\) as the proportionate reduction in the RSS (residual sum of squares) starting from the null model \\(ϕ\\):\n\\[\nR2 = 1 − \\frac{RSS(ω)}{RSS(ϕ)}\n\\]\nIn a two level random intercept model we can define \\(R^2\\) (with a total model variance of \\(σ^2 + τ^2_0\\)):\n\\[\nR^2 = 1 − \\frac{\\hatσ^2(ω) + τ^2_0(ω)}{\\hatσ^2(ϕ) + τ^2_0(ϕ)}\n\\]\nThis can also be calculated by level:\nFor macro level 2:\n\\[\nR^2_{L2}= 1- \\frac{ τ^2_0(ω)}{ τ^2_0(ϕ)}\n\\] For micro level 1:\n\\[\nR^2_{L1} = 1 − \\frac{\\hatσ^2(ω) }{\\hatσ^2(ϕ) }\n\\] Note: Unlike linear models R2 is not guaranteed to increase when variables are added!\n\n2.10.1 Exercise\nYou have the following models and the related variance estimates: Note: Model A. corresponds to the null model; Model D. to the substantial model.\n\nCompute \\(R^2\\), \\(R^2_{L1}\\), \\(R^2_{L2}\\)\n\n# Level 1\n1 - (6.973/8.694)\n\n[1] 0.1979526\n\n# Level 2\n1 - (0.991/2.271)\n\n[1] 0.5636284\n\n# R2\n1 - ((6.973+0.991)/(8.694+2.271))\n\n[1] 0.273689\n\n\nOther way to do it in R is with\nlibrary(mitml)\nmultilevelR2(mod, print=c(\"RB1\",  #explained variance on level 1\n\"RB2\", #explained variance on level 2 both according to Raudenbuch and Bryk (2002, pp. 74 and 79)\n\"SB\", #total variance explained according ot Snijders and Bosker (2012, p.112)\nMVP\") #total variance explained based on multilevel variance partioning by LaHUis, Hakoyama and Clark (2014)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multilevel Data Modelling</span>"
    ]
  },
  {
    "objectID": "multi-level.html#heteroscedasticity",
    "href": "multi-level.html#heteroscedasticity",
    "title": "2  Multilevel Data Modelling",
    "section": "2.11 Heteroscedasticity",
    "text": "2.11 Heteroscedasticity\nHeteroscedasticity can be modelled in ML-Models, i.e. residual variance depends on observed variables.\nE.g., random part at level one \\(= R_{0ij} + R_{1ij}x_{1ij}\\)\nHere the level-1 variance is a quadratic function of \\(X\\):\n\\[\nvar (R_{0ij} + R_{1ij}x_{1ij} ) = σ^2_0 + 2σ_{01}x_{1ij} + σ^2_1x^2_{1ij}\n\\]\nFor \\(σ^21 = 0\\), this is a linear function:\n\\[\nvar (R_{0ij} + R_{1ij}x_{1ij} ) = σ^2_0 + 2σ_{01}x_{1ij}\n\\]\nThis is possible as a variance function, without random effects interpretation.\nExample:\nYou find \\(\\hatσ^2_0=σ 37.85\\) and \\(\\hatσ_{01} = −1.89\\) is a gender effect on the variance term\nThen: The estimated residual (level-1) variance is 37.85 for boys and \\(37.85 − 2 × 1.89 = 34.07\\) for girls\nExample in R:\nHeteroscedasticity with respect to IQ:\n\nlibrary(nlme)\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nThe following object is masked from 'package:lme4':\n\n    lmList\n\nmod &lt;- lme(langPOST ~  IQ_verb + ses + \n               IQ_mean + SES_mean +  \n               IQ_verb*ses +\n               IQ_mean*SES_mean,\ndata=DAT,random = ~ IQ_verb|schoolnr,\nweights=varFixed(~IQ_verb))\nsummary(mod)\n\nLinear mixed-effects model fit by REML\n  Data: DAT \n       AIC      BIC    logLik\n  24609.84 24677.44 -12293.92\n\nRandom effects:\n Formula: ~IQ_verb | schoolnr\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev       Corr  \n(Intercept) 3.3536284565 (Intr)\nIQ_verb     0.0008512364 -0.002\nResidual    8.0814117189       \n\nVariance function:\n Structure: fixed weights\n Formula: ~IQ_verb \nFixed effects:  langPOST ~ IQ_verb + ses + IQ_mean + SES_mean + IQ_verb * ses +      IQ_mean * SES_mean \n                    Value Std.Error   DF   t-value p-value\n(Intercept)      41.62918 0.2850563 3240 146.03844  0.0000\nIQ_verb           2.38666 0.1151238 3240  20.73127  0.0000\nses               0.18968 0.0118918 3240  15.95052  0.0000\nIQ_mean           0.98568 0.3588633  207   2.74668  0.0066\nSES_mean         -0.11226 0.0505054  207  -2.22278  0.0273\nIQ_verb:ses      -0.01506 0.0101003 3240  -1.49149  0.1359\nIQ_mean:SES_mean -0.10614 0.0404586  207  -2.62332  0.0094\n Correlation: \n                 (Intr) IQ_vrb ses    IQ_men SES_mn IQ_vr:\nIQ_verb          -0.018                                   \nses               0.005 -0.143                            \nIQ_mean          -0.104 -0.100  0.036                     \nSES_mean          0.026  0.036 -0.235 -0.497              \nIQ_verb:ses      -0.042  0.047 -0.085  0.011  0.004       \nIQ_mean:SES_mean -0.398  0.008 -0.001  0.202  0.036 -0.076\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-6.64829880 -0.48269019  0.02393963  0.51663401  4.29185099 \n\nNumber of Observations: 3454\nNumber of Groups: 211 \n\n\nNote: From lme function’s help: weights: one-sided formula describing the within-group heteroscedasticity structure.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multilevel Data Modelling</span>"
    ]
  },
  {
    "objectID": "multi-level.html#diagnostics",
    "href": "multi-level.html#diagnostics",
    "title": "2  Multilevel Data Modelling",
    "section": "2.12 Diagnostics",
    "text": "2.12 Diagnostics\n\\[\nY_{ij} = γ_0 + \\sum_{h=1}^r \\gamma_h x_{hij} + U_{0j} +\\sum_{h=1}^p U_{hj}x_{hij} + R_{ij}\n\\]\nWhat has to be checked: - Does the fixed part contain the right variables? → Test effects and group means - Does the random part contain the right variables? → Test effects and group means - Are the level-1 residuals normally distributed? →Level-1 residual analysis - Do the level-1 residuals have constant variance ?→ Study heteroscedasticity - Are the level-2 random coefficients normally distributed with mean 0? → Level-2 residual analysis and level-1 residual analysis, - Do the level-2 random coefficients have a constant covariance matrix? → Level-2 residual analysis\n\n2.12.1 Are there contextual effects not considered? Test effects and group means\n\nFor every level-1 variable \\(X_h\\) check the fixed effect of the group mean \\(\\overline{X}_h\\).\n\\(U_0j\\) must not be correlated with the \\(X_{hij}\\) + Include the fixed effect of \\(X_h\\) if it is significant, and continue to use a random effects model. (Also check effects of variables \\(X_h·_jZ_j\\) for cross-level interactions involving \\(X_h\\)) + Random slopes \\(U_hj\\) must not be correlated with the \\(X_{kij}\\) → This can be checked by testing the fixed effect of \\(X_{k·j}X_{hij}\\)\n\n\n\n2.12.2 Level-1 residual analysis\nLevel-1 specification can be studied by disaggregation to the within-group level\n\nTest the fixed part of the level-1 model using OLS level-1 residuals, calculated per group separately\nTest the random part of the level-1 model using squared standardized OLS residuals (check for homoscedasticity)\n\nNote: The construction of the OLS within-group residuals implies that this tests only the level-one specification, independent of the correctness of the level-2 specification\n\n\n\nlevel-1 residual analysis\n\n\n\n2.12.2.1 Exercise:\nModel: We have the following 2-level Model:\n\nwith IQ, SES, gender, IQ, SES as explanatory variables\nwith an interaction effect between IQ and SES\nwith a random intercept for the school level and a random slope for IQ\n\nTask: Conduct Level-1 residual analysis for that model\nEstimate the related within model and extract the residuals.\n\nDAT_w &lt;- DAT #create a new data set \n\n# get all the residuals for the fixed parts of level 1 by substracting the individual score from the group mean\nDAT_w$IQ_verb &lt;- DAT_w$IQ_verb - mean(DAT_w$IQ_verb)\nDAT_w$langPOST &lt;- DAT_w$langPOST - mean(DAT_w$langPOST)\nDAT_w$ses &lt;- DAT_w$ses - mean(DAT_w$ses) \n\n# test the random part of level-1 model by using squared standardized OLS residuals and get the residuals\n\nres_w &lt;- lm(langPOST ~ IQ_verb + ses + sex + IQ_verb*ses, data=DAT_w)$residuals\n\nsummary(res_w)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-28.9569  -4.2417   0.5021   0.0000   4.8489  24.5437 \n\n# get the rounded values for IQ_verb residuals from the minimum to the maximum\nrange(round(DAT_w$IQ_verb))\n\n[1] -8  7\n\n\nStudy the residuals in comparison to the related IQ value, by forming mean residuals in IQ categories and by plotting. Add to each mean residual in each IQ category formed a measure of uncertainty, e.g. ± twice the std. dev. of the estimated mean residual Hint: Use R code snippets given at the next slide.\n\nRES &lt;- NULL\nfor(iq in seq(from=-8, to=7, by=0.5)){\n  res_cat &lt;- res_w[DAT_w$IQ_verb &gt;= iq & DAT_w$IQ_verb &lt;= (iq+1)]\n  RES &lt;- rbind(RES, c(iq, mean(res_cat), 2*sqrt(var(res_cat)/length(res_cat))))\n}\nplot(RES[,1], RES[,2], pch=20, xlab=\"IQ\", ylab=\"mean residual (IQ categ.)\", ylim=c(-5,3))\nfor(i in 1:nrow(RES)){\n  lines(rep(RES[i,1],2), c(RES[i,2]-RES[i,3], RES[i,2]+RES[i,3]))\n}\n\n\n\n\n\n\n\n\nWhat do you see? What do you conclude?\nThe first plot shows a curvilinear effect of IQ, which indicates that the assumption of heteroskedasticity is violated.\nTask: Check normality of the residuals using a normal probability plot of standardized level-1 OLS residuals What do you conclude? For Standardizing: divide residuals by their standard derivation (function sd in R) Use the R function qqnorm and qqline for that task.\n\nres_std_w &lt;- res_w/sd(res_w)\nqqnorm(res_std_w)\nqqline(res_std_w)\n\n\n\n\n\n\n\n\nThe normal Q-Q Plot shows, that while there is a normal line to the center, on the tails end especially in the lower end there is a higher deviation, what could be a sign for heteroskedasticity.\nIf the assumption of homoskedasticity is violated, transform! More infomormation here!\nor use a quadric\n\n\n\nquadric IQ\n\n\n\n\n\n2.12.3 Level-2 residual analysis\nEstimated level-2 residuals are always confounded with the estimated level-1 residuals:\n\nfirst conduct level-1 residual analysis, then conduct level-2 residual analysis\nFor level-2 residual analysis use the empirical Bayes estimates of the level-2 random effects (i.e., used as level-2 residuals)\nE.g.: plot unstd. level-2 residuals as a function of relevant level-2 variables to check for possible nonlinearity\nNormal probability plots may be made by std. level-2 residuals\nSquared residuals may be plot as a function of level-2 variables to check for homoscedasticity\nNote: Deviation from normality at level-2 might also be caused by an incorrect specification of fixed effects\n\n\n2.12.3.1 Exercise\nModel: We have the following 2-level Model:\n\nwith IQ, IQ2−, IQ2+ , SES, gender, \\(\\overline{IQ}\\), \\(\\overline{SES}\\) as explanatory variables\nwith an interaction effect between IQ and SES\nwith an interaction effect between \\(\\overline{IQ}\\) and \\(\\overline{SES}\\)\nwith a random intercept for the school level and a random slope for IQ\n\nTask: Conduct Level-2 residual analysis for that model Estimate the related within model and extract the level-2 residuals (Mind that these residuals can be described by the estimated random effects)\n\n#compute IQ squared minus and IQ squared plus to have homoskedasticity\n?ifelse\n\nHelp on topic 'ifelse' was found in the following packages:\n\n  Package               Library\n  base                  /home/runner/.cache/R/renv/sandbox/R-4.3/x86_64-pc-linux-gnu/5cd49154\n  data.table            /home/runner/work/_temp/renv/cache/v5/R-4.3/x86_64-pc-linux-gnu/data.table/1.16.4/38bbf05fc2503143db4c734a7e5cab66\n\n\nUsing the first match ...\n\nDAT$IQsq_min &lt;- ifelse(DAT$IQ_verb&lt;0, DAT$IQ_verb^2, 0)\nDAT$IQsq_pl &lt;- ifelse(DAT$IQ_verb&lt;0, 0, DAT$IQ_verb^2)\n\n#create a model including this new variables\nmod10 &lt;- lmer(langPOST ~  IQ_verb + IQsq_min + IQsq_pl + ses + \n                IQ_mean + SES_mean +  \n                IQ_verb*ses +\n                IQ_mean*SES_mean +\n                (IQ_verb|schoolnr), data=DAT)\nsummary(mod10)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: langPOST ~ IQ_verb + IQsq_min + IQsq_pl + ses + IQ_mean + SES_mean +  \n    IQ_verb * ses + IQ_mean * SES_mean + (IQ_verb | schoolnr)\n   Data: DAT\n\nREML criterion at convergence: 22700.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.2420 -0.6253  0.0960  0.6870  2.9187 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n schoolnr (Intercept)  8.717   2.9525        \n          IQ_verb      0.113   0.3362   -0.91\n Residual             37.695   6.1396        \nNumber of obs: 3454, groups:  schoolnr, 211\n\nFixed effects:\n                   Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)        41.53703    0.26042  227.00252 159.498  &lt; 2e-16 ***\nIQ_verb             3.04827    0.13812 1307.60896  22.070  &lt; 2e-16 ***\nIQsq_min            0.19935    0.03692 1455.30999   5.399 7.82e-08 ***\nIQsq_pl            -0.27149    0.04154 1701.53726  -6.536 8.31e-11 ***\nses                 0.17408    0.01224 3240.41850  14.221  &lt; 2e-16 ***\nIQ_mean             0.78388    0.29975  223.47954   2.615  0.00953 ** \nSES_mean           -0.08140    0.04383  253.35206  -1.857  0.06445 .  \nIQ_verb:ses        -0.01272    0.00575 1076.32695  -2.211  0.02722 *  \nIQ_mean:SES_mean   -0.09896    0.03327  263.90254  -2.974  0.00321 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) IQ_vrb IQsq_m IQsq_p ses    IQ_men SES_mn IQ_vr:\nIQ_verb     -0.141                                                 \nIQsq_min    -0.114  0.765                                          \nIQsq_pl     -0.077 -0.791 -0.536                                   \nses         -0.006 -0.143  0.014  0.057                            \nIQ_mean     -0.085 -0.072 -0.056 -0.066  0.054                     \nSES_mean     0.048  0.044  0.047  0.010 -0.270 -0.507              \nIQ_verb:ses  0.012  0.020 -0.255 -0.201 -0.141  0.043 -0.148       \nIQ_mn:SES_m -0.362  0.054  0.032 -0.086  0.020  0.175 -0.017 -0.101\n\n\nStudy the level-2 residuals in comparison to the related group mean of IQ, by plotting. Add a smoothed line of the residuals to assess whether the chosen explanatory variables and random effects match the assumptions (key word: linearity) For this use the R function smooth.spline\n\nbe_u &lt;- ranef(mod10)$schoolnr[[1]]\nRES &lt;- cbind(unique(DAT$schoolnr), be_u, DAT[!duplicated(DAT$schoolnr), \"IQ_mean\"])\nplot(RES[,3], RES[,2], xlab=\"IQ_mean\", ylab=\"U_0j\")\nsmooth &lt;- smooth.spline(x=RES[,3], y=RES[,2])\npoints(smooth$x, smooth$y, pch=20)\n\n\n\n\n\n\n\n\nThe smooth spline indicates homoskedasticity, the assumption is supported by this plot.\nCheck normality of the residuals using a normal probability plot of standardized level-2 residuals Overall: What do you see? What do you conclude?\n\nres_std_w &lt;- be_u/sd(be_u)\nqqnorm(res_std_w)\nqqline(res_std_w)\n\n\n\n\n\n\n\n\nWe see a straight line with more deviation at the edges, which could be a hint that there is heteroskedasticity is going on.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multilevel Data Modelling</span>"
    ]
  },
  {
    "objectID": "panel_data.html",
    "href": "panel_data.html",
    "title": "3  Panel Data",
    "section": "",
    "text": "3.1 Load libraries\nRecommended literature:\nGreene, W. H. (2000). Econometric Analysis from 1990. 4th edition. International edition, New Jersey: Prentice Hall, 201-215.\nlibrary(haven)\nlibrary(sandwich)\nlibrary(tidyverse) ##basic\nlibrary(stargazer) ##compare models\nlibrary(magrittr)\nlibrary(multigroup)\nlibrary(plm)\nlibrary(lme4)\nlibrary(lmtest)\nlibrary(lmerTest)\nlibrary(Matrix)\nlibrary(mitml)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Panel Data</span>"
    ]
  },
  {
    "objectID": "panel_data.html#properties",
    "href": "panel_data.html#properties",
    "title": "3  Panel Data",
    "section": "3.2 Properties",
    "text": "3.2 Properties\nGiven a set of observational units \\((i = 1, . . . ,N)\\) you have measurements on variables at multiple time points \\((t = 1, . . . ,T)\\) Observational units can be individuals, households, firms, countries, schools, regions.\nDimension of panel data (important is important for properties of (parameter) estimators):\n\n\\(N ≫ T\\) (Micro panel data): only few time points but a lot of N per Time\n\\(N ≡ T\\) (Time-series-cross-section data)\n\\(T ≫ N\\) (Macro panel data): a lot time points but less N, i.e. investigate economic development of countries over the time\n\nIn general panel data allow for:\n\nmodelling and thus controlling for individual specific heterogeneity\navoiding aggregation bias\nidentifying otherwise non-detectable effects (like intra-individual development, how certain units change over time)\nmore data\n\nmore information (more degrees of freedom are possible)\nmore variability\nand less multicollinearity (with less data points there is often colinearity although in reality it is not and only artificial)\n(likely) higher statistical efficiently (more secure about what is estimated)\n\nmodeling and analysis of dynamics and you can study causality because you have a time dimension in it (x takes place first and y takes place after)\n\nThree kinds of variables arise in the context of panel data\n\nvariables with variation over observational units and time \\((X_{it})\\) + socio-economic positions\nvariables with variation only over\nobservational units \\((Z_i)\\) + time constant variables like the birthday, ethnicity = variables with variation only over time, but not over\nobservational units \\((R_t)\\) + external shocks, mega trends like regime, politics, floods, financial crisis",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Panel Data</span>"
    ]
  },
  {
    "objectID": "panel_data.html#advantages-disadvantages-of-panel-data",
    "href": "panel_data.html#advantages-disadvantages-of-panel-data",
    "title": "3  Panel Data",
    "section": "3.3 Advantages & Disadvantages of Panel data",
    "text": "3.3 Advantages & Disadvantages of Panel data\n\nare very costly\nconfront the analyst with selectivity (self-selectivity, non response, attrition)\n\nprospective data collection is possible, you can follow certain trends and variables in the actual time. Very advantageous, because not the problem of lying or remembering. BUT: dropouts, non response → self-selectiveness\n\navailability of actual data\ndata collection might be difficult (coverage, collection costs, interview spacing, etc.)\nchallenges for statistical analysis and modelling\n\nExamples for individual or household panel data\n\nSocio Economic Panel (SOEP, DIW Berlin), since 1984\nNational Longitudinal Survey of Youth (NLS, U.S. Bureau of Labor Statistics), cohorts born between 1980-84\nPanel Study of Income Dynamics (PSID, University Michigan), since 1968\nBritish Household Panel Survey (BHPS, ESRC & University of Essex), since 1991\n\nFirm panel data\n\nBetriebspanel (IAB, N¨urnberg), since 1993\nGrunfelds Investmentdata (N = 11 U.S. firms), since 1950\n\nLinked panel data\n\nLinked Employer-Employee-Dataset of the IAB\n\nRegional or national panel data\n\nPenn World Tables (N = 167 countries), since 1950\nWorld Development Indicators Database (World Bank)\nGlobal Development Finance Database (World Bank)\nInternational Financial Statistics Database (IMF)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Panel Data</span>"
    ]
  },
  {
    "objectID": "panel_data.html#selectivity-and-attrition",
    "href": "panel_data.html#selectivity-and-attrition",
    "title": "3  Panel Data",
    "section": "3.4 Selectivity and Attrition",
    "text": "3.4 Selectivity and Attrition\nQuality of panel data is depending on stability of participation.\nProblem:\nDeutsche Forschungsgemeinschaft (DFG) only gives money for gifts but not for incentives, BUT: for the response rate it is so important to compensate people with money for their time.\nAlso an ethical question:\nHow much money is appropriate. * can strenghten bias * can be understand as an insult, because people would rather do it voluntarily (example:refugees) * for rich people: give a donation in their name * based on experience you have to learn whom to give what * families with kids: give the kids some little gift\nPanel mortality might lead to challenges in statistical inference since it likely implies selective (systematic) drop-outs. Information from previous waves can be used to control for this selectivity. This kind of correction refers to a selection correction as provided, for example, in the statistical framework of Heckman (1979) or inverse probability weighting.\n\nVerbeek, M., Nijman, T. (1992). Testing for selectivity bias in panel data models. International Economic Review, 681-703.\nFitzgerald, J., Gottschalk, P., Moffitt, R. (1998): An Analysis of Sample Attrition in Panel Data,\nThe Journal of Human Resources Sabine Zinn Hausman, J., Wise, D. (1979): Attrition ABQiaMs in Experimental and Pane8 l/ 5D6 ata, Econometrica\n\nExample: Family Survey of Deutsches Jugendinstitut In the first wave (in 1988) 10043 families were surveyed. In the second wave (in 1994) 4997 of these families were again surveyed and in the third wave (in 2000) only 2002 remained (≈ 20%).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Panel Data</span>"
    ]
  },
  {
    "objectID": "panel_data.html#balancedness",
    "href": "panel_data.html#balancedness",
    "title": "3  Panel Data",
    "section": "3.5 Balancedness",
    "text": "3.5 Balancedness\nBalanced data structure: For each of the N observational units, data could be collected at each of the \\(T\\) measurement points, providing a number of \\(NT\\) observations.\nUnbalanced data structure: For at least one observational unit, measurement on at least one occasion is missing, providing \\(\\sum^N_{i=1} T_i\\) measurements (with \\(T_i\\) denoting the number of measurements per observational unit \\(i\\)).\nNote that in principle all models and methods for balanced data are applicable to unbalanced data as well. However, in case of unbalanced panel data you have to clarify the causes why values are missing, i.e. to specify the process leading to the missing values.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Panel Data</span>"
    ]
  },
  {
    "objectID": "panel_data.html#peculairities",
    "href": "panel_data.html#peculairities",
    "title": "3  Panel Data",
    "section": "3.6 Peculairities",
    "text": "3.6 Peculairities\nPanel data require explicit statements regarding measurement consistency over time. Do you really measure the same thing over time?\nPanel data are in general characterized as data sets with (at least) two indices, where these two indices refer to observational units and time, or hierarchical structures, e.g. students in classes, multiple firms in regions, etc.\nIn biostatistics & social sciences, mostly the term “longitudinal data” is used instead of “panel data”.\nTime series and cross-sectional data can be thought of as special cases of panel data: * Time series: only one panel member over several measurement points + very common in econometrics like ARIMA, ARMA Models\n\nCross section: many observational units (persons, firms, etc) at one time point",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Panel Data</span>"
    ]
  },
  {
    "objectID": "panel_data.html#exercise",
    "href": "panel_data.html#exercise",
    "title": "3  Panel Data",
    "section": "3.7 Exercise",
    "text": "3.7 Exercise\nLook into the Data 1: Panel data on wages in the U.S.\nRead in:\n\nDAT1 &lt;- read_dta(\"data/2.Example1.dta\")\n\n\nHow many observation for units and times:\n\n\nnrow(DAT1)\n\n[1] 4165\n\n\n\nHow many time points?\n\n\nlength(unique(DAT1$time))\n\n[1] 7\n\n\n\nHow many units?\n\n\nlength(unique(DAT1$id))\n\n[1] 595\n\n\nGet a summary for all variables:\n\nsummary(DAT1)\n\n       id           time        exp             wks           bluecol     \n Min.   :  1   Min.   :1   Min.   : 1.00   Min.   : 5.00   Min.   :1.000  \n 1st Qu.:149   1st Qu.:2   1st Qu.:11.00   1st Qu.:46.00   1st Qu.:1.000  \n Median :298   Median :4   Median :18.00   Median :48.00   Median :2.000  \n Mean   :298   Mean   :4   Mean   :19.85   Mean   :46.81   Mean   :1.511  \n 3rd Qu.:447   3rd Qu.:6   3rd Qu.:29.00   3rd Qu.:50.00   3rd Qu.:2.000  \n Max.   :595   Max.   :7   Max.   :51.00   Max.   :52.00   Max.   :2.000  \n      ind             south           smsa          married     \n Min.   :0.0000   Min.   :1.00   Min.   :1.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:1.00   1st Qu.:1.000   1st Qu.:2.000  \n Median :0.0000   Median :1.00   Median :2.000   Median :2.000  \n Mean   :0.3954   Mean   :1.29   Mean   :1.654   Mean   :1.814  \n 3rd Qu.:1.0000   3rd Qu.:2.00   3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :1.0000   Max.   :2.00   Max.   :2.000   Max.   :2.000  \n      sex            union             ed            black      \n Min.   :1.000   Min.   :1.000   Min.   : 4.00   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:12.00   1st Qu.:1.000  \n Median :1.000   Median :1.000   Median :12.00   Median :1.000  \n Mean   :1.113   Mean   :1.364   Mean   :12.85   Mean   :1.072  \n 3rd Qu.:1.000   3rd Qu.:2.000   3rd Qu.:16.00   3rd Qu.:1.000  \n Max.   :2.000   Max.   :2.000   Max.   :17.00   Max.   :2.000  \n     lwage      \n Min.   :4.605  \n 1st Qu.:6.395  \n Median :6.685  \n Mean   :6.676  \n 3rd Qu.:6.953  \n Max.   :8.537  \n\n\nHow many time points per id? Is it balanced?\n\n#group the distinct time points by id\nDAT1_sum &lt;- DAT1 %&gt;%\n                  group_by(id) %&gt;%\n                  summarise(N = n_distinct(time))\n\n# have all id 7 time points?\nDAT1_sum$balanced &lt;- DAT1_sum$N == 7\ntable(DAT1_sum$balanced)\n\n\nTRUE \n 595 \n\n\nAll 595 units in the data frame has 7 time points.\n\n3.7.1 Look into the Data 2:\n\nDAT2 &lt;- read_dta(\"data/2.Example2.dta\")\n\n\nHow many observation for units and times:\n\n\nnrow(DAT2)\n\n[1] 1173\n\n\n\nHow many time points?\n\n\nlength(unique(DAT2$year))\n\n[1] 30\n\n\n\nHow many units?\n\n\nlength(unique(DAT2$state))\n\n[1] 46\n\n\nIs it balanced?\n\nDAT2_sum &lt;- DAT2 %&gt;%\n                  group_by(state) %&gt;%\n                  summarise(N = n_distinct(year))\n# have all id 7 time points?\nDAT2_sum$N\n\n [1] 24 25 22 26 25 27 28 23 23 21 27 26 26 29 24 29 26 25 27 24 28 23 25 27 24\n[26] 21 24 26 25 25 22 26 23 24 27 27 26 27 27 26 25 28 27 29 25 29\n\n\nIt ins not balanced, there is not data for all 30 years given for each state. In each state at least one data point for a year is missing.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Panel Data</span>"
    ]
  },
  {
    "objectID": "panel_data.html#linear-panel-models",
    "href": "panel_data.html#linear-panel-models",
    "title": "3  Panel Data",
    "section": "3.8 Linear Panel Models",
    "text": "3.8 Linear Panel Models\n\n3.8.1 Notation and Assumptions\nBased on a linear model:\n\\[\ny= \\alpha + \\beta x + \\epsilon\n\\]\nBut the linear model is extended:\nThe \\(y\\) is the dependent variable and is a stacked vector → the observations of the individual is stacked, because it brackets the time points from 1 to NT →\n\\[\ndim(y) = \\sum^N_{i=1}T_i * 1\n\\]\n\\(X\\) is a matrix of \\(k\\) conditioning variables\n\\[\ndim(x) = \\sum^N_{i=1}T_i * k\n\\]\nalso \\(\\epsilon\\), the vector of error terms is stacked\n\\[\ndim(\\epsilon) = \\sum^N_{i=1}T_i * 1\n\\]\n\\(N_T\\) are all observations in time of balanced structure\n→ problem: the observations are not independent from each other\nsubsequently\n\\[\nN_T = \\sum^N_{i=1} T_i\n\\]\nThe linear model is based on the following assumptions:\n\\(E[y] = X\\beta\\) and \\(E[\\epsilon]= 0\\) (error is zero)\n\\(Cov [y] = \\sigma^2 | =Cov(\\epsilon) = E[\\epsilon\\epsilon']\\) Covariances of \\(y\\) and \\(\\epsilon\\) independent of each other\nX is exogenous, not random and has a full rank → we can not be sure, what was first in a linear model and if it is not endogenous → entangle the X from Y and make sure it is not endogenous\nTwo situations yield an exogenous X:\n\nan experimental setup, where X is designed (full control of X),\nX is assumed to be the result of a random experiment (providing y)\n\n\n\n3.8.2 Basic Framework for Panel Data\nThe basic framework for panel data is a regression of the form\n\\[\ny_{it} = x′_{it}β + z′_iα + ϵ_{it} .\n\\]\nThere are K regressors in \\(x′_{it}\\) , not including a constant term.\nThe heterogeneity or individual effect \\(z′_iα\\), where \\(z′i\\) contains a constant term and a set of individual or group specific variables, which may be\n\nobserved (such as race, sex, location), or\nunobserved (such as family specific characteristics, individual heterogeneity in skill or preferences), → makes a lot of problems, because must be included in the model.\nyou estimate one \\(\\beta\\) for each individual effect, not for each individual.\n\nAs it stands, this model is a classical regression model. If \\(z′i\\) is observed for all individuals, then the entire model can be treated as an ordinary linear model and fit by least squares.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Panel Data</span>"
    ]
  },
  {
    "objectID": "panel_data.html#pooled-regression",
    "href": "panel_data.html#pooled-regression",
    "title": "3  Panel Data",
    "section": "3.9 Pooled Regression",
    "text": "3.9 Pooled Regression\nThree ways of dealing with panel data. Pooled regression is the easiest way. Pooled Regression is a homogeneous (or pooled) panel data model, because its assume that the model parameters are common across individuals. Fixed and random effects models are heterogeneous models, because they allow for any or all of the model parameters to vary across individuals. Fixed effects and random effects models are both examples of heterogeneous panel data models.\nThe panel regression formula without the \\(z'i\\) → because it is assumed, that there are no unobservable individual-specific effects. This is a strong assumption, because with this assumed, that all the observations within groups are independent of each other.\n→ consequently Pooled OLS ignores time and individual characteristics and focuses only on the dependencies between the individual.\n\\[\ny_{it} = α + x′_{it}β + ϵ_{it} , i = 1, . . . , t = 1, . . . ,T_i\n\\]\nThe residuals depends on all the residual\n\\[\nE[ϵ_{it} |x_i1, x_i2, . . . , ] = 0,\n\\]\n\\[\nVar [ϵ_{it} |x_i1, x_i1, . . . , ] = σ^2_ϵ\n\\]\n\\[\nCov[ϵ_{it}ϵ_{js} |x_i1, x_i1, . . . , x_{iTj} ] = 0,\n\\]\nif \\(i \\not= j\\) or \\(t \\not=s\\)\n\n3.9.1 Specifing the estimation model\nIn this form, if the remaining assumptions of the classical linear model are met (zero conditional mean of \\(ϵ_{it}\\) , homoscedasticity (residuals are over time and the individuals are the same or very similar), independence across observations \\(i\\) , and strict exogenity of \\(x′it)\\), then OLS is the efficient estimator and inference can reliably proceed along the linear model. However, the crux of panel data is that all these assumptions in combination are unlikely to be met.\nThe question is what can be expected of the estimator when the heterogeneity does differ across individuals?\nWe have to allow for heteroscedasticity, because data is often not fitting to homoscedasticity.\n\nVary the classical assumptions of the linear regression model as follows and in doing so allow for heteroscedasticity and/ or autocorrelation.\n\n\\[\nE[\\epsilon \\epsilon']= \\sum \\not= \\sigma^2I\n\\]\n\nAt the end: an unbiased estimator for \\(\\beta\\), which is a fixed estimator! The only entrance of randomness is the \\(y\\).\nWhat is the issue coming, when you gave to allow for it?\n\\(\\beta\\) we get a unbiased estimator with fixed effects! It is a fixed estimator! The only entrance of randomness is the y.\nThe problem is not the \\(\\beta\\), the problem is the covariance \\(Cov(\\hat{β})\\). \\(\\sigma^2\\) can’t be used to give the quality of the estimator → because we have heteroscedasticity from the individuals and in time, so we cannot get the right standard errors for the estimator, because of the error term. → we don’t access the right variance in population, because the variance is influenced by parameters in units AND time!\nProblem: everything depends on everything!\nThe next steps will lead us to better compute the covariance:\nCovariance matrix Note that \\(Σ\\) is of dimension \\(NT × NT\\) thus is a positive definite covariance matrix involving \\(NT (NT + 1)/2\\) parameters.\n\nSo for 1000 units and 5 time points, we would need to estimate 2500 parameters. NOT feasible!\n\nWhich form is suited or necessary depends on the data structure?\n\ninstead: for panel data analysis block diagonal structures)\n\n→ I assume, that across individuals in time is no correlation, it is assumed to be zero. Like in multi-level models, I assume that only variables in one individual (unit) separately from another individual (unit) are varying in time.\n→ This is an estimator for the covariance matrix:\n\n\nautocorrelation: the estimator is an dyadic products.\nNow, we come to the formulas, we need:\n\nWhat does it imply?\nOLS Pooled Regression treats the panel data as a single large dataset and estimates a single regression model for all observations. It ignores the individual-specific effects and assumes that the relationship between the variables is the same for all individuals. This approach does not account for the potential heterogeneity across individuals. It does not allow for intercept or slope differences among individuals. A common relationship for all individuals is assumed.\n\n\n3.9.2 Robust Estimation using Group Means\nBetween Model or Group Means Model → a group is one individual with all time points! You can also reverse the logic and group the time points over individuals.\n\n\n\nGroup Mean Model\n\n\nBy making the averaging, you are in a heteroscedasticity setting, because you kicked out the auto correlation → then the White estimator is a robust estimator for robust standard errors:\n\n\n\nWhite Estimator\n\n\nWe have a uncertainty on the cluster level we did not have before.\nA group mean regression estimates separate regression models for each group or individual in the panel over time. It takes into account the individual-specific effects by including group-specific intercepts or fixed effects in the regression model. This approach allows for heterogeneity across individuals and captures the differences in the relationship between the variables for each group (individuals).\nIn sum: It allows for individual-specific effects and captures heterogeneity across groups or individuals in the data.\n\n\n3.9.3 Example Data\nIs build in R\n\n\n\nExample Data\n\n\n There are differences in the estimates. Explanation for that follows in the excerise\n\n\n3.9.4 Exercise\n\nReplicate this the table from ?fig-compare\n\n\n#Load data\ndata(Wages)\nDAT &lt;- Wages\n\n# add the squared experience variable\nDAT$expSq &lt;- DAT$exp^2\n\n#add an ID\nDAT$id &lt;- rep(1:595, each=7)\n\n# add a time point\nDAT$time &lt;- rep(1:7, 595)\n\nA panel of 595 individuals from 1976 to 1982, taken from the Panel Study of Income Dynamics (PSID).\nVariables: log wage (lwage);\nyears of full time work experience (exp); weeks worked (wks);  1 if blue-collar occupation, 0 if not (bluecol);  1 if the individual works in a manufactural industry, 0 if not (ind);  1 if the individual resides in the South, 0 if not (south);  1 if the individual resides in an metropolitan area (smsa), 0 if not;  1 if individual is married, 0 if not (married);  1 if the individual wage is set by a union contract, 0 if not (union);  years of education (ed);  1 if the individual is female, 0 if not (sex);  1 if the individual is black, 0 if not (black)\nCreate a model like the one from Cornwell and Rupert (1988):\nPooled OLS Regression {#sec-ex.pool}\nOLS can be used to pool observations of the same individual recorded at different time points. However, observations of the same individual are then treated as if they originate from other individuals. So a typical linear regression should lead to the same results as a pooled OLS regression\n\nmodL &lt;- lm(lwage ~ exp + expSq + wks + bluecol + ind + south + smsa + married + sex + union + ed + black, data=DAT)\nsummary(modL)\n\n\nCall:\nlm(formula = lwage ~ exp + expSq + wks + bluecol + ind + south + \n    smsa + married + sex + union + ed + black, data = DAT)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.18965 -0.23536 -0.00988  0.22906  2.08738 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.251e+00  7.129e-02  73.662  &lt; 2e-16 ***\nexp          4.010e-02  2.159e-03  18.574  &lt; 2e-16 ***\nexpSq       -6.734e-04  4.744e-05 -14.193  &lt; 2e-16 ***\nwks          4.216e-03  1.081e-03   3.899 9.82e-05 ***\nbluecolyes  -1.400e-01  1.466e-02  -9.553  &lt; 2e-16 ***\nind          4.679e-02  1.179e-02   3.967 7.39e-05 ***\nsouthyes    -5.564e-02  1.253e-02  -4.441 9.17e-06 ***\nsmsayes      1.517e-01  1.207e-02  12.567  &lt; 2e-16 ***\nmarriedyes   4.845e-02  2.057e-02   2.355   0.0185 *  \nsexfemale   -3.678e-01  2.510e-02 -14.655  &lt; 2e-16 ***\nunionyes     9.263e-02  1.280e-02   7.237 5.45e-13 ***\ned           5.670e-02  2.613e-03  21.702  &lt; 2e-16 ***\nblackyes    -1.669e-01  2.204e-02  -7.574 4.45e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3494 on 4152 degrees of freedom\nMultiple R-squared:  0.4286,    Adjusted R-squared:  0.427 \nF-statistic: 259.5 on 12 and 4152 DF,  p-value: &lt; 2.2e-16\n\n\nOr alternatively: In this model we get the robust panel standard errors.\n\nmodO &lt;- plm(lwage ~ exp + expSq + wks + bluecol + ind + south + smsa + married + sex + union + ed + black, data=DAT, \n            model=\"pooling\", index = c(\"id\", \"time\"))\nsummary(modO, vcov=vcovHC(modO, method=\"arellano\")) \n\nPooling Model\n\nNote: Coefficient variance-covariance matrix supplied: vcovHC(modO, method = \"arellano\")\n\nCall:\nplm(formula = lwage ~ exp + expSq + wks + bluecol + ind + south + \n    smsa + married + sex + union + ed + black, data = DAT, model = \"pooling\", \n    index = c(\"id\", \"time\"))\n\nBalanced Panel: n = 595, T = 7, N = 4165\n\nResiduals:\n      Min.    1st Qu.     Median    3rd Qu.       Max. \n-2.1896534 -0.2353558 -0.0098805  0.2290643  2.0873770 \n\nCoefficients:\n               Estimate  Std. Error t-value  Pr(&gt;|t|)    \n(Intercept)  5.2511e+00  1.2326e-01 42.6005 &lt; 2.2e-16 ***\nexp          4.0105e-02  4.0671e-03  9.8607 &lt; 2.2e-16 ***\nexpSq       -6.7338e-04  9.1106e-05 -7.3911 1.751e-13 ***\nwks          4.2161e-03  1.5384e-03  2.7405 0.0061609 ** \nbluecolyes  -1.4001e-01  2.7181e-02 -5.1511 2.710e-07 ***\nind          4.6789e-02  2.3609e-02  1.9818 0.0475634 *  \nsouthyes    -5.5637e-02  2.6100e-02 -2.1317 0.0330876 *  \nsmsayes      1.5167e-01  2.4048e-02  6.3069 3.143e-10 ***\nmarriedyes   4.8449e-02  4.0850e-02  1.1860 0.2356910    \nsexfemale   -3.6779e-01  4.5470e-02 -8.0885 7.858e-16 ***\nunionyes     9.2627e-02  2.3618e-02  3.9219 8.927e-05 ***\ned           5.6704e-02  5.5519e-03 10.2135 &lt; 2.2e-16 ***\nblackyes    -1.6694e-01  4.4228e-02 -3.7745 0.0001626 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    886.9\nResidual Sum of Squares: 506.77\nR-Squared:      0.42861\nAdj. R-Squared: 0.42696\nF-statistic: 66.2142 on 12 and 594 DF, p-value: &lt; 2.22e-16\n\n#Check if same \nstargazer::stargazer(modO,modL, type=\"text\", out = \"ml.html\")\n\n\n========================================================\n                                Dependent variable:     \n                            ----------------------------\n                                       lwage            \n                              panel           OLS       \n                              linear                    \n                               (1)            (2)       \n--------------------------------------------------------\nexp                          0.040***      0.040***     \n                             (0.002)        (0.002)     \n                                                        \nexpSq                       -0.001***      -0.001***    \n                            (0.00005)      (0.00005)    \n                                                        \nwks                          0.004***      0.004***     \n                             (0.001)        (0.001)     \n                                                        \nbluecolyes                  -0.140***      -0.140***    \n                             (0.015)        (0.015)     \n                                                        \nind                          0.047***      0.047***     \n                             (0.012)        (0.012)     \n                                                        \nsouthyes                    -0.056***      -0.056***    \n                             (0.013)        (0.013)     \n                                                        \nsmsayes                      0.152***      0.152***     \n                             (0.012)        (0.012)     \n                                                        \nmarriedyes                   0.048**        0.048**     \n                             (0.021)        (0.021)     \n                                                        \nsexfemale                   -0.368***      -0.368***    \n                             (0.025)        (0.025)     \n                                                        \nunionyes                     0.093***      0.093***     \n                             (0.013)        (0.013)     \n                                                        \ned                           0.057***      0.057***     \n                             (0.003)        (0.003)     \n                                                        \nblackyes                    -0.167***      -0.167***    \n                             (0.022)        (0.022)     \n                                                        \nConstant                     5.251***      5.251***     \n                             (0.071)        (0.071)     \n                                                        \n--------------------------------------------------------\nObservations                  4,165          4,165      \nR2                            0.429          0.429      \nAdjusted R2                   0.427          0.427      \nResidual Std. Error                    0.349 (df = 4152)\nF Statistic (df = 12; 4152) 259.544***    259.544***    \n========================================================\nNote:                        *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nLeads to the same results.\nCheck also for standard errors:\n\nmodL_robust_se &lt;- as.vector(summary(modL,robust = T)$coefficients[,\"Std. Error\"])\nmodO_robust_se &lt;- as.vector(summary(modO,robust = T)$coefficients[,\"Std. Error\"])\ndata.frame(modL_robust_se,modO_robust_se)\n\n   modL_robust_se modO_robust_se\n1    7.128679e-02   7.128679e-02\n2    2.159175e-03   2.159175e-03\n3    4.744313e-05   4.744313e-05\n4    1.081366e-03   1.081366e-03\n5    1.465670e-02   1.465670e-02\n6    1.179350e-02   1.179350e-02\n7    1.252710e-02   1.252710e-02\n8    1.206870e-02   1.206870e-02\n9    2.056867e-02   2.056867e-02\n10   2.509705e-02   2.509705e-02\n11   1.279951e-02   1.279951e-02\n12   2.612826e-03   2.612826e-03\n13   2.204219e-02   2.204219e-02\n\n\nGroup Mean pooled regression\n\nmodG &lt;- plm(lwage ~ exp + expSq + wks + bluecol + ind + south + smsa + married + sex + union + ed + black, data=DAT, \n            model=\"between\", #choose this argument for using group means of individuals\n            index = c(\"id\", \"time\")) \nsummary(modG) \n\nOneway (individual) effect Between Model\n\nCall:\nplm(formula = lwage ~ exp + expSq + wks + bluecol + ind + south + \n    smsa + married + sex + union + ed + black, data = DAT, model = \"between\", \n    index = c(\"id\", \"time\"))\n\nBalanced Panel: n = 595, T = 7, N = 4165\nObservations used in estimation: 595\n\nResiduals:\n      Min.    1st Qu.     Median    3rd Qu.       Max. \n-0.8268412 -0.1816744  0.0039229  0.1867997  0.7169636 \n\nCoefficients:\n               Estimate  Std. Error t-value  Pr(&gt;|t|)    \n(Intercept)  5.12143093  0.20424937 25.0744 &lt; 2.2e-16 ***\nexp          0.03190113  0.00477687  6.6783 5.656e-11 ***\nexpSq       -0.00056563  0.00010485 -5.3945 1.001e-07 ***\nwks          0.00918910  0.00360440  2.5494 0.0110457 *  \nbluecolyes  -0.16761971  0.03381666 -4.9567 9.416e-07 ***\nind          0.05791753  0.02554122  2.2676 0.0237187 *  \nsouthyes    -0.05705355  0.02596784 -2.1971 0.0284078 *  \nsmsayes      0.17577535  0.02575680  6.8244 2.222e-11 ***\nmarriedyes   0.11478166  0.04769750  2.4065 0.0164190 *  \nsexfemale   -0.31706119  0.05472529 -5.7937 1.128e-08 ***\nunionyes     0.10906865  0.02923185  3.7312 0.0002093 ***\ned           0.05143597  0.00555456  9.2601 &lt; 2.2e-16 ***\nblackyes    -0.15780429  0.04501188 -3.5058 0.0004902 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    92.322\nResidual Sum of Squares: 42.073\nR-Squared:      0.54428\nAdj. R-Squared: 0.53489\nF-statistic: 57.926 on 12 and 582 DF, p-value: &lt; 2.22e-16\n\n\nCompare:\n\nstargazer(modL, modG, type=\"text\", out = \"ml.html\")\n\n\n=======================================================================\n                                    Dependent variable:                \n                    ---------------------------------------------------\n                                           lwage                       \n                               OLS                      panel          \n                                                        linear         \n                               (1)                       (2)           \n-----------------------------------------------------------------------\nexp                          0.040***                  0.032***        \n                             (0.002)                   (0.005)         \n                                                                       \nexpSq                       -0.001***                 -0.001***        \n                            (0.00005)                  (0.0001)        \n                                                                       \nwks                          0.004***                  0.009**         \n                             (0.001)                   (0.004)         \n                                                                       \nbluecolyes                  -0.140***                 -0.168***        \n                             (0.015)                   (0.034)         \n                                                                       \nind                          0.047***                  0.058**         \n                             (0.012)                   (0.026)         \n                                                                       \nsouthyes                    -0.056***                  -0.057**        \n                             (0.013)                   (0.026)         \n                                                                       \nsmsayes                      0.152***                  0.176***        \n                             (0.012)                   (0.026)         \n                                                                       \nmarriedyes                   0.048**                   0.115**         \n                             (0.021)                   (0.048)         \n                                                                       \nsexfemale                   -0.368***                 -0.317***        \n                             (0.025)                   (0.055)         \n                                                                       \nunionyes                     0.093***                  0.109***        \n                             (0.013)                   (0.029)         \n                                                                       \ned                           0.057***                  0.051***        \n                             (0.003)                   (0.006)         \n                                                                       \nblackyes                    -0.167***                 -0.158***        \n                             (0.022)                   (0.045)         \n                                                                       \nConstant                     5.251***                  5.121***        \n                             (0.071)                   (0.204)         \n                                                                       \n-----------------------------------------------------------------------\nObservations                  4,165                      595           \nR2                            0.429                     0.544          \nAdjusted R2                   0.427                     0.535          \nResidual Std. Error     0.349 (df = 4152)                              \nF Statistic         259.544*** (df = 12; 4152) 57.926*** (df = 12; 582)\n=======================================================================\nNote:                                       *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nIn both models I have included all explanatory and control variables and regress them from the natural log in Wage for all units.\nIn both models all included variables are significant and have an effect in the same direction.\nOur explanatory variable we are interested in is the return to education, which is grasped by the variable Ed. The effect of this variable is quite similar in both models, but in the second one a bit smaller.\nVariables with similar estimates and robust standard errors in both models:\n\nexp - years full of work experience\nexpSq - years full of work experience squared\nbluecolyes - in blue collar occupation\nsouthyes - reside in south\nsmsa - resides in metropolian area\nunion - part of a union\nblack - if a person is black\n\nVariables with different estimates and robust standard errors in both models:\n\nwks - weeks of work, the estimate in the second model is more than twice as high than in the first\nmarried- if a person is married, the estimate in the second model is more than twice as high than in the first.\n\nWhy is this the case?\n\nThe OLS and the Panel Robust Standard error are for considering time points variation for one individual (more appropriate within level) → its accounting for period heteroscedasticity\nThe group mean regression and the White Robust S.E. are for considering individual variance across time (within level variation comparing between variation) → accounting for cross-sectional heteroscedasticity.\nMarriage and weeks worked are indicators, that vary over time. The group mean estimator account for cross-sectional heteroscedasticity, not for periodical. But effects of worked week and marriage are changing with time. So when you make for example a group effect of people who are married , they earn way more than people who are never been married. But in this group model different time effects are not included and it can be, that in the beginning of a marriage the effect is small and grows over time, because you get older and more experienced etc., too. So the time being married and the differences in the impact of marriage on earning are not included.\nOn the other hand the modG cancels autocorrelation out and allows for individual specific variations.\n\nANOVA for investigating variance Depending on the model, we have different estimators. Where comes the differences from? From the individual or the time level? Therefore, we do an ANOVA.\nWe fit first e linear regression for the individual:\n\nfit &lt;- lm(lwage~id, data=DAT)\nanova(fit)\n\nAnalysis of Variance Table\n\nResponse: lwage\n            Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nid           1   7.07  7.0655  33.431 7.927e-09 ***\nResiduals 4163 879.84  0.2113                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA tells us that there is a significant difference in the means of an individual over time.\nThe total sum of squares (the total variation across all units) is 886.91.\nIt shows the within variation with the residuals of the individual (the mean squared error of the residuals estimates the variation of the errors around the group means, the MSE)\n\nwithin &lt;- anova(fit)[\"Residuals\", \"Mean Sq\"]; within\n\n[1] 0.2113475\n\n\nWhat differs, when a variable of the individual level changes? I.e. what changes for the individual, when he/she gets married?\nBetween variance with individuals (estimates the varianceof the different group or in this case means for the individuals around the grand mean):\n\nbetween &lt;- anova(fit)[\"id\", \"Mean Sq\"]; between\n\n[1] 7.065452\n\n\nSo this gives us an insight, i. e. what differs for the group of individuals that is married in comparison to the group that is not married?\nNext step: What variance is there all over?\n\ntotal &lt;- between + within; total\n\n[1] 7.276799\n\n\nBecause the variance in the mean squared for ID is much larger than the mean squared error, the F-ratio is not nearly close to 1 and we have a signifcant value to reject the null hypothesis, that there are no differences in means between the indivduals.\nSteps described generally:\n## anov &lt;- aov(lwage~id+Error(id),data=DAT)\n## summary(anov)\n## grandmean &lt;- as.vector(anov$\"(Intercept)\"[[1]][1])\n## msq_with &lt;- summary(anov)$\"Error: Within\"[[1]]$\"Mean Sq\" ## Mean Squared Error for within group variance (for group = individuals)\n## dfId &lt;- summary(anov)$\"Error: id\"[[1]]$\"Df\"\n## dfWithin &lt;- summary(anov)$\"Error: Within\"[[1]]$\"Df\"\n## msq_bet &lt;- summary(anov)$\"Error: id\"[[1]]$\"Mean Sq\" ## Mean Squared Error for between group variance (for group = individuals)\n## between &lt;- (msq_bet-msq_with)/((dfWithin/(dfId+1))+1) ## (S1^2-S2^2)/J\n## total &lt;- msq_bet + msq_with\n## msq_with ## within ok\n## msq_bet ## between ok\n## total",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Panel Data</span>"
    ]
  },
  {
    "objectID": "panel_data.html#fixed-effects-model",
    "href": "panel_data.html#fixed-effects-model",
    "title": "3  Panel Data",
    "section": "3.10 Fixed Effects Model",
    "text": "3.10 Fixed Effects Model\nNo random component added to \\(\\beta\\). The fixed effects model arises from the assumption that omitted effects, \\(c_i\\) , in the general model\n\\[\ny_{it} = x′_{it}β + c_i + ϵ_{it}\n\\]\nare correlated with the included variables, i.e.\n\\[\nE[c_i |X_i ] = h(X_i).\n\\]\nAssumption: \\(C_i\\), the “unobservable” effect of omitted effects are correlated to the observable and included variables.\nUnobservable effects could be motivations, personality traits.\nExample: The chance of marriage is not only explained by the x, but dependent on the x conditioned by c.\nSince the conditional means is the same in every period, it gives the unobserved effect a home, because it is conditional over time and dependent on the observed variables.\n\\[\ny_{it} = x′_{it}β + h(X_i ) + ϵ_{it} + [c_i − h(X_i )]\\\\\n= x′_{it}β + α_i + ϵ_{it} + [c_i − h(X_i)].\n\\]\n$ h(X_i )$ is the unobserved, what I observed on the other variables → this is the fixed effect.\nBy design the bracket term is uncorrelated with Xi , thus we may absorb it in the disturbance:\n\\[\ny_{it} = x′_{it}β + α_i + ϵ_{it} .\n\\]\nA further assumption is that \\(Var (ci |Xi )\\) is constant. Thus the model above is a classical linear regression model.\n→ what we have in conclusion is a linear model.\nThe fixed effects formulation implies that differences across groups can be captures in differences in the constant term. Each \\(α_i\\) is treated as an unknown parameter to be estimated.\nThe major shortcoming of the fixed effects approach is that any time-invariant variable in the set of explanatory variables will mimic the individual specific constant term.\nThat is, the fixed effects formulation of the model absorbs the all time-invariant terms in the regression in \\(α_i\\) .\nThus the coefficient on the time-invariant variables cannot be estimated. (Hence, fixed effects models cannot be used to investigate time-invariant causes of the dependent variables.)\nThis lack of identification is the price of robustness of the specification to unmeasured correlation between the omitted effects and the included variables.\n→ it is not possible to get an estimate for constant variables over time.\nIf you want to compare groups, use group means: Then, you can infer, what a belonging to a certain gender, an age cohort, an occupation has on the individual or on the group.\nThe fixed effects model is useful to investigate dynamics: Here, you get rid of all constant factors in holding them fixed. Your focus here is to estimate differences in time, like enter a new job, get married, or external shocks like inflation.\nIn sum, the fixed effect model is useful whenever you are only interested in analyzing the impact of variables that vary over time (the time effects).\nThe fixed-effects model controls for all time-invariant differences between the individuals: estimated coefficients cannot be biased because of omitted time-invariant characteristics. Technically, time-invariant characteristics of the individuals are perfectly collinear with the group dummies.\nFixed effects models serve to study the causes of changes within a group/person.A time-invariant characteristic cannot cause such a change, because it is constant for each group/person.\nImportant: You cannot control for everything! So, you must assume, that the constants are not changing over time.\n\n3.10.1 Least suqares dummy variable model (LSDV)\nBUT: you can reduce the \\(\\alpha\\) estimates:\n\nBecause too many estimates:\n\n\n3.10.2 Partioned Regression\n\n(The Frisch-Waugh-Lovell theorem states that) in this regression the residuals \\(\\hatϵ\\) and the OLS estimate \\(\\hatβ_2\\) will be numerically identical to the residuals and the OLS estimate for \\(β_2\\) in the following regression: \\[\nM_1y = M_1X_2β_2 + η\n\\] where \\(M_1\\) is the annihilator matrix for the regressors \\(X_1\\). The theorem states that having a regression with a constant and another regressor is equivalent to subtracting the means from the dependent variable and the regressor and then running the regression for the demeaned variables but without the constant term.\nFor each of the observed points you subtract the mean, so \\(y+TN - \\overline{y}_N\\)\n→ you standardized your \\(y\\) over the time. Also often called demeaned regression. You get rid of the part of the model you are not interesting in. You transform your y into another y, which is free from the constants.\n\n\n3.10.3 Consistency if partitioned regression estimator\nThus, as long as the data is well behaving and number \\(N\\) of individual units is large, we can expect consistency of \\(\\hatβ\\). However, this is mostly not true for \\(\\hatα_i\\) : if \\(T\\) is small, \\(\\hatα_i\\) is an inconsistent estimator of \\(α_i\\) (this is the individual effect).\nSo you should test for \\(\\hatα_i\\)\n\nExample for \\(\\beta\\) :\n\nwhat is the effect of entering marriage on wages?\nhow large is the new task of taking informal care for relatives or intensify your care, what does it make with the well-being?\n\nExample for \\(\\alpha\\) :\n\npersonality traits in the effect of entering marriage on wages?\nintrinsic motivation in taking care for relatives or intensify your care\n\n→ \\(\\alpha\\) is not what we are interesting in, but it does play a role, specific for each individual, not a parameter for a feature like \\(\\beta\\)!\n\n\n3.10.4 Testing with the F-Test\n\n\n\n3.10.5 Exercise\nLeast squares dummy variable model\n1.Estimate a fixed effects model with lwage as dependent variable and the regressors described before (where possible). For that purpose, use the LSDV approach. Mind that in this model there is no grand mean but group specific intercepts. In R, you omit the grand mean from estimation by adding ‘-1’ to the right side of the regression equation and you get group specific effects by defining the related group identifiers as factors ‘as.factor()’.\n\n# use the same explanatory and control variables as before but exclude all time invariant factors. This is, because they are mimiced by the dummies for id. \nfixed_lsdv &lt;- lm(lwage ~ as.factor(id) #include the group specific effect\n                 + exp + expSq + wks + bluecol + ind + south + smsa + married + union\n                 -1, #subtract the grand mean from the estimation\n                 data=DAT) \nsummary(fixed_lsdv) \n\nthe summary(fixed_lsdv) gives you all estimates for each individual (in this data set 595 individuals) and estimates for each explanatory and control variable. A look in taht output shows, that all the estimates for the ids are significant and ranges from 3 to over 6.\nBut we are not interested in the data overfitting estimates for each individual, we are interested in the other variables.\nWhen we want to have only the coefficients, this code is useful:\n\nfixed_lsdv$coefficients[-c(1:595)]\n\n          exp         expSq           wks    bluecolyes           ind \n 0.1132082750 -0.0004183513  0.0008359460 -0.0214764983  0.0192101222 \n     southyes       smsayes    marriedyes      unionyes \n-0.0018611924 -0.0424691528 -0.0297258386  0.0327848598 \n\n\nCompared to the group mean model and the pooled regression the estimates have changed a lot. This is, because the LSDV model is only capable of modelling dynamics, so how an individual with certain heterogenity is changing over time because of different variables. The OLS regression does not account for individual specific effects and the group mean not for individual changings over time. It depends on the question and the theoretical framework, which model is appropriate to get the right estimates. With the estimates of the fixed effect model we analyze dynamics for individuals. So: What does marriage have for an effect on the wage of an individual?\nBecause many \\(\\alpha\\) terms, if \\(N\\) gets larger, not feasible (so I have spare to print the summary of that model)\npartioned regression / within estimator\n\nfixed_w &lt;- plm(lwage ~ exp + expSq + wks + bluecol + ind + south + smsa + married + union, data=DAT, model=\"within\", index = c(\"id\", \"time\")) \nsummary(fixed_w) \n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = lwage ~ exp + expSq + wks + bluecol + ind + south + \n    smsa + married + union, data = DAT, model = \"within\", index = c(\"id\", \n    \"time\"))\n\nBalanced Panel: n = 595, T = 7, N = 4165\n\nResiduals:\n      Min.    1st Qu.     Median    3rd Qu.       Max. \n-1.8122282 -0.0519417  0.0038855  0.0614706  1.9434306 \n\nCoefficients:\n              Estimate  Std. Error t-value  Pr(&gt;|t|)    \nexp         1.1321e-01  2.4710e-03 45.8141 &lt; 2.2e-16 ***\nexpSq      -4.1835e-04  5.4595e-05 -7.6629 2.329e-14 ***\nwks         8.3595e-04  5.9967e-04  1.3940   0.16340    \nbluecolyes -2.1476e-02  1.3784e-02 -1.5581   0.11930    \nind         1.9210e-02  1.5446e-02  1.2437   0.21370    \nsouthyes   -1.8612e-03  3.4299e-02 -0.0543   0.95673    \nsmsayes    -4.2469e-02  1.9428e-02 -2.1859   0.02889 *  \nmarriedyes -2.9726e-02  1.8984e-02 -1.5659   0.11747    \nunionyes    3.2785e-02  1.4923e-02  2.1970   0.02809 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    240.65\nResidual Sum of Squares: 82.267\nR-Squared:      0.65815\nAdj. R-Squared: 0.60026\nF-statistic: 761.751 on 9 and 3561 DF, p-value: &lt; 2.22e-16\n\n\nThe results for the least squares dummy regression and the partioned regression are the same. Like the LSDV the partioned regression also estimates the effect of the dynamic for each individual.\npooled model als a null model\nWhile the partioned regression take individual specifics and time dynamics into account, the pooled one does not and only account for heteroscedasticity of periods. Therefore, we should test, what the better model is:\n\npooled &lt;- lm(lwage ~ exp + expSq + bluecol + ind + south + smsa + married + sex + union + ed + black, data=DAT)\n\nTo compare this model to the other, we use the function pFtest. This function is a F Test for individual and time effects and is written for the comparison of pooled and partioned regression models.\n\n# test on common significance of dummies for groups/ individuals\npFtest(fixed_w, #first argument has to be the fixed model\n       pooled) #second argument the pooled one\n\n\n    F test for individual effects\n\ndata:  lwage ~ exp + expSq + wks + bluecol + ind + south + smsa + married +  ...\nF = 31.174, df1 = 592, df2 = 3561, p-value &lt; 2.2e-16\nalternative hypothesis: significant effects\n\n\nWith an F of 31 and this number of degrees of freedom, the F test for individual effects is significant. We must reject the null hypothesis, that there are no fixed individual effects.\nRegarding the F-test in which a pooled regression and a fixed effect model are compared to each other, we should go with an fixed effect model.\nThe fixed effects model models dynamics over time. The unobserved factors are not captured by the control variables, “the something in the back”. In the fixed effects model, though, the something in the back is conditioned by the observed variables, like assumed in the fixed effects model.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Panel Data</span>"
    ]
  },
  {
    "objectID": "panel_data.html#random-effects-model",
    "href": "panel_data.html#random-effects-model",
    "title": "3  Panel Data",
    "section": "3.11 Random Effects Model",
    "text": "3.11 Random Effects Model\nNote: In the following we learn typical problems with random effects models with panel data. This problems are also true multi-level random models!\n\n3.11.1 Ideas\nThe fixed effects model allows the unobserved individual effects to be correlated with the included variables.\nAs a consequence, the differences between units are strictly modeled as parametric shifts of the regression function. → for people who are not in the sample, we cannot predict for! This model might be viewed as applying only to the cross-sectional units in the study, not to additional ones outside the sample.\nMain idea for random effects models: The individual-specific effect is a random variable is uncorrelated with the explanatory variables.\nThe variation across entities is assumed to be random and uncorrelated to the predictor or independent variables in the model.\n\n\n3.11.2 Model specification\nIf \\(N\\) is large, one may view the ‘effects’ as unobserved random variables and not as incidental parameters: random effects (RE). The model becomes more ‘parsimonious’, as it has less parameter\n\nso random effects models are attractive for large \\(N\\) and small \\(T\\)\nAssumption: If the individual effects are strictly uncorrelated with the regressors, then it might be appropriate to model the individual specific constant terms as randomly distributed across cross-sectional units.\n\nShort explanation of the concept:\n\n( If individual effects are strictly uncorrelated with the regressors, it may be appropriate to model the individual specific constant terms as randomly distributed across cross-sectional units. This view would be appropriate if we believe that sampled cross-sectional units were drawn from a large population.\nIf you have reason to believe that differences across entities have some influence on your dependent variable, then you should use random effects. In a random effects model, you need to specify those individual characteristics that may or may not influence the predictor variables. The problem with this is that data on some variables (i.e., individual characteristics such as innate ability) may not be available, hence leading to omitted variable bias in the model.\nAn advantage of using random effects method is that you can include time invariant variables (e.g., geographical contiguity, distance between states) in your model. In the fixed effects model, these variables are absorbed by the intercept.\n\nAnother advantage: The random effects model elaborates on the fixed effects model by recognizing that, since the individuals in the panel are randomly selected, their characteristics, measured by the intercept \\(β_{1i}\\) should also be random.\nThe payoff to this form is that it greatly reduces the number of parameters to be estimated.\n!The cost is the possibility of inconsistent estimates, should the assumption turn out to be inappropriate! → if there is correlation with unobserved part and the variables included in the model, you get wrong estimates!\nthe unobserved individual heterogeneity can be assumed to be uncorrelated with the included variables, then the model may be formulated as\n\\[\n\\\\\ny_{it} = x′_{it}β + E[z′iα] + {z′iα − E[z′\niα]} + ϵ_{it} \\\\\n= x′_{it}β + (α + u_i ) + ϵ_{it}\n\\\\\n\\]\nwhere there are \\(K\\) regressors including a constant and the single constant term is the mean of the unobserved heterogeneity, \\(E[z′iα]\\). In a random effects model I assume that \\(α\\) is the same for everybody, so all individuals get the same intercept, I assume everybody is the same.\nThe shaky part is \\(u_i\\), the part hold responsible for the variance. It is this: \\(\\{z′iα] − E[z′iα]\\}\\).\nIn this case, it is not \\(\\tau^2\\), it is \\(\\sigma^2\\) that explain the variance of the group deviation, the \\(u_i\\).\nThe component \\(u_i\\) is the random heterogeneity specific to the with observation and is constant through time. So you don’t have to estimate for each individual a certain constant parameter. BUT: This comes with a price!\n\n\n\nAssumption\n\n\n \n\n\n3.11.3 Least Squares Estimation\n\n\n\nLeast Squares Estimation\n\n\n\n\n\nwihtin and between group estimtors\n\n\n\n\n3.11.4 Generalized Least Squares\n Only true for random effects model!\nweighting the data in the estimating procedure, making full use of the data, because between and within-group is regarded in this model.\n Testing Random Effects with the Lagrange multipliert test:\n$ H_0: _u = 0$ (or \\(Corr(w_{it, W_is})= 0)\\) \\(H_1: \\sigma^2_u \\not= 0\\)\nUnder the null hypothesis, LM is distributed as chi-squared with one degree of freedom.\n\n\n3.11.5 Exercise\n\nEstimate a random effects model.\n\n\nrandom &lt;- plm(lwage ~ exp + expSq + wks + bluecol + ind + south + smsa + married + sex + union + ed + black, \n              data = DAT, model = \"random\", # specify model\n              random.method = \"amemiya\", #stands for the Amemiya-MacCurdy estimation\n              index #use this method\n              = c(\"id\", \"time\"), effect=\"individual\") # define the effect, do you think its the individual or time? \nsummary(random)\n\nOneway (individual) effect Random Effect Model \n   (Amemiya's transformation)\n\nCall:\nplm(formula = lwage ~ exp + expSq + wks + bluecol + ind + south + \n    smsa + married + sex + union + ed + black, data = DAT, effect = \"individual\", \n    model = \"random\", random.method = \"amemiya\", index = c(\"id\", \n        \"time\"))\n\nBalanced Panel: n = 595, T = 7, N = 4165\n\nEffects:\n                  var std.dev share\nidiosyncratic 0.02304 0.15180 0.021\nindividual    1.06368 1.03135 0.979\ntheta: 0.9445\n\nResiduals:\n      Min.    1st Qu.     Median    3rd Qu.       Max. \n-1.9129800 -0.0679600  0.0073847  0.0742140  2.0130032 \n\nCoefficients:\n               Estimate  Std. Error z-value Pr(&gt;|z|)    \n(Intercept)  3.0303e+00  2.0907e-01 14.4944  &lt; 2e-16 ***\nexp          1.0915e-01  2.4055e-03 45.3760  &lt; 2e-16 ***\nexpSq       -4.8419e-04  5.3138e-05 -9.1121  &lt; 2e-16 ***\nwks          8.3771e-04  5.8933e-04  1.4215  0.15518    \nbluecolyes  -2.3848e-02  1.3477e-02 -1.7695  0.07680 .  \nind          1.5450e-02  1.5006e-02  1.0296  0.30320    \nsouthyes     4.1898e-03  3.1740e-02  0.1320  0.89498    \nsmsayes     -4.6397e-02  1.8697e-02 -2.4815  0.01308 *  \nmarriedyes  -3.7679e-02  1.8567e-02 -2.0294  0.04242 *  \nsexfemale   -1.6102e-01  1.3613e-01 -1.1829  0.23686    \nunionyes     3.6777e-02  1.4522e-02  2.5325  0.01132 *  \ned           1.3847e-01  1.5234e-02  9.0895  &lt; 2e-16 ***\nblackyes    -2.6570e-01  1.6615e-01 -1.5991  0.10979    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    242.65\nResidual Sum of Squares: 92.859\nR-Squared:      0.61731\nAdj. R-Squared: 0.6162\nChisq: 6697.38 on 12 DF, p-value: &lt; 2.22e-16\n\n\nIn the random effects model a single and constant term for the unobserved randomness is computed, so we have an intercept in here and can integrate time invariant variables like sex and if a person is black or not.\nAlternatively to the function plm we can use also the lm(), the results differs slightly from each other:\n\n# create a multilinear mixed model \nrandom_ml &lt;- lmer(lwage ~ exp + expSq + wks + bluecol + ind + south + smsa + married + sex + union + ed + black+(1|id),data = DAT)\nsummary(random_ml)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: lwage ~ exp + expSq + wks + bluecol + ind + south + smsa + married +  \n    sex + union + ed + black + (1 | id)\n   Data: DAT\n\nREML criterion at convergence: -519.5\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-11.9385  -0.3404   0.0412   0.4086  12.6948 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n id       (Intercept) 0.71114  0.8433  \n Residual             0.02357  0.1535  \nNumber of obs: 4165, groups:  id, 595\n\nFixed effects:\n              Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)  3.124e+00  1.773e-01  5.962e+02  17.621  &lt; 2e-16 ***\nexp          1.072e-01  2.456e-03  3.662e+03  43.669  &lt; 2e-16 ***\nexpSq       -5.140e-04  5.425e-05  3.666e+03  -9.476  &lt; 2e-16 ***\nwks          8.400e-04  6.046e-04  3.436e+03   1.389  0.16477    \nbluecolyes  -2.509e-02  1.379e-02  3.562e+03  -1.820  0.06887 .  \nind          1.383e-02  1.530e-02  3.712e+03   0.903  0.36633    \nsouthyes     5.748e-03  3.164e-02  4.129e+03   0.182  0.85585    \nsmsayes     -4.746e-02  1.898e-02  3.889e+03  -2.501  0.01244 *  \nmarriedyes  -4.131e-02  1.900e-02  3.556e+03  -2.174  0.02973 *  \nsexfemale   -1.753e-01  1.136e-01  4.870e+02  -1.544  0.12321    \nunionyes     3.869e-02  1.482e-02  3.672e+03   2.610  0.00908 ** \ned           1.357e-01  1.272e-02  4.911e+02  10.668  &lt; 2e-16 ***\nblackyes    -2.613e-01  1.381e-01  4.654e+02  -1.892  0.05908 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCorrelation matrix not shown by default, as p = 13 &gt; 12.\nUse print(x, correlation=TRUE)  or\n    vcov(x)        if you need it\n\n\n\nEstimate a random effects model using an ordinary least squares approach.\n\n\nrandom_ols &lt;- lm(lwage ~ exp + expSq + wks + bluecol + ind + south + smsa + married + sex + union + ed + black, \n              data = DAT)\nsummary(random_ols, vcov=vcovHC(random_ols, method=\"arellano\", cluster=\"id\")) ## get panel robust std. errors\n\n\nCall:\nlm(formula = lwage ~ exp + expSq + wks + bluecol + ind + south + \n    smsa + married + sex + union + ed + black, data = DAT)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.18965 -0.23536 -0.00988  0.22906  2.08738 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.251e+00  7.129e-02  73.662  &lt; 2e-16 ***\nexp          4.010e-02  2.159e-03  18.574  &lt; 2e-16 ***\nexpSq       -6.734e-04  4.744e-05 -14.193  &lt; 2e-16 ***\nwks          4.216e-03  1.081e-03   3.899 9.82e-05 ***\nbluecolyes  -1.400e-01  1.466e-02  -9.553  &lt; 2e-16 ***\nind          4.679e-02  1.179e-02   3.967 7.39e-05 ***\nsouthyes    -5.564e-02  1.253e-02  -4.441 9.17e-06 ***\nsmsayes      1.517e-01  1.207e-02  12.567  &lt; 2e-16 ***\nmarriedyes   4.845e-02  2.057e-02   2.355   0.0185 *  \nsexfemale   -3.678e-01  2.510e-02 -14.655  &lt; 2e-16 ***\nunionyes     9.263e-02  1.280e-02   7.237 5.45e-13 ***\ned           5.670e-02  2.613e-03  21.702  &lt; 2e-16 ***\nblackyes    -1.669e-01  2.204e-02  -7.574 4.45e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3494 on 4152 degrees of freedom\nMultiple R-squared:  0.4286,    Adjusted R-squared:  0.427 \nF-statistic: 259.5 on 12 and 4152 DF,  p-value: &lt; 2.2e-16\n\n\nAfter the model creation, you can access the coefficents and standard errors with this code:\n\ncoeftest(random_ols, vcovHC(random_ols, method = \"arellano\"))\n\n\nt test of coefficients:\n\n               Estimate  Std. Error  t value  Pr(&gt;|t|)    \n(Intercept)  5.2511e+00  7.4794e-02  70.2078 &lt; 2.2e-16 ***\nexp          4.0105e-02  2.1678e-03  18.5001 &lt; 2.2e-16 ***\nexpSq       -6.7338e-04  4.8142e-05 -13.9874 &lt; 2.2e-16 ***\nwks          4.2161e-03  1.1506e-03   3.6643 0.0002511 ***\nbluecolyes  -1.4001e-01  1.4991e-02  -9.3394 &lt; 2.2e-16 ***\nind          4.6789e-02  1.2033e-02   3.8884 0.0001025 ***\nsouthyes    -5.5637e-02  1.2787e-02  -4.3509 1.388e-05 ***\nsmsayes      1.5167e-01  1.2118e-02  12.5157 &lt; 2.2e-16 ***\nmarriedyes   4.8449e-02  2.0605e-02   2.3513 0.0187560 *  \nsexfemale   -3.6779e-01  2.3229e-02 -15.8332 &lt; 2.2e-16 ***\nunionyes     9.2627e-02  1.2379e-02   7.4823 8.864e-14 ***\ned           5.6704e-02  2.7368e-03  20.7193 &lt; 2.2e-16 ***\nblackyes    -1.6694e-01  2.0874e-02  -7.9973 1.635e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWhile the fixed effect model only account for within variance, the random effects model can do both:\n\nin the fixed effects model the coefficients indicate the change on the output \\(y\\) when the predictor change one unit over time\nBeta coefficients indicate the change in the output \\(y\\) when the predictors change one unit over time and across entities (average effect) → it is the average effect across the entities\n\nThe difference between the OLS model and the GLS model is, that the GLS model is way more efficient, because it yields to homoscedastic disturbance terms. The random OLS model (I don’t really get why it is a random OLS, because it seems to me the same model with exactly the same results you have asked in ?sec-ex.pool for) does not account for cross-sectional heteroskedasticity and assumes, that all entities are uncorrelated to each other, even the individuals itself. It doesn’t take individual specific effects into account.\nBecause the random effects model accounts for between and within variability the estimates show the effects across individuals over time. With the Lagrange Multiplier Test we can test for random effects: It test for individual or time effects for panel models. It tests, if the variance in the random effects is zero. The LM-statistic follows a chi-squared distribution with 1 df, because we are testing for one measure only (the variance of the random effects term). If the nullhypothesis is rejected, the Random Effects Model is appropriate. With the Lagrange Multiplier test the Random Effects Model is compared to a pooled OLS, where no individual specific variation is assumed.\n\nplmtest(random, \"individual\", type=\"bp\")  \n\n\n    Lagrange Multiplier Test - (Breusch-Pagan)\n\ndata:  lwage ~ exp + expSq + wks + bluecol + ind + south + smsa + married +  ...\nchisq = 3497, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: significant effects\n\n\nNull hypothesis is clearly rejected, we should go with the Random Effects Model.\n\n\n3.11.6 Testing for Random Effects: Hausman’s Specification Test\nRandom or fixed effects model: Which should be used?\nFrom a purely practical standpoint, the dummy variable approach is costly in terms of degrees of freedom lost. On the other hand, the fixed effects approach has one considerable virtue: There is little justification for treating the individual effects as uncorrelated with the other regressors, as is assumed in the random effects model. The random effects treatment, therefore, may suffer from the inconsistency due to this correlation between the included variables and the random effect → be careful, we cannot back up the assumption random effect is build on!\nThe specification test devised by Hausman (1978) is used to test for orthogonality of the random effects and the regressors.\nThe test is based on the idea that under the hypothesis of no correlation (H0):\n\nOLS (in the LSDV model) and GLS are consistent, but\npure OLS is inefficient. → makes too much effort in data!\n\nWhereas under the alternative (H1)\n\nOLS (in the LSDV model) is consistent, but * GLS is not.\n\n→ random effects model is biased, if there is a correlation between unobserved and included variables, then: OLS and GLS differs. Under null hypothesis the two estimates (OLS in LSDV and GLS) should not differ systematically and the test can be based on the difference.\nHow much the \\(\\beta\\)s in both models differ from each other?\nTherefore, under the null hypothesis, the two estimates (OLS in LSDV and GLS) should not differ systematically, and a test can be based on the difference.\nto test the difference: Test statistic (based on the Wald criterion):\n\\(W = [b − \\hatβ]′Ψ−1[b − \\hatβ],\\)\nwhere \\(b\\) is the LSDV estimator and \\(\\hatβ\\) is the GLS estimator of the slope parameters of the linear panel model. And \\(Var [b] − Var [\\hatβ] = Ψ.\\)\nFor \\(\\hatΨ\\), we use the estimated covariance matrices of the slope estimator in the LSDV model and the estimated covariance matrix in the random effects model, excluding the constant term. Thus, the Hausman test for the fixed and random effects regressions is based on the parts of the coefficient vectors and the asymptotic covariance matrices that correspond to the slopes in the models, that is, ignoring the constant term(s).\nUnder the null hypothesis, \\(W\\) has a (limiting) chi-squared distribution with \\(K − 1\\) degrees of freedom.\n\n\n3.11.7 Improving Random Effects Model\nEven with a test available, choosing between the fixed and random effects specification presents a bit of a dilemma.\nBoth specifications have unattractive shortcomings: * The fixed effects approach is robust to correlation between the omitted heterogeneity and the regressors, but it proliferates parameters and cannot accommodate time-invariant regressors. * The random effects model hinges on an unlikely assumption, that the omitted heterogeneity is uncorrelated with the regressors.\nThere are several modifications of the random effects model that (at least partly) overcome its deficit.\nConcretely: The failure of the random effects model is that the mean independence assumption,\n\\[E[c_i | X_i ] = E[α + u_i | X_i ] = α\\]\nis likely untenable.\n\n\n3.11.8 Mundlak’s Approach\n Mundlak’s approach is often used as a compromise between the fixed and the random effects model.\nOne side benefit: provides another convenient approach to Hausman test. With the given model specification, the difference between “fixed effects” and “random effects” model is the nonzero \\(\\gamma\\).\nAs such, a statistical test of the null hypothesis that γ equals zero is an alternative approach to the Lagrange multiplier and the Hausman test → an accordant test is the Variable Addition Test * under the null hypothesis (\\(\\gamma = 0\\)) VAT has chi=suqared distribution with \\(k\\) degrees of freedom (\\(k\\) is the number of means \\(\\overline{x}'_i\\) considered)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Panel Data</span>"
    ]
  },
  {
    "objectID": "panel_data.html#which-model-to-take",
    "href": "panel_data.html#which-model-to-take",
    "title": "3  Panel Data",
    "section": "3.12 Which model to take?",
    "text": "3.12 Which model to take?\n\n\n3.12.1 Exercise\n\nTest whether a fixed effects or a random effects model fits the data better.\n\n\n#use phtest, this is the function for the Hausman test\nphtest(fixed_w, # fixed effects\n       random) # random effects\n\n\n    Hausman Test\n\ndata:  lwage ~ exp + expSq + wks + bluecol + ind + south + smsa + married +  ...\nchisq = 302.46, df = 9, p-value &lt; 2.2e-16\nalternative hypothesis: one model is inconsistent\n\n\nThe Hausman test shows, that the fixed effects model, the LSDV model is consistent, while the random effects model is not. The null hypothesis can be rejected: We can’t prove, that there is no correlation between the unobserved individual-specific effects are uncorrelated with the regressors. In not proving this, the pivotal assumption of a random regression model is violated.\nWe should go with the fixed effects model.\n\nFind out which panel model fits the data best by following the decision tree diagram given on the previous slide. Explain.\n\nThe data is introduced to us as as a sample data drawn from the years 1976- 1982 from the “Non-Survey of Economy Opportunity”.Because panel data is often sampled as random observation from the population, it highly probable, that this condition is also true for this data set.\nIn the former steps, we have created a fixed (fixed_w) and random effects regression (random). The Hausman test in the prior exercise is a strong indicator to go with the fixed effect model. It indiciates differences in the coefficients between OLS and GLS.\n\nImplement Mundlak’s approach. What do you find? Interpret.\n\n\n#for Mundlak's approach we include group means for the regressors of interests, in this case the experience and work hours\nA1 &lt;- aggregate(DAT$exp, by=list(DAT$id), mean)\nA2 &lt;- aggregate(DAT$expSq, by=list(DAT$id), mean)\nA3 &lt;- aggregate(DAT$wks, by=list(DAT$id), mean)\nM &lt;- cbind(A1, A2[,2], A3[,2])\ncolnames(M) &lt;- c(\"id\", \"exp_m\", \"expSq_m\", \"wks_m\")\nDAT &lt;- merge(DAT, M, by=\"id\", all.x=TRUE)\nrandom_mun &lt;- plm(lwage ~ exp + expSq + wks + bluecol + ind + south + smsa + married + sex + union + ed + black + exp_m + expSq_m + wks_m, \n                data = DAT, model = \"random\", random.method = \"amemiya\", index = c(\"id\", \"time\"), effect=\"individual\")\nsummary(random_mun)\n\nOneway (individual) effect Random Effect Model \n   (Amemiya's transformation)\n\nCall:\nplm(formula = lwage ~ exp + expSq + wks + bluecol + ind + south + \n    smsa + married + sex + union + ed + black + exp_m + expSq_m + \n    wks_m, data = DAT, effect = \"individual\", model = \"random\", \n    random.method = \"amemiya\", index = c(\"id\", \"time\"))\n\nBalanced Panel: n = 595, T = 7, N = 4165\n\nEffects:\n                  var std.dev share\nidiosyncratic 0.02304 0.15180 0.021\nindividual    1.06368 1.03135 0.979\ntheta: 0.9445\n\nResiduals:\n      Min.    1st Qu.     Median    3rd Qu.       Max. \n-1.8247265 -0.0535394  0.0041373  0.0620001  1.9479602 \n\nCoefficients:\n               Estimate  Std. Error z-value  Pr(&gt;|z|)    \n(Intercept)  5.0475e+00  6.3666e-01  7.9281 2.225e-15 ***\nexp          1.1322e-01  2.3045e-03 49.1295 &lt; 2.2e-16 ***\nexpSq       -4.1816e-04  5.0917e-05 -8.2126 &lt; 2.2e-16 ***\nwks          8.3376e-04  5.5926e-04  1.4908  0.136010    \nbluecolyes  -2.3786e-02  1.2775e-02 -1.8619  0.062612 .  \nind          1.9860e-02  1.4226e-02  1.3960  0.162721    \nsouthyes    -1.4556e-02  3.0102e-02 -0.4835  0.628709    \nsmsayes     -3.2588e-02  1.7736e-02 -1.8374  0.066152 .  \nmarriedyes  -2.9272e-02  1.7604e-02 -1.6628  0.096360 .  \nsexfemale   -4.1521e-01  1.3072e-01 -3.1763  0.001491 ** \nunionyes     3.4746e-02  1.3781e-02  2.5213  0.011694 *  \ned           7.1661e-02  1.4774e-02  4.8504 1.232e-06 ***\nblackyes    -1.4222e-01  1.5764e-01 -0.9022  0.366961    \nexp_m       -7.7882e-02  1.7098e-02 -4.5549 5.240e-06 ***\nexpSq_m     -1.7464e-04  3.7699e-04 -0.4632  0.643188    \nwks_m        7.9190e-03  1.2226e-02  0.6477  0.517157    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    242.65\nResidual Sum of Squares: 83.374\nR-Squared:      0.65639\nAdj. R-Squared: 0.65515\nChisq: 7925.89 on 15 DF, p-value: &lt; 2.22e-16\n\n\nImproving the random effects model (random) with Mundlak’s approach and including the group means We can see that R-squared has incread from 0.61 in the random to 0.65 in the random_mun.\nThe effects in the individual and theta stay the same.\nAlso, there are substantial differences in the coefficients:\n\nThe intercept in the random_mun is quite higher. * Exp and ExpSq is significant and positive in both models, but while exp is higher in random_mun, expSq is lower in random _mun compared to the random model.This can be explained the significant positive effect of the group mean of experience. * Including the group mean of Exp time invariant variables like sothyes, unionyes or blackyes, marriedyes, sexfemale and ed differs a lot!\n\n→ not treating the random effect and regressors as uncorrelated but conditional connected by the group mean of especially exp make huge differences in the strength of effects.\n\nConduct the Hausman test also with the Mundlak model. What do you find?\n\n\nphtest(fixed_w, random_mun) \n\n\n    Hausman Test\n\ndata:  lwage ~ exp + expSq + wks + bluecol + ind + south + smsa + married +  ...\nchisq = 2.1255, df = 9, p-value = 0.9893\nalternative hypothesis: one model is inconsistent\n\n\nComparing the former fixed effects model to the random effects model with Mundlak’s approach, the Hausman test is very different to the results in Exercise 1. Here we have a strong indicator, that there is no correlation between the random effect and the regressors, because including the group means for experience in years holds account for possible conditional relationships between the unobserved and the variables included.\nIn this case, we should go with the random effects model with Mundlak’s approach.\n\n#fitting an OLS regression with means included \nrandom_mun_ols &lt;- lm(lwage ~ exp + expSq + wks + bluecol + ind + south + smsa + married + sex + union + ed + black + exp_m + expSq_m + wks_m, \n                 data = DAT)\nsummary(random_mun_ols, vcov=vcovHC(random_mun_ols, method=\"arellano\", cluster=\"id\")) #get the pabel robust std. errors\n\n\nCall:\nlm(formula = lwage ~ exp + expSq + wks + bluecol + ind + south + \n    smsa + married + sex + union + ed + black + exp_m + expSq_m + \n    wks_m, data = DAT)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.05493 -0.19779  0.00387  0.19910  2.00608 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.1123391  0.0856609  59.681  &lt; 2e-16 ***\nexp          0.1130584  0.0049326  22.921  &lt; 2e-16 ***\nexpSq       -0.0004065  0.0001091  -3.726 0.000197 ***\nwks          0.0008848  0.0011978   0.739 0.460111    \nbluecolyes  -0.1372689  0.0127533 -10.763  &lt; 2e-16 ***\nind          0.0512831  0.0102737   4.992 6.23e-07 ***\nsouthyes    -0.0606274  0.0108997  -5.562 2.83e-08 ***\nsmsayes      0.1619797  0.0105342  15.377  &lt; 2e-16 ***\nmarriedyes   0.0789305  0.0179278   4.403 1.10e-05 ***\nsexfemale   -0.3452255  0.0219195 -15.750  &lt; 2e-16 ***\nunionyes     0.0965354  0.0114552   8.427  &lt; 2e-16 ***\ned           0.0544956  0.0022799  23.903  &lt; 2e-16 ***\nblackyes    -0.1594888  0.0191794  -8.316  &lt; 2e-16 ***\nexp_m       -0.0803979  0.0053347 -15.071  &lt; 2e-16 ***\nexpSq_m     -0.0001695  0.0001179  -1.438 0.150481    \nwks_m        0.0081826  0.0019437   4.210 2.61e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3039 on 4149 degrees of freedom\nMultiple R-squared:  0.5678,    Adjusted R-squared:  0.5663 \nF-statistic: 363.4 on 15 and 4149 DF,  p-value: &lt; 2.2e-16\n\n\nWith this model, we get the panel robust standard errors from a linear multi level model, fittd by REMl. Instead of using one single and constant term for the unobserved randomness like in the random effects model, we group the models by id. Linear multi level models account for period heteroskedasticity and time variation and therefore appropriate to investigate within variation. In the linear multi level model nearly all variables are significant. Though, the estimates are very similar to the random effects using Mundlak’s.\nIt is important to choose a model accordingly to our research interest, because not all models can explain the same things!\nOpen question, because I am not really confident in the former answer: I don’t understand, why the random_mun_ols is included in addition to the random effects model with Mundlak’s approach. Is the lm model an analysis of within variation and the random effects is an analysis for within and between variation? How have you interpreted the results of the different models ? And I don’t see the random effect in the random_mun_ols code, for me its a simple linear regression including the means for the variables, we are interested in. Or does we use the random_mun_ols only to get panel robust std. errors for our random_mun model?\nIn the next step, we can test with the Lagrange Multipliert test for individual and/ or effects of the panel model. The test compares the random effects model including the Mundlak’s approach to a pooled regression, where no individual speicfic variation is assumed.\n\nplmtest(random_mun, \"individual\", type=\"bp\")  \n\n\n    Lagrange Multiplier Test - (Breusch-Pagan)\n\ndata:  lwage ~ exp + expSq + wks + bluecol + ind + south + smsa + married +  ...\nchisq = 6707.2, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: significant effects\n\n\nNull hypothesis is clearly rejected, we should go with the Random Effects Model.\n\nplmtest(random, \"individual\", type=\"bp\")  \n\n\n    Lagrange Multiplier Test - (Breusch-Pagan)\n\ndata:  lwage ~ exp + expSq + wks + bluecol + ind + south + smsa + married +  ...\nchisq = 3497, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: significant effects",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Panel Data</span>"
    ]
  },
  {
    "objectID": "missing-data.html",
    "href": "missing-data.html",
    "title": "4  Missing data and Statistical Modelling",
    "section": "",
    "text": "4.1 Load libraries\nRecommended literature:\nVan Buuren, S. (2018). Flexible imputation of missing data. CRC press.Online Version",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing data and Statistical Modelling</span>"
    ]
  },
  {
    "objectID": "missing-data.html#basics",
    "href": "missing-data.html#basics",
    "title": "4  Missing data and Statistical Modelling",
    "section": "4.2 Basics",
    "text": "4.2 Basics\nSplit Questionnaire Designs (SQDs) randomly assign respondents to different fractions of the full questionnaire (modules) and, subsequently, impute the data that are missing by design\nkey question: Why is the data missing?\n\nby design\nnon-response\nin panel: wave non-response\nsometimes information can be imputed from the last wave like the university degree or the biological birth age, sex → last observational carried forward\n\n\n4.2.1 Missing data mechanisms\n1. Missing completely at random (MCAR): Missingness probability independent of observed & unobserved variable values \\(Y\\) \\(f (R | Y ) = f (R)\\) (with Y studied variables, R missing data indicator) → wie can deal easy with\n2. Missing at random (MAR): Conditioned on observed variable values \\(Y_{obs}\\) , missingness probability is independent of unobserved variable values \\(Y_{mis}\\) \\(f (R | Y ) = f (R |Y_{obs})\\) → we can control for e.g. the interviewer we can control for\n3. Missing not at random (MNAR, i.e. not ignorable): Missingness mechanism is neither MCAR nor MAR \\(f (R | Y ) = f (R | Y_{obs},Y_{mis})\\) i.e. the rich often don’t report their income or their wealth, so data is not missing at random → this is really problem and complicated to solve → topic of this class today\n\n\n4.2.2 Littles test for MCAR\nLiterature: Little, R. J. (1988). A test of missing completely at random for multivariate data with missing values. Journal of the American statistical Association, 83(404), 1198-1202.\nTests the null hypothesis that the missing data is MCAR\n\nThe idea is that conditional on the missing indicator \\(r_i\\), the means of the observed \\(y\\)s are expected to vary across different patterns, if the missingness mechanism is MAR.\n\\(H_0\\): missingness mechanism is MCAR, \\(H1\\): missingness mechanism is not MCAR\np-value \\(&lt; 0.05\\): missing data mechanism is not MCAR\n\nNote: Implemented in the R package ‘naniar’, function mcar_test. AND: takes a little while, be patient.\n\n\n4.2.3 Exercise\nLoad data:\n\nDAT &lt;- read_dta(\"data/3.Example1.dta\")\nsummary(DAT)\n\n       id            grade            sex              comp        \n Min.   :    9   Min.   :1.000   Min.   :0.0000   Min.   :-6.7697  \n 1st Qu.: 9326   1st Qu.:2.000   1st Qu.:0.0000   1st Qu.:-0.7458  \n Median :18734   Median :3.000   Median :0.0000   Median :-0.0180  \n Mean   :18442   Mean   :2.956   Mean   :0.4952   Mean   :-0.0607  \n 3rd Qu.:27580   3rd Qu.:4.000   3rd Qu.:1.0000   3rd Qu.: 0.6776  \n Max.   :36380   Max.   :6.000   Max.   :1.0000   Max.   : 4.8601  \n                 NA's   :848                      NA's   :2426     \n   eduMother           age             mig        specialNeeds    \n Min.   :0.0000   Min.   :13.58   Min.   :0.00   Min.   :0.00000  \n 1st Qu.:0.0000   1st Qu.:15.08   1st Qu.:0.00   1st Qu.:0.00000  \n Median :0.0000   Median :15.42   Median :0.00   Median :0.00000  \n Mean   :0.2272   Mean   :15.47   Mean   :0.16   Mean   :0.02661  \n 3rd Qu.:0.0000   3rd Qu.:15.75   3rd Qu.:0.00   3rd Qu.:0.00000  \n Max.   :1.0000   Max.   :20.08   Max.   :1.00   Max.   :1.00000  \n NA's   :2269                     NA's   :993    NA's   :8        \n     cognAb               GY        \n Min.   :-5.70067   Min.   :0.0000  \n 1st Qu.:-0.99414   1st Qu.:0.0000  \n Median :-0.22912   Median :0.0000  \n Mean   :-0.06858   Mean   :0.4203  \n 3rd Qu.: 0.85708   3rd Qu.:1.0000  \n Max.   : 3.98392   Max.   :1.0000  \n NA's   :130                        \n\n\nData:\n\nData on 9th grade school children from Germany (N=10,531); file: “ex3 1Level.dta”\nVariables: id id of pupil, sex (0 male, 1 female), age, GY (1 Gymnasium, 0 other), specialNeeds (0 no, 1 yes), cognAb (measure of cogn. abilities), grade (grade in GERM), mig (German native speaker, 0 no, 1 yes), eduMother (mother’s educational attainment, 1 university degree, 0 other), comp (orthography competence)\n\nWith a first look in the data with summary we can see, that the amount of missing data differs a lot for different variables.\n\nHave a look at the missingness pattern and find the proportion of missing values in each variable. Use the table command for this purpose, with the argument exclude=NULL Use the is.na” (or “!is.na”) function. Use the md.pattern function from the mice package.\n\n\ntable(is.na.data.frame(DAT),  exclude = NULL)\n\n\nFALSE  TRUE \n98636  6674 \n\n\nOverall, there are 6674 missing data points in the data set.\n\nmissP &lt;- md.pattern(DAT, plot=F)\n#Last rows of the table divided by the data to get an insight of percentages:\nround(missP[nrow(missP),]/nrow(DAT)*100,2) \n\n          id          sex          age           GY specialNeeds       cognAb \n        0.00         0.00         0.00         0.00         0.08         1.23 \n       grade          mig    eduMother         comp              \n        8.05         9.43        21.55        23.04        63.37 \n\n\nWhile the ID is not missing at all (and what would not make sense to expect missings here) there are high missing rates in mother’s educational attainment (21.55 %) and orthography competence (23.04 %).\nFor all variables with a missingness less than 5 % the data can be used without thinking twice. 5 % is a rule of thumb. If you do it right, you can also use variables with a missing rate about 60 %, but be sure about that the estimates are quite uncertain. Recommended: not more than 40 %.\nLooking not at single variables, but at complete cases:\n\ntable(complete.cases(DAT))/nrow(DAT)\n\n\n    FALSE      TRUE \n0.4442123 0.5557877 \n\n\nOnly about 55 % of pupils have completed all questions in the questionnaire.\n\nVisualize the missingness pattern using the following functions of the R package naniar (here “DAT” is the whole data set): “vis miss(DAT)” and gg miss upset(DAT) (See also https://cran.r-project.org/web/packages/naniar/vignettes/naniar-visualisation.html)\n\n\nvis_miss(DAT)\n\n\n\n\n\n\n\n\nIn this graph we can see the missings per variable. The The black lines show the missingness for each observation per variable. More black lines indicate more missings.\nA better tool for visualisation is this:\n\nlinked_miss &lt;-gg_miss_upset(DAT)\nlinked_miss\n\n\n\n\n\n\n\n\nThe absolute numbers of missings is depicted is in the graphs itself and we can see linked missingness.\n\nThe missings in a certain variable is linked to a bar, that indicates the absolute number of missings.\nThe connected dots shows, that the absolute number indiciated by the bar is missing in both variables.\nor in other words: The set size indicates how many NA is missing per variable. The intersection size show how many NA is missing in one variable and at least another.\nThe bars are ordered from the highest amount of missingness to the lowest.\n\n\nDescribe & interpret the resulting graphs.\n\nPlot 1:\n\noverall 6.3% of the data is missing.\nthe graph corresponds to the missing pattern in Section 4.2.3, there are a lot missings in comp and eduMother.\n\nPlot 2 gives more information that plot 1:\n\nmost cases missing in the competence, then in mothers education, then in migration, then in grade and then in cognitive ability\nfor example we can see there is one case, in which five variables are missing → this case should be excluded from analysis. Because if cases have missings in all main variables of interest, you should exclude these cases.\nwe can also see, there is a high number in missings for orthography competence and that there is a high missingsness for orthography competence AND mothers education in combination.\nThere is a high combination in the missingness for migration and mothers education. In all cases, where the question of migration is not answered, the question of mothers education is not answered, too.→ this is a strong inticator, that MCAR is not true and there are systematic missings.\n\n\nTest whether the missingness pattern indicates MCAR.\n\n\n# test with Little's missing completely at random test (MCAR)\nmcar_test(DAT) \n\n# A tibble: 1 × 4\n  statistic    df p.value missing.patterns\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;            &lt;int&gt;\n1     2093.   232       0               32\n\n\nThe p-value is so small, that MCAR must be rejected. There is no indicator, that the missings differ randomly. We must reject \\(H_0\\). Looking at the plot before, that is not suprisingly at all.\nThe Little Test split the data into groups and looks for patterns of missings in this groups. We have 10.000 rows in the data sets. You make different subsets of the sample and compute the mean of the variables. Then the means are compared in different groups. If it is random, the average of the average is always the same, because the holes are random. → if the holes are random, the holes don’t play a role in average computations and we should get the same values.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing data and Statistical Modelling</span>"
    ]
  },
  {
    "objectID": "missing-data.html#general-ways-to-deal-with-missing-data",
    "href": "missing-data.html#general-ways-to-deal-with-missing-data",
    "title": "4  Missing data and Statistical Modelling",
    "section": "4.3 General ways to deal with missing data",
    "text": "4.3 General ways to deal with missing data\n\nIgnore it:\n\n\nlistwise deletion, complete case analysis\npartly ok if MCAR, however, reduction in statistical power\nrule of thumb: if percentage of missing data \\(&lt; 5%\\) statistical uncertainty due to sampling and (parameter) estimation larger than gain when modeling / imputing missing data\nexception: if one has a special interest in the missingness process, e.g. why do less than 5% not participate in a study\n\n\nModel or impute it:\n\n\nnecessary for MAR and MNAR, otherwise biased estimated might result\n\noften in case of income: income is often not missing randomly. Aggregate level data of income differs from surveys on individual level → data for people with a wealth above 2 million are missing often in survey. On the side of the lower income we often have income levels around 399 Euro because of the tax benchmark of 400 Euro. People at lower level often respond in surveys because of the incentives.\n\nMNAR is problematic since the missingness probability depends in the missingness itself (often the case with income data)\nMNAR cannot fully be counteracted by a model or a imputation routine; a good approach is using sensitivity analysis (e.g. Zinn & Gnambs (2019). Modeling competence development in the presence of selection bias. BRM.)\nMAR Modelling, e.g. data augmentation (e.g. Tan, Tian, & Ng (2009). Bayesian missing data problems: EM, data augmentation and noniterative computation CRC Press.) or Full information maximum likelihood (FIML, see Enders 2010)\nMAR (multiple) imputation\n\ncompletely open source package",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing data and Statistical Modelling</span>"
    ]
  },
  {
    "objectID": "missing-data.html#multiple-imputation",
    "href": "missing-data.html#multiple-imputation",
    "title": "4  Missing data and Statistical Modelling",
    "section": "4.4 Multiple Imputation",
    "text": "4.4 Multiple Imputation\nOne way of dealing with missing data, but meanwhile the most used one.\n\nThere are many way of “imputing” missing values: hot deck imputation, mean imputation, regression imputation (impute by estimators of regression model)\nTo account for the variability induced by imputing missing values: impute missing values several times by making random draws from multivariate distributions\ndata is incomplete, fill with imputed data from a data set with predictive power, depends on the distribution of the data set you fill the holes and account for the uncertainty with random imputations from multivariate distributions. Then you analyse the data and pooled the results.\n\n\n\n\nMultiple Imputation\n\n\n\nyou need 20 or 30 data sets with different distributions for robust estimators\nThese multivariate distributions have to be informative for the missingness mechanism at hand\nNote: More data does not help more; data without any predictive power for the missingness mechanism just unnecessarily increases the variability between the imputed values\n\n\n4.4.1 Rubin’s Combing Rules\nWe pool the data with Rubin´s Rules (RR)\n\nare designed to pool parameter estimates, such as mean differences, regression coefficients, standard errors and to derive confidence intervals and p-values.\nWhen RR are used, it is assumed that the repeated parameter estimates are normally distributed. This cannot be assumed for all statistical test statistics, e.g. correlation coefficients. (For these test statistics, transformations are first performed before RR can be applied.) → Bayesian idea: if you estimate a parameter, Rubin assumes the parameter is a random value (not fixed), because it has a standard error. From this random distribution for the parameters you can compute a pooled parameter estimate.\nThe pooled parameter estimate \\(\\overlineθ\\) is computed as follows, it is the mean of the estimates:\n\n\\[\n\\overlineθ = \\frac{1}{m}\\sum^m_{i=1}θ_i\n\\]\n\nwith \\(m\\) is the number of imputed datasets, \\(θ_i\\) is the sum of the parameter estimate (i.e. mean difference) using the \\(ith\\) imputed dataset.\nThe pooled standard error is derived from different components that reflect the within (the sampling variance of the mean within a data set for each data set, measured by the standard error) and between (the sampling variance of a parameter \\(X\\) between the different data sets) sampling variance of the mean difference in the multiple imputed datasets.\n\nWithin imputation variance:\n\naverage of the mean of the within variance estimate, i.e. squared standard error, in each imputed dataset\n\n\\[\nV_w= \\frac{1}{m}\\sum^m_{i=1}SE^2_i\n\\]\n\nWith \\(SE^2_i\\) is the sum of the squared standard error (SE), estimated in each imputed data set \\(i\\) * reflects the sampling variance, i.e. the precision of the parameter of interest in each completed dataset\nwill be large in small samples and small in large samples\n\nBetween imputation variance:\n\nVariance due to imputing missing data\nestimated by taking the variance of the parameter of interest estimated over imputed datasets\n\n\\[\nV_b= \\frac{\\sum^m_{1=1}(\\theta_i -\\overline\\theta_i)^2  }{m-1}\n\\]\n\nThis formula is equal to the formula for the (sample) variance which is commonly used in statistics.\nThis value is large when the level of missing data is high and smaller when the level of missing data is small\nFor significance testing a univariate Wald test is used, see van Buuren (2018) The derivation of the degrees of freedom for testing, how to get p-values, and confidence intervals is described in very detail in as well",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing data and Statistical Modelling</span>"
    ]
  },
  {
    "objectID": "missing-data.html#issues-in-multivariate-imputation",
    "href": "missing-data.html#issues-in-multivariate-imputation",
    "title": "4  Missing data and Statistical Modelling",
    "section": "4.5 Issues in multivariate imputation",
    "text": "4.5 Issues in multivariate imputation\n\nMost imputation models for \\(Y_j\\) (the missing data point in the variable j) use the remaining columns \\(Y_{-j}\\) (the values from the other variables that are not missing) as predictors\nit builds on the idea of MAR: The probability that an observation is missing may depend on \\(Y_{obs}\\) but not on \\(Y_{mis}\\), so the probability that \\(Y_j\\) is missing for an individual may be related to the values of variables \\(Y_h, Y_k,\\dots\\)\nThe rationale is that conditioning on \\(Y_{-j}\\) preserves the relations among the \\(Y_j\\) \\(Y_j\\) in the imputed data\nPractical problems that can occur in multivariate missing data\n\nThe predictors \\(Y_{-j}\\) themselves can contain missing values, so if competence is the \\(Y\\), maybe although the missings for competence are exluded to impute the others, there are still missings in mothers education\n“Circular” dependence can occur, where \\(Y^{mis}_j\\) depends on \\(Y^{mis}_h\\) , and \\(Y^{mis}_h\\) depends \\(Y^{mis}_j\\) with \\(h \\not= j\\) , because in general \\(Y_j\\) and \\(Y_h\\) are correlated (even given other variables)\nVariables are of different types (e.g., binary, unordered, ordered, continuous); application of theoretically convenient models (multivariate normal) theoretically inappropriate\n\nmost models are based on normal distribution but in fact in the actual worlds is not true\n\nEspecially with large proportion of missingness and small sample size, collinearity or empty cells can occur\nThe ordering of the rows and columns can be meaningful, e.g., as in longitudinal data\nThe relation between \\(Y_j\\) and predictors \\(Y_{−j}\\) can be complex (non-linear, subject to censoring, etc.) Imputation can create impossible combinations, such as pregnant fathers\n\n\n→ ALL that problems can be coped with! mice can handle it (:\n\n4.5.1 Congeniality\n\nCongeniality: Relation between the imputation model and the analysis model\nImputation model should be more general than the substantive model, i.e., analysis procedure should be congenial to the imputation model\nLeast condition: all variables of the analysis model have to be part of the imputation model → imputation model can be larger, but not the other way around.\nNon-congeniality may lead to biased parameter estimates of the analysis model",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing data and Statistical Modelling</span>"
    ]
  },
  {
    "objectID": "missing-data.html#single-level-imputation-of-mar-data",
    "href": "missing-data.html#single-level-imputation-of-mar-data",
    "title": "4  Missing data and Statistical Modelling",
    "section": "4.6 Single level imputation of MAR data",
    "text": "4.6 Single level imputation of MAR data\n\n4.6.1 Joint Modelling (JM)\n– Everything is included in one step. Skipped, because not often used nowadays –\nGeneral idea: - Assumes ignorability: applicable under MCAR or MAR JM starts from the assumption that the data can be described by a multivariate distribution - Imputations are created as draws from the fitted distribution - The model can be based on any multivariate distribution - The multivariate normal distribution is most widely applied - Further details: e.g. Schafer, J. L. 1997. Analysis of Incomplete Multivariate Data. London: Chapman & Hall",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing data and Statistical Modelling</span>"
    ]
  },
  {
    "objectID": "missing-data.html#fully-conditional-specification-fcs",
    "href": "missing-data.html#fully-conditional-specification-fcs",
    "title": "4  Missing data and Statistical Modelling",
    "section": "4.7 Fully Conditional Specification (FCS)",
    "text": "4.7 Fully Conditional Specification (FCS)\n\nFCS imputes multivariate missing data on a variable-by-variable basis → step-wise & Specification of an imputation model for each incomplete variable required; creation of imputations per variable in an iterative fashion\n\ni.e. for income a ln model, because income is distributed logarithmic normally → ln income is a ln income model of the variables of the income in the data set. Consequently, you need a good prediction model that is feasible, that you can use as an imputation model\n\nFCS specifies the multivariate distribution \\(P(Y ,X, R | θ)\\) through a set of conditional densities \\(P(Y_j | X,Y_j , R, ϕ_j )\\) (I identify Y_j by another variable X, accounting for the missingness with the last \\(ϕ_j\\), the parameters in the prediction model)\nThis conditional density is used to impute \\(Y_j\\) given \\(X,Y−j\\) and \\(R\\)\nStarting from simple random draws from the marginal distribution, imputation under FCS is done by iterating over the conditionally specified imputation models FCS directly specifies the conditional distributions from which draws should be made → bypasses the need to specify a multivariate model for the data (as needed in JM)\n\n\n\n\nMICE Algorithm\n\n\n\nThe MICE algorithm is a Markov chain Monte Carlo (MCMC) method, where the state space is the collection of all imputed values\nMore specifically, (if conditionals are compatible) the MICE algorithm is a Gibbs sampler: a Bayesian simulation technique that samples from the conditional distributions in order to obtain samples from the joint distribution\nMind: in the MICE algorithm, the conditional distributions are under direct control of the imputer: the joint distribution only implicitly known & may not exist\nFrom theoretical view point: not as good (we do not know to which the algo converges), in practice: algo works very well (shown by a multitude of simulations) + a testical toolbox is missing, only graphical tests are available at the moment\nDiagnostics of imputations are necessary (e.g. having a look at the trace plots produced by the sampler)\n\n\n4.7.1 Compatibility\n\nTwo conditional densities \\(p(Y1 | Y2)\\) (income depending on age) and \\(p(Y1 | Y2)\\) (age depending on income) are compatible if a joint distribution, p(Y1,Y2) exists that has \\(p(Y1 | Y2)\\) and \\(p(Y1 | Y2)\\) as its conditional densities\nPROBLEM: this is not always clear, i.e. pregnancy and gender → then the algorithm gives not feasible results\nCan the imputed data be trusted when we cannot find such a joint distribution? Incompatibility easily arises if deterministic functions of the data are imputed along with their originals, especially the case for interaction terms, quadratic terms\nSuch terms introduce feedback loops and impossible combinations into the system, which can invalidate the imputations\nSimulation studies: imputations robust against violations of compatibility, as long as each conditional model is correctly specified\n\ni.e. if you are interested in pregnancy, you make one model for men and another for women, only women are included in the imputation model\n\n\n\n\n4.7.2 Imputation Techniques\n\n\n\nImputation Methods\n\n\nMEANWHILE EXTENDED! → Complete List of Mice Methods in R\n\n“Logreg”, “polyreg” and “polr” tend to preserve the main effects well provided that the parameters are identified and can be reasonably well estimated - Often: the ratio of the number of fitted parameters relative to the number of events easily drops below 10 → use more robust methods, like predictive mean matching or classification and regression trees (CART)\n\nIn the following: Two methods explained, that doing well in practice!\n\n4.7.2.1 Predictive Mean Matching (PMM), Algorithm\npmm in library(mice) → very robust, often put in\n\nPMM calculates the predicted value of target variable Y according to the specified imputation model\nFor each missing entry: PMM uses a set of candidate donors (3, 5 (the default number of donors), 10) from all complete cases that have predicted values closest to the predicted value for the missing entry\n\nfrom the complete cases I can compute a regression line. The donors are the complete/ observed cases, that donor their distance from the regression line for the computation of the value of the missing cell\nOne donor is randomly drawn from the candidates, and the observed value of the donor is taken to replace the missing value The assumption is the distribution of the missing cell is the same as the observed data of the candidate donors → the missing cells are also part of the predicted regression line\n\nPMM works best with large samples\nPMM is fairly robust to transformations of the target variable (e.g. imputing \\(log(Y)\\) yields similar results to imputing \\(expY\\)) Less vulnerable to model misspecification (e.g. mostly linear model is not in context of imputation)\n\nbecause imputation is based on the donors, if the donors (mis-)specificate the imputed values will does the same.\n\n\nDescription of the algorithm:\n\nAdapted from https://statisticalhorizons.com/predictive-mean-matching\nSuppose there is a single variable \\(Y\\) that has some cases with missing data, and a set of variables \\(X\\) (with no missing data) that are used to impute \\(Y\\).\n\n\nFor cases with no missing data, estimate a linear regression of Y on X, producing a set of coefficients b\nMake a random draw from the posterior predictive distribution of \\(b\\), producing a new set of coefficients \\(b⋆\\). (Typically this is a random draw from a multivariate normal distribution with mean \\(b\\) and the estimated covariance matrix of \\(b\\) (with an additional random draw for the residual variance). This step is necessary to produce sufficient variability in the imputed values, and is common to all proper methods for multiple imputation.)\nUsing \\(b⋆\\), generate predicted values for \\(Y\\) for all cases, both those with data missing on \\(Y\\) and those with data present.\nFor each case with missing \\(Y\\), identify a set of cases with observed \\(Y\\) whose predicted values are close to the predicted value for the case with missing data.\nFrom among those close cases, randomly choose one and assign its observed value to substitute for the missing value.\nRepeat steps 2 through 5 for each completed data set.\n\nNote: Here, linear regression is just used to construct a metric for matching cases with missing data to similar cases with data observed.\nCaution: SPSS and Stata (here it is called ICE), have implemented PMM with a default setting of only one donor that actually, invalidates the method\nPitfalls - There’s no mathematical theory to justify it (only simulations) - Danger of the duplication of the same donor value many times: - Problem more likely to occur if the sample is small, or if there are many more missing data than observed data in a particular region of the predicted value (likely under high proportion of missing values) - Method does not work for a small number of predictors (e.g. Heitjan and Little 1991 for two predictors only) - May be inadequate in the presence of strong nonlinear relations (i.e. if imputation model is strongly misspecified) → Any terms appearing in the complete-data model need to be accounted for in the imputation model (mind: congeniality is necessary)\n\n\n4.7.2.2 Classification and Regression Trees (CART)\nNot only a method for imputation, but also for modelling & predicting, method for random forests\ncart method in mice package in R or approaches using random forests are also available, idea is similar (rf)\nThe name CART is from the 80s, nowadays known under the name Classification and Regression Trees\n\nCART (Breiman et al. 1984) are a popular class of machine learning algorithms\nCART models seek predictors and cut points in the predictors that are used to split the sample\nThe cut points divide the sample into more homogeneous subsamples\nThe splitting process is repeated on both subsamples, so that a series of splits defines a binary tree\n\n\n\n\nRegression trees\n\n\n\nyou identify different groups of data points in form of random nodes\nnow we not use donors, we use the random nodes to impute the missing data points → Approach:\n\n\nTraverse the tree and find the appropriate terminal node\nForm the donor group of all observed cases at the terminal node\nRandomly draw a case from the donor group, and take its reported value as the imputed value\n\nThe idea is identical to PMM, where the “predictive mean” is now calculated by a tree model instead of a regression model\nProperties\n\nCART is robust against outliers, can deal with multicollinearity and skewed distributions, and are flexible enough to fit interactions and nonlinear relations\nmany aspects of model fitting have been automated, so there is “little tuning needed by the imputer” (Burgette and Reiter 2010)\n\n\n\n\n4.7.3 Exercise\nWe want to find out the grade by the orthography competence, the sex and mothers education → Analysis Model:\n\\[\ngrade_i = β_0 + β_1comp_i + β_2sex_i + β_3eduMother_i + ϵ_i\n\\]\n\nConduct a complete case analysis.\n\n\ncc &lt;- lm(grade ~ comp + sex + eduMother, data=DAT)\nsummary(cc)\n\n\nCall:\nlm(formula = grade ~ comp + sex + eduMother, data = DAT)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.10780 -0.55174 -0.00511  0.52484  2.82866 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.08966    0.01582  195.28  &lt; 2e-16 ***\ncomp        -0.30627    0.01034  -29.63  &lt; 2e-16 ***\nsex         -0.28604    0.02111  -13.55  &lt; 2e-16 ***\neduMother   -0.12222    0.02459   -4.97 6.88e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7905 on 5955 degrees of freedom\n  (4572 observations deleted due to missingness)\nMultiple R-squared:  0.1935,    Adjusted R-squared:  0.1931 \nF-statistic: 476.2 on 3 and 5955 DF,  p-value: &lt; 2.2e-16\n\n\nThis model is a complete case analysis. All data available in the data set is used. Though, there are a lot missings in the explanatory variables as we have investigated in Section 4.2.3\n\nImpute missing values (using mice’s default settings):\n\nHave a look at your imputed data sets using the command complete: Setting action to 1 we can have a look in the first imputed data set. Overall, we have imputed 5 data sets.\nWhich imputation techniques have been used (imp$method). Do you think the default settings fit the data? Try some other reasonable imputation techniques.\n\n#access the method used for each variable in the data set\nmeth &lt;- imp$method\nmeth\n\n          id        grade          sex         comp    eduMother          age \n          \"\"        \"pmm\"           \"\"        \"pmm\"        \"pmm\"           \"\" \n         mig specialNeeds       cognAb           GY \n       \"pmm\"        \"pmm\"        \"pmm\"           \"\" \n\n\nBecause there is no missing data points in id, GY, sex and age, no method is reported. For all the other variables pmm is used.\neduMother is a binary and has a lot missings, consequently we go with a more robust method and switch to cart.\nI set mig also to cart, because it is also a binary and missings are high, too. This is not done in the given R File, though.\n\nmeth[\"eduMother\"] &lt;- \"cart\" #method for eduMother should be cart\nmeth[\"mig\"] &lt;- \"cart\"\nmeth\n\n          id        grade          sex         comp    eduMother          age \n          \"\"        \"pmm\"           \"\"        \"pmm\"       \"cart\"           \"\" \n         mig specialNeeds       cognAb           GY \n      \"cart\"        \"pmm\"        \"pmm\"           \"\" \n\n\nFirst of all, id is not a predictor for the other variables, so we set ID to 0\n\npred &lt;- imp$predictorMatrix\npred[, \"id\"] &lt;- 0\npred\n\n             id grade sex comp eduMother age mig specialNeeds cognAb GY\nid            0     1   1    1         1   1   1            1      1  1\ngrade         0     0   1    1         1   1   1            1      1  1\nsex           0     1   0    1         1   1   1            1      1  1\ncomp          0     1   1    0         1   1   1            1      1  1\neduMother     0     1   1    1         0   1   1            1      1  1\nage           0     1   1    1         1   0   1            1      1  1\nmig           0     1   1    1         1   1   0            1      1  1\nspecialNeeds  0     1   1    1         1   1   1            0      1  1\ncognAb        0     1   1    1         1   1   1            1      0  1\nGY            0     1   1    1         1   1   1            1      1  0\n\n\n\nimp &lt;- mice(DAT, predictorMatrix = pred, #define the prediction matrix\n            method=meth) #define method new, because we have changed the method for eduMother and set ID to 0 in predictor matrix\n\n\n iter imp variable\n  1   1  grade  comp  eduMother  mig  specialNeeds  cognAb\n  1   2  grade  comp  eduMother  mig  specialNeeds  cognAb\n  1   3  grade  comp  eduMother  mig  specialNeeds  cognAb\n  1   4  grade  comp  eduMother  mig  specialNeeds  cognAb\n  1   5  grade  comp  eduMother  mig  specialNeeds  cognAb\n  2   1  grade  comp  eduMother  mig  specialNeeds  cognAb\n  2   2  grade  comp  eduMother  mig  specialNeeds  cognAb\n  2   3  grade  comp  eduMother  mig  specialNeeds  cognAb\n  2   4  grade  comp  eduMother  mig  specialNeeds  cognAb\n  2   5  grade  comp  eduMother  mig  specialNeeds  cognAb\n  3   1  grade  comp  eduMother  mig  specialNeeds  cognAb\n  3   2  grade  comp  eduMother  mig  specialNeeds  cognAb\n  3   3  grade  comp  eduMother  mig  specialNeeds  cognAb\n  3   4  grade  comp  eduMother  mig  specialNeeds  cognAb\n  3   5  grade  comp  eduMother  mig  specialNeeds  cognAb\n  4   1  grade  comp  eduMother  mig  specialNeeds  cognAb\n  4   2  grade  comp  eduMother  mig  specialNeeds  cognAb\n  4   3  grade  comp  eduMother  mig  specialNeeds  cognAb\n  4   4  grade  comp  eduMother  mig  specialNeeds  cognAb\n  4   5  grade  comp  eduMother  mig  specialNeeds  cognAb\n  5   1  grade  comp  eduMother  mig  specialNeeds  cognAb\n  5   2  grade  comp  eduMother  mig  specialNeeds  cognAb\n  5   3  grade  comp  eduMother  mig  specialNeeds  cognAb\n  5   4  grade  comp  eduMother  mig  specialNeeds  cognAb\n  5   5  grade  comp  eduMother  mig  specialNeeds  cognAb\n\n\nPool the results of all your imputations:\n\nfit_ex1 &lt;- with(data=imp, exp=lm(grade ~ comp + sex + eduMother))\nsummary(pool(fit_ex1))\n\n         term   estimate   std.error  statistic        df       p.value\n1 (Intercept)  3.1136024 0.012982669 239.827604 225.05644 4.196059e-273\n2        comp -0.2986724 0.008347539 -35.779692  43.83768  4.411478e-34\n3         sex -0.2719736 0.017270884 -15.747520 193.26957  1.720737e-36\n4   eduMother -0.1319027 0.020334903  -6.486516 254.89505  4.536220e-10\n\n\nNot much differences in the coefficients between the imputation model and the complete cases model. The p-values are much more smaller in the imputation model than in the complete cases one.\nWhat have we done? What we have checked for is that the model is robust against missings at random (MAR).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing data and Statistical Modelling</span>"
    ]
  },
  {
    "objectID": "missing-data.html#predictor-selection",
    "href": "missing-data.html#predictor-selection",
    "title": "4  Missing data and Statistical Modelling",
    "section": "4.8 Predictor selection",
    "text": "4.8 Predictor selection\n\nGeneral rule: use every bit of available information yields multiple imputations that have minimal bias and maximal efficiency\nIncluding as many predictors as possible tends to make the MAR assumption more plausible, thus reducing the need to make special adjustments for MNAR mechanisms\nFor data sets containing hundreds or thousands of variables, using all predictors may not be feasible (because of multicollinearity and computational problems) → approach of machine learners, very problematic\nRule of thumb for Social Science: It is expedient to select a suitable subset of data that contains no more than 15 to 25 variables\nStrategy to select predictors, not more than twice of variables of the causal model should not included in the subset\n\nInclude all variables that appear in the complete-data model\nNote that interactions of scientific interest also need to be included in the imputation model\nInclude the variables that are related to the nonresponse (e.g. of which distributions differ between the response and nonresponse groups, check correlation using response indicator of that variable)\nInclude variables that explain a considerable amount of variance (identified by their correlation with the target variable)\n\ncreate on complete case data set and one with all the missings in it. Compare them to each other with a dummy variable approach, when the variable is missing in one data set and not in another it get a 1. Get a look, which variables are crucial for the missings\n\nRemove variables selected that way that have too many missing values within the subgroup of incomplete cases (measured by percentage of observed cases within this subgroup)\n\nONLY when this are additional model, not when they are part of the substantial model\n\n\nA useful feature of the mice function is the ability to specify the set of predictors to be used for each incomplete variable\n\nThe basic specification is made through the predictorMatrix argument, which is a square matrix of size ncol(data) containing 0/1 data\nEach row in “predictorMatrix” identifies which predictors are to be used for the variable in the row name The mice package contains several tools that aid in automatic predictor selection\nThe quickpred() function is a quick way to define the predictor matrix using the strategy outlined above\n\nThe mice() function detects multicollinearity, and solves the problem by removing one or more predictors for the model. (Each removal is noted in the loggedEvents element, see below.) → very helpful to look what is removed!\n\n\n4.8.1 Exercise\nHave a look at the predictor matrix that mice uses by default. Do you think that it makes sense regarding every aspect?\nThe prediction matrix in Section 4.7.3 shows the default setting, in which the square matrix of ncol(data) rows and columns with all 1’s, except for the diagonal. Note: For two-level imputation models (which have “2l” in their names) other codes (e.g, 2 or -2) are also allowed. This does not make a lot of sense, especially for the variable id, which is no predictor at all. It does not make sense either, that age can be predicted by gender or migration background or other variables. The same is true for sex and other variables.\nI can change the prediction matrix by hand with the following steps (for more description see here, Prediction Matrix Task 2)\n\npred[, \"id\"] &lt;- 0 #id is not used as a predictor for the others\npred[\"id\", ] &lt;- 0 #id cannot be predicted by the others\npred[\"sex\", ] &lt;- 0 #sex can not be predicted\npred[\"comp\", ] &lt;- c(0,1,0,0,1,0,1,1,1,1) # would assume that competence can be predicted by this values with a 1\npred[\"eduMother\", ] &lt;- c(0,0,0,1,0,1,1,0,1,1) # would assume such relationships\npred[\"age\", ] &lt;- 0 # can not be predicted\npred[\"mig\", ] &lt;- 0 #  can not be predicted\npred[\"specialNeeds\", ] &lt;- c(0,1,0,1,1,0,1,0,1,1) # would assume such relationships\npred[\"cognAb\", ] &lt;- c(0,1,0,1,1,1,1,1,0,1) # would assume such relationships\npred[\"GY\", ] &lt;- c(0,0,0,1,1,0,1,1,1,0) # would assume such relationships\npred\n\n             id grade sex comp eduMother age mig specialNeeds cognAb GY\nid            0     0   0    0         0   0   0            0      0  0\ngrade         0     0   1    1         1   1   1            1      1  1\nsex           0     0   0    0         0   0   0            0      0  0\ncomp          0     1   0    0         1   0   1            1      1  1\neduMother     0     0   0    1         0   1   1            0      1  1\nage           0     0   0    0         0   0   0            0      0  0\nmig           0     0   0    0         0   0   0            0      0  0\nspecialNeeds  0     1   0    1         1   0   1            0      1  1\ncognAb        0     1   0    1         1   1   1            1      0  1\nGY            0     0   0    1         1   0   1            1      1  0\n\n\nUse the quickpred() of mice.\n\nquickpred(DAT) # Selects predictors according to simple statistics\n\n             id grade sex comp eduMother age mig specialNeeds cognAb GY\nid            0     0   0    0         0   0   0            0      0  0\ngrade         1     0   1    1         1   1   1            1      1  1\nsex           0     0   0    0         0   0   0            0      0  0\ncomp          1     1   1    0         1   1   1            1      1  1\neduMother     1     1   0    1         0   1   1            0      1  1\nage           0     0   0    0         0   0   0            0      0  0\nmig           1     1   0    1         0   0   0            0      1  1\nspecialNeeds  0     0   0    1         0   1   0            0      1  1\ncognAb        0     1   0    1         1   1   1            1      0  1\nGY            0     0   0    0         0   0   0            0      0  0\n\n\nHuge difference to my hand made prediction matrix. While sex and age can’t also here be computed, in this matrix GY can’t but migration. Furthermore, id can be a predictor in here, the mothers education can be predicted from the grade and the age, what does not make a lot of sense.\nTherefore, in the next step we stricten the criteria:\nRedo imputation and analysis, considering only values with 40 percent usable cases. Hint: For this use the argument minpuc in the quickpred function.\n\nquickpred(DAT, \n          mincor = 0.4) # all predictors with absolute correlation over 0.4\n\n             id grade sex comp eduMother age mig specialNeeds cognAb GY\nid            0     0   0    0         0   0   0            0      0  0\ngrade         0     0   0    1         0   0   0            0      0  0\nsex           0     0   0    0         0   0   0            0      0  0\ncomp          0     1   0    0         0   0   0            0      1  1\neduMother     0     0   0    0         0   0   0            0      0  0\nage           0     0   0    0         0   0   0            0      0  0\nmig           0     0   0    0         0   0   0            0      0  0\nspecialNeeds  0     0   0    0         0   0   0            0      0  0\ncognAb        0     0   0    1         0   0   0            0      0  0\nGY            0     0   0    0         0   0   0            0      0  0\n\n\nNow, the predictionMatrix looks quite different and only a few variables can be used for prediction. This criterion could be a bit to strict, then we loose predictive power. BUT: This can also be a hint of multicollinearity between the variables.\nDo you detect multicollinearity hindering feasible imputation?\nWe can check this with a correlation matrix:\nNo correlation is above 0.6. I wonder a bit about the huge deviation from the predicitonMatrix. Why is GY in the predictionMatrix not predicted by comp, but in here?\n\nncol(DAT)\n\n[1] 10\n\ncor(DAT, use=\"na.or.complete\")\n\n                        id       grade           sex        comp    eduMother\nid            1.0000000000 -0.09368735 -0.0007952184  0.05464007  0.037915306\ngrade        -0.0936873543  1.00000000 -0.2465051450 -0.40732044 -0.114182264\nsex          -0.0007952184 -0.24650515  1.0000000000  0.23628199 -0.008284387\ncomp          0.0546400740 -0.40732044  0.2362819871  1.00000000  0.158323730\neduMother     0.0379153058 -0.11418226 -0.0082843875  0.15832373  1.000000000\nage           0.0007907618  0.17967951 -0.0836541092 -0.24776020 -0.134213163\nmig          -0.0760558585  0.12243852 -0.0095102820 -0.11902221 -0.036304492\nspecialNeeds -0.0126538053  0.07489106 -0.0424774084 -0.13974678 -0.018240552\ncognAb        0.0228079067 -0.19708916 -0.0176951858  0.39416113  0.144073575\nGY            0.0435209049 -0.15791329  0.0469059471  0.50678735  0.248731327\n                       age          mig specialNeeds      cognAb          GY\nid            0.0007907618 -0.076055858  -0.01265381  0.02280791  0.04352090\ngrade         0.1796795123  0.122438516   0.07489106 -0.19708916 -0.15791329\nsex          -0.0836541092 -0.009510282  -0.04247741 -0.01769519  0.04690595\ncomp         -0.2477602019 -0.119022215  -0.13974678  0.39416113  0.50678735\neduMother    -0.1342131632 -0.036304492  -0.01824055  0.14407358  0.24873133\nage           1.0000000000  0.088253513   0.12889415 -0.17932327 -0.24343696\nmig           0.0882535126  1.000000000  -0.01175476 -0.10682109 -0.05518325\nspecialNeeds  0.1288941489 -0.011754759   1.00000000 -0.09692705 -0.09833512\ncognAb       -0.1793232697 -0.106821087  -0.09692705  1.00000000  0.36289332\nGY           -0.2434369606 -0.055183250  -0.09833512  0.36289332  1.00000000\n\n\nWhy is multicollinearity a problem for model imputation?\nMulticollinearity in predictors can impact the imputation of missing data in a model. Imputation models aim to use as much information as possible to obtain the estimates required to complete any missing data. This is founded on the assumption that the data is missing at random. Therefore, overfitting the imputation models is less of a problem as any available associations, small or large are used. If these associations are estimated with large error, it will be reflected in the variance across imputation sets and taken into account when properly pooling analysis results.\nThis reasoning applies to multicollinearity as well. However, in situations of high multicollinearity, there might be practical implications as your computer might not be able to fit the model at all. In such cases, you might have to ‘dumb down’ your model or use a less multicollinearity sensitive model. The presence of multicollinearity can also lead to numerical problems with imputation algorithms.\nSources: https://stats.stackexchange.com/questions/278672/is-multicollinearity-problematic-for-imputation-models https://ete-online.biomedcentral.com/articles/10.1186/s12982-021-00095-3\nBecause 0.25 could be a bit to harsh, we go with 0.25, what indicates a medium correlation. Additionally, we go with the default of 5 imputations in 5 iterations.\n\nimp &lt;- mice(DAT, pred = quickpred(DAT, minpuc = 0.25, #A scalar, vector (of size ncol(data)) or matrix (square, of size ncol(data) specifying the minimum threshold(s) for the proportion of usable cases.\n                                  exclude=\"id\"), \n            method=meth) #although it wasn't in the R File I added the method argument, so we have the same methods as in the model before\n\n\n iter imp variable\n  1   1  grade  comp  eduMother  mig  specialNeeds  cognAb\n  1   2  grade  comp  eduMother  mig  specialNeeds  cognAb\n  1   3  grade  comp  eduMother  mig  specialNeeds  cognAb\n  1   4  grade  comp  eduMother  mig  specialNeeds  cognAb\n  1   5  grade  comp  eduMother  mig  specialNeeds  cognAb\n  2   1  grade  comp  eduMother  mig  specialNeeds  cognAb\n  2   2  grade  comp  eduMother  mig  specialNeeds  cognAb\n  2   3  grade  comp  eduMother  mig  specialNeeds  cognAb\n  2   4  grade  comp  eduMother  mig  specialNeeds  cognAb\n  2   5  grade  comp  eduMother  mig  specialNeeds  cognAb\n  3   1  grade  comp  eduMother  mig  specialNeeds  cognAb\n  3   2  grade  comp  eduMother  mig  specialNeeds  cognAb\n  3   3  grade  comp  eduMother  mig  specialNeeds  cognAb\n  3   4  grade  comp  eduMother  mig  specialNeeds  cognAb\n  3   5  grade  comp  eduMother  mig  specialNeeds  cognAb\n  4   1  grade  comp  eduMother  mig  specialNeeds  cognAb\n  4   2  grade  comp  eduMother  mig  specialNeeds  cognAb\n  4   3  grade  comp  eduMother  mig  specialNeeds  cognAb\n  4   4  grade  comp  eduMother  mig  specialNeeds  cognAb\n  4   5  grade  comp  eduMother  mig  specialNeeds  cognAb\n  5   1  grade  comp  eduMother  mig  specialNeeds  cognAb\n  5   2  grade  comp  eduMother  mig  specialNeeds  cognAb\n  5   3  grade  comp  eduMother  mig  specialNeeds  cognAb\n  5   4  grade  comp  eduMother  mig  specialNeeds  cognAb\n  5   5  grade  comp  eduMother  mig  specialNeeds  cognAb\n\n\nIn the next step we pool the coefficients of the regression models on the 4 imputed data sets.\n\nfit_ex2 &lt;- with(data=imp, exp=lm(grade ~ comp + sex + eduMother))\nsummary(pool(fit_ex2))\n\n         term   estimate  std.error statistic        df      p.value\n1 (Intercept)  3.1139346 0.01390008 224.02273  65.20258 7.538822e-96\n2        comp -0.2975177 0.01015659 -29.29307  14.40224 3.045094e-14\n3         sex -0.2712933 0.01765091 -15.36993 123.83933 1.747964e-30\n4   eduMother -0.1272092 0.02171720  -5.85753  71.79517 1.306656e-07\n\n\nWhat we can see in comparison to the complete case analysis and the first model with imputed data is, that the coefficients are still the same. But: Standard Error and p-value are decreased a lot.\nTo access the coefficients for linear regressions on the imputed data sets on its own, this lines of code are needed. So we can get a look how much the coefficients differ.\n\nsummary(lm(grade~comp+sex+eduMother, data=complete(imp, action=5)))\n\n\nCall:\nlm(formula = grade ~ comp + sex + eduMother, data = complete(imp, \n    action = 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.03520 -0.55835 -0.01148  0.53959  3.02601 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.12263    0.01211 257.866  &lt; 2e-16 ***\ncomp        -0.29025    0.00700 -41.467  &lt; 2e-16 ***\nsex         -0.27579    0.01604 -17.198  &lt; 2e-16 ***\neduMother   -0.13683    0.01897  -7.212 5.89e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8015 on 10527 degrees of freedom\nMultiple R-squared:  0.1998,    Adjusted R-squared:  0.1996 \nF-statistic: 876.4 on 3 and 10527 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing data and Statistical Modelling</span>"
    ]
  },
  {
    "objectID": "missing-data.html#derived-variables",
    "href": "missing-data.html#derived-variables",
    "title": "4  Missing data and Statistical Modelling",
    "section": "4.9 Derived Variables",
    "text": "4.9 Derived Variables\nExample: e.g. weight / height ratio defined \\(whr = wgt/hgt\\)\nHow to deal with the derived information?\nCompute the derived variabdle first and then impute does not work. Derived the imputation afterwards does not work.\n\nIf any one of the triplet, s missing, then the missing value can be calculated with certainty by a simple deterministic rule.\nApproach known as impute, then transform (von Hippel 2009)\nObvious problem: observations in e.g. whr not used by the imputation model → biases the estimates of parameters related to whr towards zero\n\nA way, that is better than the others is\n\n4.9.1 passive imputation\nTransformation is done on-the-fly within the imputation algorithm\n\nSince the transformed variable is available for imputation, the hope is that passive imputation removes the bias of the Impute, then transform methods\nwhile restoring consistency among the imputations that was broken in JAV (create derived variable first and impute just as another variable (JAV))\n\n\n→ not of the procedures are perfect, but this is the best we have\n→ Neither of the methods introduced so far works well for imputing the ratio \\(whr\\)\nBeware: \\(whr\\) has to be a column after \\(hgt\\) and \\(wgt\\) to be updated based on the newly imputed \\(hgt\\) and \\(wgt\\) values in the related iteration step\n\n4.9.1.1 Excercise\nUse ‘Passive imputation’ as imputation technique for the interaction term.\n\n#create data set with derived variable eduMSex (an interaction term)\nDAT_ext &lt;- DAT\nDAT_ext$eduMSex &lt;- DAT$eduMother * DAT$sex\n\n#compute a complete cases analysis with the derived variable as a predictor (interaction term)\ncc_ex3 &lt;- lm(grade ~ comp + sex + eduMother + eduMSex, data=DAT_ext)\nsummary(cc_ex3)\n\n\nCall:\nlm(formula = grade ~ comp + sex + eduMother + eduMSex, data = DAT_ext)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.09063 -0.55242 -0.01419  0.53217  2.84182 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.07666    0.01681 183.044   &lt;2e-16 ***\ncomp        -0.30564    0.01034 -29.573   &lt;2e-16 ***\nsex         -0.26063    0.02385 -10.927   &lt;2e-16 ***\neduMother   -0.06657    0.03460  -1.924   0.0544 .  \neduMSex     -0.11088    0.04853  -2.285   0.0224 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7902 on 5954 degrees of freedom\n  (4572 observations deleted due to missingness)\nMultiple R-squared:  0.1942,    Adjusted R-squared:  0.1936 \nF-statistic: 358.7 on 4 and 5954 DF,  p-value: &lt; 2.2e-16\n\n\nThe interaction term is significant negative and decreases the still significant negative effect of mothers education. So if a pupil is female and has a higher educated mother the probability to get better grades increases.\n\nfit_ex3 &lt;- with(data=imp_pas, exp=lm(grade ~ comp + sex + eduMother + eduMSex))\nsummary(pool(fit_ex3))\n\n         term    estimate   std.error  statistic         df       p.value\n1 (Intercept)  3.10476854 0.014485763 214.332408  70.701244 3.311248e-101\n2        comp -0.29533933 0.007955626 -37.123330  76.543029  1.061616e-50\n3         sex -0.26019946 0.020228856 -12.862787  80.955385  2.905715e-21\n4   eduMother -0.08185273 0.053831933  -1.520524   7.342119  1.702088e-01\n5     eduMSex -0.10163749 0.041408408  -2.454513 352.872999  1.458981e-02\n\n\nIn comparison to the complete cases model with the interaction term included we can see slightly different coefficients. The deviation of the coefficients are very, very small, though. But in the imputed model we have much lower p-values and a bit smaller standard errors.\nAnother way to impute derived variables:\n\n\n\n4.9.2 Rejection Sampling\n\nCreate imputations that are congenial with the substantive (complete-data) model\nThe imputation method requires a specification of the complete-data model\nBartlett, J. W., S. R. Seaman, I. R. White, and J. R. Carpenter. 2015. “Multiple Imputation of Covariates by Fully Conditional Specification: Accommodating the Substantive Model.” Statistical Methods in Medical Research 24 (4): 462–87.\n\n\n4.9.2.1 Exercise\nUse ‘Rejection Sampling for Imputation’ as imputation technique for the interaction term.\n\n#remove first column (the id, because not a predictor)\nDATs_ext &lt;- as.data.frame(DAT_ext[,-1])\n\n#for each variable in the data set you define a model like in mice!\nmeth &lt;- c(\"\", \"\", \"norm\", \"logreg\", \"\", \"logreg\", \"logreg\", \"norm\", \"\", \"eduMother*sex\") \n\n#impute and define complete cases model\nimps &lt;- smcfcs(originaldata = DATs_ext, meth = meth, smtype = \"lm\",\n               smformula = \"grade ~ comp + sex + eduMother + eduMSex\",\n               m=5, rjlimit = 15000, numit = 10)\n\n[1] \"Outcome variable(s): grade\"\n[1] \"Passive variables: eduMSex\"\n[1] \"Partially obs. variables: comp,eduMother,mig,specialNeeds,cognAb\"\n[1] \"Fully obs. substantive model variables: sex\"\n[1] \"Imputation  1\"\n[1] \"Imputing missing outcomes using specified substantive model.\"\n[1] \"Imputing:  comp  using  eduMother,mig,specialNeeds,cognAb,sex  plus outcome\"\n[1] \"Imputing:  eduMother  using  comp,mig,specialNeeds,cognAb,sex  plus outcome\"\n[1] \"Imputing:  mig  using  comp,eduMother,specialNeeds,cognAb,sex  plus outcome\"\n[1] \"Imputing:  specialNeeds  using  comp,eduMother,mig,cognAb,sex  plus outcome\"\n[1] \"Imputing:  cognAb  using  comp,eduMother,mig,specialNeeds,sex  plus outcome\"\n[1] \"Imputation  2\"\n[1] \"Imputation  3\"\n[1] \"Imputation  4\"\n[1] \"Imputation  5\"\n\n\nVery good method, because handle the interaction better! Similar to mice, but this method is not useful for multi level modeling!\n\n#make regressions for all imputated data sets\nfit_s &lt;- lapply(imps$impDatasets, lm,\n              formula = grade ~ comp + sex + eduMother + eduMSex)\n# pool the models\nsummary(pool(fit_s))\n\n         term    estimate  std.error  statistic         df      p.value\n1 (Intercept)  3.10259949 0.01307367 237.316602 1241.24967 0.000000e+00\n2        comp -0.30462155 0.00901524 -33.789620   25.07864 1.952160e-22\n3         sex -0.24802686 0.02039486 -12.161242   75.82634 1.708761e-19\n4   eduMother -0.07921871 0.03391365  -2.335894   26.03366 2.747000e-02\n5     eduMSex -0.12311174 0.05561163  -2.213777   13.73100 4.430166e-02\n\n\nAlso in this model the coefficients are quite the same as in the other models. p-values and standard errors are also very small in comparison to the complete case model.\n→ both models are an improvement in comparison to the complete cases model and shows, that the model coefficients are robust, because MAR holds.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing data and Statistical Modelling</span>"
    ]
  },
  {
    "objectID": "missing-data.html#interaction-terms-quadratic-relations",
    "href": "missing-data.html#interaction-terms-quadratic-relations",
    "title": "4  Missing data and Statistical Modelling",
    "section": "4.10 Interaction Terms & Quadratic Relations",
    "text": "4.10 Interaction Terms & Quadratic Relations\nThe standard MICE algorithm only models main effects\n\nFor categorical data: imputing the data in separate groups, i.e. by splitting the dataset into two or more parts, run “mice” on each part\nGeneric methods to preserve interactions include tree-based regression and classification\nAgain: Rejection Sampling for Imputation is doing a better job than ‘transform then impute’, ‘JAV’, or ‘passive imputation’\n\nPassive imputation cannot handle interaction and quadratic terms, but mice\nOne way to analyze nonlinear relations by a linear model is to include quadratic or cubic versions of the explanatory variables into the model. - Not working well: ‘transform then impute’, ‘JAV’, ‘passive imputation’ - Idea: impute polynomial combination \\(Z = Y β_1 + Y^2β_2\\) instead of \\(Y\\) and \\(Y^2\\) → shown by simulation, does a good job! - Approach available in ‘mice’ as method ‘quadratic’ - The idea can be generalized to polynomial bases of higher orders\n\n4.10.1 Exercise\nAnalysis model: \\(grade_i = β_0 + β_1comp_i + β_2sex_i + β_3eduMother_i + β_4sex_i × eduMother_i + ϵ_i\\)\n\nUse ‘Passive imputation’ as imputation technique for the interaction term.\n\n\nDAT_ext &lt;- DAT\nDAT_ext$eduMSex &lt;- DAT$eduMother * DAT$sex\ncc_ex3 &lt;- lm(grade ~ comp + sex + eduMother + eduMSex, data=DAT_ext)\nsummary(cc_ex3)\n\n\nCall:\nlm(formula = grade ~ comp + sex + eduMother + eduMSex, data = DAT_ext)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.09063 -0.55242 -0.01419  0.53217  2.84182 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.07666    0.01681 183.044   &lt;2e-16 ***\ncomp        -0.30564    0.01034 -29.573   &lt;2e-16 ***\nsex         -0.26063    0.02385 -10.927   &lt;2e-16 ***\neduMother   -0.06657    0.03460  -1.924   0.0544 .  \neduMSex     -0.11088    0.04853  -2.285   0.0224 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7902 on 5954 degrees of freedom\n  (4572 observations deleted due to missingness)\nMultiple R-squared:  0.1942,    Adjusted R-squared:  0.1936 \nF-statistic: 358.7 on 4 and 5954 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing data and Statistical Modelling</span>"
    ]
  },
  {
    "objectID": "missing-data.html#conditional-imputation",
    "href": "missing-data.html#conditional-imputation",
    "title": "4  Missing data and Statistical Modelling",
    "section": "4.11 Conditional Imputation",
    "text": "4.11 Conditional Imputation\nIn some cases it makes sense to restrict the imputations, possibly conditional on other data\n\nE.g one way of dealing with a mismatch between imputed and observed values is to censor the values at some specified minimum or maximum value\nIdea: Adjust/correct inconsistent imputed values ex-post in certain limits\n\n\n4.11.1 Exercise\ncreate the model by split in very bad and very good students.\n\ncreate a new data set with a new variable only with pupils with bad grades\n\n\nDAT_b &lt;- DAT\nDAT_b$grade56 &lt;- ifelse(DAT$grade %in% NA, NA, ifelse(DAT$grade %in% c(5,6), 1,0))\ntable(DAT_b$grade56, exclude=NULL)\n\n\n   0    1 &lt;NA&gt; \n9332  351  848 \n\n\nOnly 351 pupils have bad grades from overall 10531 pupils.\n\nComplete Case Model only with bad grades\n\n\ncc_ex3 &lt;- glm(grade56 ~ comp + sex + eduMother, family = binomial(link = \"logit\"), data=DAT_b)\nsummary(cc_ex3)\n\n\nCall:\nglm(formula = grade56 ~ comp + sex + eduMother, family = binomial(link = \"logit\"), \n    data = DAT_b)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -3.71840    0.13355 -27.843  &lt; 2e-16 ***\ncomp        -0.78864    0.07555 -10.439  &lt; 2e-16 ***\nsex         -0.57516    0.19565  -2.940  0.00329 ** \neduMother   -0.06255    0.22161  -0.282  0.77774    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1349.3  on 5958  degrees of freedom\nResidual deviance: 1209.9  on 5955  degrees of freedom\n  (4572 observations deleted due to missingness)\nAIC: 1217.9\n\nNumber of Fisher Scoring iterations: 7\n\n\nInterestingly, in this model comp and sex are still negatively sigificant, eduMother is not.\n\n#create on imputed data set\nimp &lt;- mice(DAT_b, m=1, maxit=1)\n\n\n iter imp variable\n  1   1  grade  comp  eduMother  mig  specialNeeds  cognAb  grade56\n\n\n\n# define the methods\nmeth &lt;- imp$method\nmeth[\"eduMother\"] &lt;- meth[\"mig\"] &lt;- meth[\"specialNeeds\"] &lt;- \"logreg\"\nDAT_b$eduMother &lt;- as.factor(DAT_b$eduMother)\nDAT_b$mig &lt;- as.factor(DAT_b$eduMother)\nDAT_b$specialNeeds &lt;- as.factor(DAT_b$specialNeeds)\nmeth[\"grade56\"] &lt;- \"\" #not imputed, because derived information done in the imputation\n\npred &lt;- imp$predictorMatrix\npred[, \"id\"] &lt;- 0\npred[, \"grade56\"] &lt;- 0 # not using grade56 for imputing other variables\npost &lt;- make.post(DAT_b) #create post as a helper for the argument in mice, specifies post-processing for a variable after each iteration of imputation\npost[\"grade56\"] &lt;- \"ifdo(grade %in% c(5,6), grade %in% (1,2,3,4)), c(1,0)\" #including the post adjustment in splitting in good grades and bad\n\n#impute data 5 times including the post processing\nimp_post &lt;- mice(DAT_b, predictorMatrix = pred, meth=meth, post=post)\n\n\n iter imp variable\n  1   1  grade  comp  eduMother  specialNeeds  cognAb\n  1   2  grade  comp  eduMother  specialNeeds  cognAb\n  1   3  grade  comp  eduMother  specialNeeds  cognAb\n  1   4  grade  comp  eduMother  specialNeeds  cognAb\n  1   5  grade  comp  eduMother  specialNeeds  cognAb\n  2   1  grade  comp  eduMother  specialNeeds  cognAb\n  2   2  grade  comp  eduMother  specialNeeds  cognAb\n  2   3  grade  comp  eduMother  specialNeeds  cognAb\n  2   4  grade  comp  eduMother  specialNeeds  cognAb\n  2   5  grade  comp  eduMother  specialNeeds  cognAb\n  3   1  grade  comp  eduMother  specialNeeds  cognAb\n  3   2  grade  comp  eduMother  specialNeeds  cognAb\n  3   3  grade  comp  eduMother  specialNeeds  cognAb\n  3   4  grade  comp  eduMother  specialNeeds  cognAb\n  3   5  grade  comp  eduMother  specialNeeds  cognAb\n  4   1  grade  comp  eduMother  specialNeeds  cognAb\n  4   2  grade  comp  eduMother  specialNeeds  cognAb\n  4   3  grade  comp  eduMother  specialNeeds  cognAb\n  4   4  grade  comp  eduMother  specialNeeds  cognAb\n  4   5  grade  comp  eduMother  specialNeeds  cognAb\n  5   1  grade  comp  eduMother  specialNeeds  cognAb\n  5   2  grade  comp  eduMother  specialNeeds  cognAb\n  5   3  grade  comp  eduMother  specialNeeds  cognAb\n  5   4  grade  comp  eduMother  specialNeeds  cognAb\n  5   5  grade  comp  eduMother  specialNeeds  cognAb\n\n\nWarning: Number of logged events: 1\n\n\nIn my comptutation is in imp_post not a warning, only in imp:\n\nimp_post$loggedEvents\n\n  it im dep      meth out\n1  0  0     collinear mig\n\n\n\nimp$loggedEvents\n\nNULL\n\n\nSo, if we go with the imp data sets we should take migration out, because it is colinear to bad grades.\nDAT_b &lt;- DAT_b %&gt;% dplyr::select(-mig)\nHere the model for the imp data set:\n\nimp1 &lt;- mice::complete(imp_post, complete=1)\nfit_ex4 &lt;- with(data=imp_post, exp=glm(grade56 ~ comp + sex + eduMother))\nsummary(pool(fit_ex4))\n\n         term     estimate   std.error   statistic         df      p.value\n1 (Intercept)  0.040635170 0.003007331  13.5120397  521.28261 6.969487e-36\n2        comp -0.032091890 0.002113474 -15.1844257   31.53910 4.710283e-16\n3         sex -0.009261914 0.003862560  -2.3978694 4693.91365 1.652957e-02\n4  eduMother1 -0.001649512 0.005135410  -0.3212036   85.01737 7.488444e-01\n\n\nOpen question: What is the function complete for? Can’t find an explanation for the argument complete=1, only for complete itself. Does it extract the completed data and 1 means, we only want the completed cases?\nCompared to the complete cases model cc_ex3 this model has totally different cofficients. In the imputed model competence doesn’t have a huge effect, neither sex nor mothers education. This shows, it really makes a difference how you deal with the missingness! While this model account for the missingness, the complete case analysis does not do.\n\n\n4.11.2 Mice Extensions\nSkipping imputations\n\nIn some cases it may also be useful to skip imputation of certain cells e.g. skip imputation of quality of life for the deceased\nCreates missing data in the predictors: (i) remove the predictor from all imputation models OR (ii) missing values have to propagated through the algorithm\nvan Hippel (2007): delete imputed dependent variable values before conducting regression estimation (yields more efficient estimates), called MID (multiple imputation than deletion) approach\n\nOverimputation\n\nImputing cells with observed data\nE.g. for evaluating whether observed data point fits the imputation model (meanwhile implemented in R package mice, “where” argument)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing data and Statistical Modelling</span>"
    ]
  },
  {
    "objectID": "missing-data.html#hybrid-imputation",
    "href": "missing-data.html#hybrid-imputation",
    "title": "4  Missing data and Statistical Modelling",
    "section": "4.12 Hybrid imputation",
    "text": "4.12 Hybrid imputation\nFor some cases, it is the best solution to do a joint modelling like for grades and competence or special needs and language, therefore hybrid imputation is the right strategy.\nCombine JM and FCS\n\nRemember: JM imputes all variables at once & FCS imputes each variable separately In actual data: variables might be interrelated (e.g., set of scale items, two variables with interaction terms, compositions adding up to a total)\nImpute interrelated variables as a block\nPartition complete set of variables into blocks and impute all variables in the same block jointly\nIn the mice package: blockargument: MICE algo iterates over blocks rather than variables\nBlock sequential regression multivariate imputation (see Zhu, J. 2016. Assessment and Improvement of a Sequential Regression Multivariate Imputation Algorithm. PhD thesis, University of Michigan):\nunits are partitioned into blocks according to the missing data pattern\nimputation model for a given variable is modified for each block (only the observed data with the block serves as predictor)\n\n\n4.12.1 Exercise\nAnalysis model: \\[\ngrade_i = β_0 + β_1comp_i + β_2sex_i + β_3eduMother_i + ϵ_i\n\\] Analyse that model using imputed data. In doing so, impute grade and comp by joint modelling and also mig and specialNeeds (using the block command of the mice function). At this, specify grade56 ex post.\n\nDAT_bl &lt;- DAT[,-1] #exclude index from data set\nDAT_bl$grade56 &lt;- ifelse(DAT$grade %in% NA, NA, ifelse(DAT$grade %in% c(5,6), 1,0)) #create binary for bad grades\nbl &lt;- list(bl1=c(\"grade\", \"comp\"), #define interrelated variables\n           bl2=c(\"eduMother\"), bl3=c(\"mig\", \"specialNeeds\"), bl4=c(\"cognAb\"))\nimp_block &lt;- mice(DAT_bl, blocks=bl, m=1, maxit = 1)\n\n\n iter imp variable\n  1   1  grade comp  eduMother  mig specialNeeds  cognAb\n\n\n\n#create  imputed model\nfit_ex5 &lt;- with(data=imp_post, exp=lm(grade56 ~ comp + sex + eduMother))\nsummary(pool(fit_ex5))\n\n         term     estimate   std.error   statistic         df      p.value\n1 (Intercept)  0.040635170 0.003007331  13.5120397  521.28261 6.969487e-36\n2        comp -0.032091890 0.002113474 -15.1844257   31.53910 4.710283e-16\n3         sex -0.009261914 0.003862560  -2.3978694 4693.91365 1.652957e-02\n4  eduMother1 -0.001649512 0.005135410  -0.3212036   85.01737 7.488444e-01\n\n\nCompared to the pooled fit_ex4 results, the values are very similar to each other, not only the coefficients, but also the standard errors and the p-value. The assumed interrelatedness between some variables does not make a huge difference. The results differs a lot in comparison to cc_ex3, which uses only the complete cases.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing data and Statistical Modelling</span>"
    ]
  },
  {
    "objectID": "missing-data.html#longitudinal-data",
    "href": "missing-data.html#longitudinal-data",
    "title": "4  Missing data and Statistical Modelling",
    "section": "4.13 Longitudinal data",
    "text": "4.13 Longitudinal data\n\nLongitudinal data can be coded into “long” and “wide” formal\nWide format: each micro unit/individual has a row, observation of distinct time points are coded as accordant variables (e.g. ‘employ.2020’, ‘employ.2021’, etc.)\nLong format: multiple lines for each micro unit/individual; one line for each time point; constant variables have the same values in each line; time variable required\nIn both formats there is an identification variable (‘id’)\nAdvantage long format: can handle unbalanced panel designs\nAdvantage wide format: standard software methods can easily be applied to that data\nIn statistical software: Wide-to-Long conversion is not really a problem\nLong-to-wide conversion can be quite cumbersome: If the data is highly unbalanced there will be many missing values in the wide format data\n\nEmphasized:\n\nUse wide format for imputation and apply single level MI (mice) techniques MI (mice) can also be done on long data; then use multi-level MI (mice) methods\nHowever : When imputing data in long format, you loose the ‘order’ information, i.e. time points are ordered; (in multi-level modelling units nested in higher clusters are not considered as being ordered)\n\n\n4.13.1 Exercise\nVariables: log wage (lwage); years of full time work experience (exp); weeks worked (wks); 1 if blue-collar occupation, 0 if not (bluecol); 1 if the individual works in a manufactural industry, 0 if not (ind); 1 if the individual resides in the South, 0 if not (south); 1 if the individual resides in an metropolitan area, 0 if not (smsa); 1 if individual is married, 0 if not (married); 1 if the individual wage is set by a union contract, 0 if not (union); years of education (ed); 1 if the individual is female, 0 if not (sex); 1 if the individual is black, 0 if not (black)\nCheck missing data pattern & test for MCAR (Hint: “mcar test” works best with data in long format)\n\n#read in data from day 2 with manually generated missings\nlDAT &lt;- read_dta(\"data/3.Example3.dta\")\n\n# test with Little's missing completely at random test (MCAR)\nmcar_test(lDAT)\n\n# A tibble: 1 × 4\n  statistic    df p.value missing.patterns\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;            &lt;int&gt;\n1      298.    38       0                4\n\n\nThe p-value is so small, that MCAR must be rejected. There is no indicator, that the missings differ randomly. We must reject \\(H_0\\). Looking at the plot before, that is not suprisingly at all.\nHow many percentage of the data is missing per variable?\n\nmiss &lt;- md.pattern(lDAT, plot=FALSE)\nmiss[nrow(miss),]/nrow(lDAT)\n\n       exp        wks    bluecol        ind      south       smsa    married \n0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 \n       sex      union      black         id       time         ed      lwage \n0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.05042017 0.17286915 \n           \n0.22328932 \n\n\nMissing data in education and lwage. While education is quite negligible, lwage, the variable that should be explained is not!\nConvert data to wide format (it is in long format), e.g. using the reshape2 package:\n\nlDAT$expSq &lt;- lDAT$exp * lDAT$exp\nwDAT &lt;- reshape(as.data.frame(lDAT), \n                v.names=c(\"lwage\", \"exp\", \"expSq\", \"wks\", \"bluecol\", \"ind\", \"south\", \"smsa\", \"married\", \"union\"), \n                idvar=\"id\",timevar=\"time\",\n                direction=\"wide\")\n#how many values are missing overall for each\ntable(complete.cases(wDAT))/nrow(wDAT)\n\n\n    FALSE      TRUE \n0.7008403 0.2991597 \n\n\nOnly 29 % of the individuals have answered all questions in all years.\nHow many percentage of the data is missing per variable in the wide format?\n\nmiss &lt;- md.pattern(wDAT, plot=FALSE)\nmiss[nrow(miss),]/nrow(wDAT)\n\n       sex      black         id      exp.1    expSq.1      wks.1  bluecol.1 \n0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 \n     ind.1    south.1     smsa.1  married.1    union.1      exp.2    expSq.2 \n0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 \n     wks.2  bluecol.2      ind.2    south.2     smsa.2  married.2    union.2 \n0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 \n     exp.3    expSq.3      wks.3  bluecol.3      ind.3    south.3     smsa.3 \n0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 \n married.3    union.3      exp.4    expSq.4      wks.4  bluecol.4      ind.4 \n0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 \n   south.4     smsa.4  married.4    union.4      exp.5    expSq.5      wks.5 \n0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 \n bluecol.5      ind.5    south.5     smsa.5  married.5    union.5      exp.6 \n0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 \n   expSq.6      wks.6  bluecol.6      ind.6    south.6     smsa.6  married.6 \n0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 \n   union.6      exp.7    expSq.7      wks.7  bluecol.7      ind.7    south.7 \n0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 \n    smsa.7  married.7    union.7         ed    lwage.6    lwage.1    lwage.3 \n0.00000000 0.00000000 0.00000000 0.05042017 0.14789916 0.16470588 0.16638655 \n   lwage.7    lwage.5    lwage.4    lwage.2            \n0.16638655 0.17815126 0.18991597 0.19663866 1.26050420 \n\n\nWe can see education is around 5 and only asked once, while the age is asked in every for. While in year 6 are the missings the lowest, there are the highest in year 2.\n\ngg_miss_upset(wDAT)\n\n\n\n\n\n\n\n\nWe can see, there are connections between the missing values, even though there are only a few cases, where the question about wage is not answered in 4 years of 7.\nOpen question : Should we exclude these cases, because it is difficult to impute values, if we only have 3 values to infer from? Or would it only be a problem, if 2 values are left to infere from?\n\nImpute missing values using mice; for this use proper imputation techniques\n\n\n# firstly, create one imputation\nimp &lt;- mice(wDAT, m=1, maxit=1)\n\n\n iter imp variable\n  1   1  ed  lwage.1  lwage.2  lwage.3  lwage.4  lwage.5  lwage.6  lwage.7\n\n\nWarning: Number of logged events: 16\n\n\nThe function has detected multicollinearity and has removed 16 times the predictors.\nA further explanation for loggedEvents:\nThe logged events form a structured report that identify problems with the data, and details which corrective actions were taken by mice(). It is a component called loggedEvents of the mids object.\nA variable is removed from the model by internal edits of the predictorMatrix, method, visitSequence and post components of the model. The data are kept intact.\nNote that setting remove.constant = FALSE or remove.collinear = FALSE bypasses usual safety measures in mice, and could cause problems further down the road. If a variable has only NA’s, it is considered a constant variable, and will not be imputed. Setting remove.constant = FALSE will cause numerical problems since there are no observed cases to estimate the imputation model, but such variables can be imputed by passive imputation by specifying the allow.na = TRUE argument.\nDuring execution of the main algorithm, the entries in loggedEvents can signal the following actions:\n\nA predictor that is constant or correlates higher than 0.999 with the target variable is removed from the univariate imputation model. The cut-off value can be specified by the threshold argument;\nIf all predictors are removed, this is noted in loggedEvents, and the imputation model becomes an intercept-only model;\nThe degrees of freedom can become negative, usually because there are too many predictors relative to the number of observed values for the target. In that case, the degrees of freedom are set to 1, and a note is written to loggedEvents.\n\nBetween which variables is multicolinearity or is there a constant in it?\n\nimp$loggedEvents\n\n   it im     dep      meth     out\n1   0  0         collinear   exp.2\n2   0  0         collinear expSq.2\n3   0  0         collinear   exp.3\n4   0  0         collinear expSq.3\n5   0  0         collinear   exp.4\n6   0  0         collinear expSq.4\n7   0  0         collinear   exp.5\n8   0  0         collinear expSq.5\n9   0  0         collinear   exp.6\n10  0  0         collinear expSq.6\n11  0  0         collinear   exp.7\n12  0  0         collinear expSq.7\n13  1  1 lwage.2       pmm south.4\n14  1  1 lwage.4       pmm south.1\n15  1  1 lwage.5       pmm south.5\n16  1  1 lwage.7       pmm south.4\n\n\nIn the first 12 data sets exp or expSq is alternately not imputed (for further explanation look here.\n\nIf exp.x is indicated under the column out, this variable is removed from the model, because its collinear with other exp.x AND/ OR expSq.x. As a result exp.xis not imputed, but expSq.x or the other exp.x. If expSq.x is in the column out, its the other way around.\nIn the imputed data sets 13 to 16 south.x was removed from the imputation model because its collinear with lwage.x. We can override this with the remove.collinear argument.\n\nOpen question: Why do we include exSq in our imputation model? exSq is computed out of ex, therefore, its a derived variable. Why don’t we impute data first and then compute expSq on the fly with passive or conditional imputation?\nWe can go on because it is okay, that there is multicollinearity between the work experience over years and the quadric variable, and it is also okay, that there is multicollinearity between the wage and working in the south.\nLets have a look at the predictorMatrix, to get a bigger picture:\n\npred &lt;- imp$predictorMatrix\n\nOpen question: Why is it okay, to have such a predictorMatrix in this case? We want to impute data on education and lwage. Why should being in an union helps to predict for example education? In the exercises before with cross-sectional data, we have said, that such a predictiorMatrix makes often no sense and we should check it with quickpred. Also, we should avoid multicollinearity, because otherwise the algorithm will not work. BUT the algorithm works appropriately in the following. I don’t really get that.\nLet’s have a look on the method:\n\nimp$method\n\n      sex        ed     black        id   lwage.1     exp.1   expSq.1     wks.1 \n       \"\"     \"pmm\"        \"\"        \"\"     \"pmm\"        \"\"        \"\"        \"\" \nbluecol.1     ind.1   south.1    smsa.1 married.1   union.1   lwage.2     exp.2 \n       \"\"        \"\"        \"\"        \"\"        \"\"        \"\"     \"pmm\"        \"\" \n  expSq.2     wks.2 bluecol.2     ind.2   south.2    smsa.2 married.2   union.2 \n       \"\"        \"\"        \"\"        \"\"        \"\"        \"\"        \"\"        \"\" \n  lwage.3     exp.3   expSq.3     wks.3 bluecol.3     ind.3   south.3    smsa.3 \n    \"pmm\"        \"\"        \"\"        \"\"        \"\"        \"\"        \"\"        \"\" \nmarried.3   union.3   lwage.4     exp.4   expSq.4     wks.4 bluecol.4     ind.4 \n       \"\"        \"\"     \"pmm\"        \"\"        \"\"        \"\"        \"\"        \"\" \n  south.4    smsa.4 married.4   union.4   lwage.5     exp.5   expSq.5     wks.5 \n       \"\"        \"\"        \"\"        \"\"     \"pmm\"        \"\"        \"\"        \"\" \nbluecol.5     ind.5   south.5    smsa.5 married.5   union.5   lwage.6     exp.6 \n       \"\"        \"\"        \"\"        \"\"        \"\"        \"\"     \"pmm\"        \"\" \n  expSq.6     wks.6 bluecol.6     ind.6   south.6    smsa.6 married.6   union.6 \n       \"\"        \"\"        \"\"        \"\"        \"\"        \"\"        \"\"        \"\" \n  lwage.7     exp.7   expSq.7     wks.7 bluecol.7     ind.7   south.7    smsa.7 \n    \"pmm\"        \"\"        \"\"        \"\"        \"\"        \"\"        \"\"        \"\" \nmarried.7   union.7 \n       \"\"        \"\" \n\n\nBecause we have only missings in lwage and ed, only for this variables methods are chosen. The pmm algorithm seems appropriate for numeric (lwage) or discrete variables (ed).\nNow we can impute, using this time not 5, but 10 imputation as a default\n\n#with exSq included\nimp_wide &lt;- mice(wDAT, pred=pred, print=FALSE, seed=25641, maxit=10, m=10)\n\nWarning: Number of logged events: 400\n\n\nOpen question: Again, why are we use expSq and not treat it like a derived variable?\n\nEstimate the above model, using the suitable strategy (fixed effects model).\n\n\nresList &lt;- vector(length=10, mode=\"list\") #object to write estimated model into a list\n\n#write a loop \nfor(i in 1:10){ \n  #i &lt;- 1\n  dat_i &lt;- complete(imp_wide, action =i) #only use competed data\n  datLong_i &lt;- reshape(dat_i, idvar = \"id\", varying = c(5:74), direction = \"long\") #transform to llong data\n  resList[[i]] &lt;- plm(lwage ~ exp + expSq + wks + bluecol + ind + south + smsa + married + union, \n                      model=\"within\", index = c(\"id\", \"time\"),data=datLong_i) # estimate model \n}\nsummary(pool(as.mira(resList)))\n\nWarning: The `exponentiate` argument is not supported in the `tidy()` method for `plm` objects and will be ignored.\nThe `exponentiate` argument is not supported in the `tidy()` method for `plm` objects and will be ignored.\nThe `exponentiate` argument is not supported in the `tidy()` method for `plm` objects and will be ignored.\nThe `exponentiate` argument is not supported in the `tidy()` method for `plm` objects and will be ignored.\nThe `exponentiate` argument is not supported in the `tidy()` method for `plm` objects and will be ignored.\nThe `exponentiate` argument is not supported in the `tidy()` method for `plm` objects and will be ignored.\nThe `exponentiate` argument is not supported in the `tidy()` method for `plm` objects and will be ignored.\nThe `exponentiate` argument is not supported in the `tidy()` method for `plm` objects and will be ignored.\nThe `exponentiate` argument is not supported in the `tidy()` method for `plm` objects and will be ignored.\nThe `exponentiate` argument is not supported in the `tidy()` method for `plm` objects and will be ignored.\n\n\n     term      estimate    std.error  statistic        df       p.value\n1     exp  0.1131887526 2.682510e-03 42.1950927  895.8802 4.114985e-215\n2   expSq -0.0004044265 6.349121e-05 -6.3698027  202.7895  1.245413e-09\n3     wks  0.0006577745 6.864566e-04  0.9582172  260.5186  3.388416e-01\n4 bluecol -0.0183777460 1.540657e-02 -1.1928510  413.6676  2.336115e-01\n5     ind  0.0165492738 1.808501e-02  0.9150823  184.1453  3.613452e-01\n6   south  0.0080990915 4.152673e-02  0.1950332  122.4989  8.456900e-01\n7    smsa -0.0364949530 2.216189e-02 -1.6467435  276.9711  1.007449e-01\n8 married -0.0338298665 2.022101e-02 -1.6730054 1744.6843  9.450550e-02\n9   union  0.0230539122 1.614320e-02  1.4280882 1003.5565  1.535776e-01\n\n\nWe have computed a fixed effect model, a partioned regression with a within estimator. So, the coefficients can only indicate the effect of a variable on a individual over time, in other words the dynamics for individuals. In consequence: What doef i.e. experience have for an effect on the wage of an individual?\n\nOppose the model on imputed data and a complete cases analysis. What do you see?\n\n\ncc &lt;- plm(lwage ~ exp + expSq + wks + bluecol + ind + south + smsa + married + union, \n    model=\"within\", index = c(\"id\", \"time\"), data=lDAT) \nsummary(cc)\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = lwage ~ exp + expSq + wks + bluecol + ind + south + \n    smsa + married + union, data = lDAT, model = \"within\", index = c(\"id\", \n    \"time\"))\n\nUnbalanced Panel: n = 595, T = 2-7, N = 3445\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-1.732757 -0.051162  0.004293  0.058480  1.943546 \n\nCoefficients:\n           Estimate  Std. Error t-value  Pr(&gt;|t|)    \nexp      1.1253e-01  2.8094e-03 40.0558 &lt; 2.2e-16 ***\nexpSq   -3.9296e-04  6.2569e-05 -6.2804 3.894e-10 ***\nwks      8.6849e-04  6.8512e-04  1.2676   0.20503    \nbluecol -1.8784e-02  1.5930e-02 -1.1792   0.23843    \nind      1.8188e-02  1.7828e-02  1.0202   0.30770    \nsouth    1.8517e-03  3.7109e-02  0.0499   0.96021    \nsmsa    -3.2472e-02  2.2201e-02 -1.4626   0.14367    \nmarried -3.6709e-02  2.1562e-02 -1.7025   0.08877 .  \nunion    1.4107e-02  1.7578e-02  0.8026   0.42228    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    196.67\nResidual Sum of Squares: 69.029\nR-Squared:      0.64901\nAdj. R-Squared: 0.57451\nF-statistic: 583.686 on 9 and 2841 DF, p-value: &lt; 2.22e-16\n\n\nIn both models only exp and expSq are significant in their effects. The estimates are quite similar to each other, but in the imputed model p-values are way more smaller. The cc model makes a good job in this case.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing data and Statistical Modelling</span>"
    ]
  }
]